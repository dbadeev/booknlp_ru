import torch
from typing import List, Dict
from razdel import sentenize, tokenize

from src.core.interfaces import BasePreprocessor
from src.core.data_structures import Token
from src.cobald_parser.modeling_parser import CobaldParser
from src.cobald_parser.configuration import CobaldParserConfig
from transformers import AutoTokenizer


class CobaldPreprocessor(BasePreprocessor):
    def __init__(self, model_path: str, device: str = "cpu"):
        print(f"üß† –ó–∞–≥—Ä—É–∑–∫–∞ CoBaLD Parser –∏–∑ {model_path}...")
        self.device = device

        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –º–æ–¥–µ–ª–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª–∞—Å—Å—ã –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –≤–∞–º–∏ —Ñ–∞–π–ª–æ–≤
        self.config = CobaldParserConfig.from_pretrained(model_path)
        self.model = CobaldParser.from_pretrained(model_path, config=self.config)
        self.model.to(self.device)
        self.model.eval()

        # 2. –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫ (–∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –º–æ–¥–µ–ª–∏)
        # –§–æ—Ä–º–∞—Ç —Å–ª–æ–≤–∞—Ä–µ–π –≤ –∫–æ–Ω—Ñ–∏–≥–µ: {"0": "nsubj", "1": "root"...}
        self.vocab = self.config.vocabulary

        print("‚úÖ CoBaLD –≥–æ—Ç–æ–≤ (–°–∏–Ω—Ç–∞–∫—Å–∏—Å + –°–µ–º–∞–Ω—Ç–∏–∫–∞)")

    def process(self, text: str) -> List[List[Token]]:
        output_sentences = []

        # 1. –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (Razdel)
        # –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º razdel, —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ char_start/end
        chunk_sents = list(sentenize(text))

        for sent_span in chunk_sents:
            # 2. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (Razdel)
            # –°–æ–±–∏—Ä–∞–µ–º —Ç–æ–∫–µ–Ω—ã –∏ –∏—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
            razdel_tokens = list(tokenize(sent_span.text))
            if not razdel_tokens:
                continue

            words = [t.text for t in razdel_tokens]

            # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç: razdel –¥–∞–µ—Ç —Å–º–µ—â–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–∞—á–∞–ª–∞ —Å—Ç—Ä–æ–∫–∏.
            # –ù–∞–º –Ω—É–∂–Ω—ã –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ text.
            # sent_span.start - –Ω–∞—á–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            token_metas = []
            for t in razdel_tokens:
                abs_start = sent_span.start + t.start
                abs_end = sent_span.start + t.stop
                token_metas.append((abs_start, abs_end))

            # 3. –ò–Ω—Ñ–µ—Ä–µ–Ω—Å CoBaLD
            # –ú–æ–¥–µ–ª—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –±–∞—Ç—á —Å–ø–∏—Å–∫–æ–≤ —Å—Ç—Ä–æ–∫
            with torch.no_grad():
                # forward(words=[['word1', ...]], inference_mode=True)
                outputs = self.model(
                    words=[words],
                    inference_mode=True
                )

            # 4. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            # –ú—ã –±–µ—Ä–µ–º [0], —Ç.–∫. –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è
            conll_tokens = self._decode_output(outputs, token_metas, words)
            output_sentences.append(conll_tokens)

        return output_sentences

    def _decode_output(self, outputs: Dict, token_metas: List[tuple], words: List[str]) -> List[Token]:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–Ω–∑–æ—Ä—ã –º–æ–¥–µ–ª–∏ –≤ –æ–±—ä–µ–∫—Ç—ã Token.
        –õ–æ–≥–∏–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∏–∑ pipeline.py
        """
        batch_idx = 0
        n_words = len(words)  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ #NULL –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è

        tokens_result = []

        # --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–µ–¥–∏–∫–∞—Ç–æ–≤ (ID –∫–ª–∞—Å—Å–æ–≤) ---
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª—é—á–∏ –∏–∑ pipeline.py

        # –õ–µ–º–º—ã
        lemma_rule_ids = outputs["lemma_rules"][batch_idx, :n_words].tolist() if "lemma_rules" in outputs else None

        # POS-—Ç–µ–≥–∏
        joint_feats_ids = outputs["joint_feats"][batch_idx, :n_words].tolist() if "joint_feats" in outputs else None

        # –°–∏–Ω—Ç–∞–∫—Å–∏—Å (UD)
        # deps_ud shape: [n_edges, 3] -> (batch_idx, head, label)
        deps_ud = outputs["deps_ud"]
        current_sent_deps = deps_ud[deps_ud[:, 0] == batch_idx][:, 1:].tolist() if deps_ud is not None else []

        # –°–µ–º–∞–Ω—Ç–∏–∫–∞ (ENG-003)
        deepslot_ids = outputs["deepslots"][batch_idx, :n_words].tolist() if "deepslots" in outputs else None
        semclass_ids = outputs["semclasses"][batch_idx, :n_words].tolist() if "semclasses" in outputs else None

        # --- –°–±–æ—Ä–∫–∞ ---
        # –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–¥–∏–º –º–∞–ø–ø–∏–Ω–≥ HEAD:REL
        # –í output –º–æ–¥–µ–ª–∏ –∏–Ω–¥–µ–∫—Å—ã 1-based (0 - —ç—Ç–æ root –∏–ª–∏ null).
        head_map = {}  # token_index (0-based) -> (head_index (1-based), rel_str)

        if self.vocab.get("ud_deprel"):
            id2rel = self.vocab["ud_deprel"]
            for arc_from, arc_to, rel_id in current_sent_deps:
                # arc_from: index of HEAD (0..N)
                # arc_to: index of DEPENDENT (1..N)
                # –í–Ω–∏–º–∞–Ω–∏–µ: –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –≤—ã–¥–∞—Ç—å arc_to > len(words), –µ—Å–ª–∏ –æ–Ω–∞ –≤—Å—Ç–∞–≤–∏–ª–∞ #NULL.
                # –ú—ã –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–≤—è–∑–∏ –∫ –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º —Ç–æ–∫–µ–Ω–∞–º.
                token_idx = arc_to - 1  # convert to 0-based
                if 0 <= token_idx < n_words:
                    rel_str = id2rel.get(rel_id, "dep")
                    head_map[token_idx] = (arc_from, rel_str)

        for i in range(n_words):
            word_text = words[i]
            char_start, char_end = token_metas[i]

            # 1. –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
            # CoBaLD –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–æ (suffix cut/append)
            lemma = word_text.lower()  # Fallback
            if lemma_rule_ids:
                rule_str = self.vocab["lemma_rule"][lemma_rule_ids[i]]
                # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å reconstruct_lemma –∏–∑ lemmatize_helper.py
                # –ù–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã: (–≤ —Ä–µ–∞–ª—å–Ω–æ–º –∫–æ–¥–µ –Ω—É–∂–µ–Ω –∏–º–ø–æ—Ä—Ç)
                lemma = self._apply_lemma_rule(word_text, rule_str)

            # 2. POS
            pos = "X"
            if joint_feats_ids:
                # Format: "UPOS#XPOS#Feats"
                tag_str = self.vocab["joint_feats"][joint_feats_ids[i]]
                pos = tag_str.split('#')[0]

            # 3. –°–∏–Ω—Ç–∞–∫—Å–∏—Å
            head_id, rel = head_map.get(i, (0, "root"))

            # 4. –°–µ–º–∞–Ω—Ç–∏–∫–∞ (Misc)
            misc = {}
            if deepslot_ids:
                slot = self.vocab["deepslot"][deepslot_ids[i]]
                if slot != "_": misc["deep_slot"] = slot

            if semclass_ids:
                s_class = self.vocab["semclass"][semclass_ids[i]]
                if s_class != "_": misc["sem_class"] = s_class

            token = Token(
                id=i + 1,
                text=word_text,
                lemma=lemma,
                pos=pos,
                head_id=head_id,
                rel=rel,
                char_start=char_start,
                char_end=char_end,
                misc=misc  # <-- –°–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ –¥–ª—è ENG-003
            )
            tokens_result.append(token)

        return tokens_result

    def _apply_lemma_rule(self, word, rule_str):
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è based on lemmatize_helper.py
        try:
            # rule format: cut_prefix|cut_suffix|append_suffix
            parts = rule_str.split('|')
            cut_prefix = int(parts[0].split('=')[1])
            cut_suffix = int(parts[1].split('=')[1])
            append_suffix = parts[2].split('=')[1]

            lemma = word[cut_prefix:]
            if cut_suffix > 0:
                lemma = lemma[:-cut_suffix]
            lemma += append_suffix
            return lemma
        except:
            return word.lower()
        