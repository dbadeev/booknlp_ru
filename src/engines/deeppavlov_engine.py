import sys
from typing import List, Dict, Optional
from razdel import sentenize, tokenize as razdel_tokenize

# –ò–º–ø–æ—Ä—Ç—ã –Ω–∞—à–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
from src.core.interfaces import BasePreprocessor
from src.core.data_structures import Token

try:
    from deeppavlov import build_model, configs
except ImportError:
    print("‚ùå DeepPavlov –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –í—ã–ø–æ–ª–Ω–∏—Ç–µ: pip install deeppavlov")
    sys.exit(1)


class DeepPavlovEngine(BasePreprocessor):
    """
    –†–µ–∞–ª–∏–∑–∞—Ü–∏—è ENG-002: DeepPavlov (RuBERT) Wrapper.
    –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ —Å–∏–º–≤–æ–ª—å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ.
    """

    def __init__(self, install: bool = False):
        print("üß† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DeepPavlov (RuBERT)...")

        # –ö–æ–Ω—Ñ–∏–≥ –∏–∑ Roadmap [cite: 89]
        self.config_name = configs.syntax.ru_syntagrus_joint_parsing

        if install:
            print("üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π DeepPavlov (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)...")
            from deeppavlov.core.commands.infer import interact_model
            from deeppavlov.core.common.file import read_json
            # –≠—Ç–æ –≤—ã–∑–æ–≤–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –≤–µ—Å–æ–≤ (~700MB+) [cite: 91]

        try:
            self.model = build_model(self.config_name, download=True)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ: python -m deeppavlov install ru_syntagrus_joint_parsing")
            raise e

        print("‚úÖ DeepPavlov Engine Ready")

    def process(self, text: str) -> List[List[Token]]:
        doc_sentences = []

        # 1. –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (Razdel - –Ω–∞—à —ç—Ç–∞–ª–æ–Ω)
        spans = list(sentenize(text))
        sent_texts = [s.text for s in spans]

        if not sent_texts:
            return []

        # 2. Batch Inference –≤ DeepPavlov
        # –ú–æ–¥–µ–ª—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ CoNLL-U —Å—Ç—Ä–æ–∫ (–æ–±—ã—á–Ω–æ)
        # –∏–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤, –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–µ—Ä—Å–∏–∏.
        # ru_syntagrus_joint_parsing –æ–±—ã—á–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
        parsed_batch = self.model(sent_texts)

        # 3. –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∏ Detokenization Mapping
        for i, dp_output in enumerate(parsed_batch):
            sent_span = spans[i]

            # –¢–æ–∫–µ–Ω—ã Razdel (–Ω–∞—à Target Grid)
            razdel_tokens = list(razdel_tokenize(sent_span.text))

            # –ü–∞—Ä—Å–∏–Ω–≥ –≤—ã—Ö–æ–¥–∞ DP (—ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–∞ CoNLL –∏–ª–∏ –æ–±—ä–µ–∫—Ç)
            dp_tokens_data = self._parse_dp_output(dp_output)

            # –°–∞–º–∞—è —Å–ª–æ–∂–Ω–∞—è —á–∞—Å—Ç—å: –°–∫–ª–µ–∏–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ DP –ø–æ–¥ Razdel
            aligned_tokens = self._align_and_merge(
                razdel_tokens=razdel_tokens,
                dp_tokens_data=dp_tokens_data,
                sent_offset=sent_span.start
            )

            doc_sentences.append(aligned_tokens)

        return doc_sentences

    def _parse_dp_output(self, output) -> List[Dict]:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—ã—Ö–æ–¥ DeepPavlov –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π.
        DeepPavlov joint_parser –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ CoNLL-U.
        """
        if isinstance(output, str):
            lines = output.strip().split('\n')
            tokens = []
            for line in lines:
                if line.startswith('#') or not line.strip():
                    continue
                parts = line.split('\t')
                if len(parts) >= 10:
                    tokens.append({
                        'text': parts[1],
                        'lemma': parts[2],
                        'pos': parts[3],
                        'head': int(parts[6]) if parts[6].isdigit() else 0,
                        'rel': parts[7]
                    })
            return tokens
        else:
            # –ï—Å–ª–∏ –≤–µ—Ä—Å–∏—è DP –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–∫–∏, –∞–¥–∞–ø—Ç–∏—Ä—É–µ–º –∑–¥–µ—Å—å
            # –î–ª—è ru_syntagrus_joint_parsing —ç—Ç–æ –æ–±—ã—á–Ω–æ string
            return []

    def _align_and_merge(self, razdel_tokens, dp_tokens_data, sent_offset) -> List[Token]:
        """
        –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞ 'detokenization mapping'.
        –ï—Å–ª–∏ DP —Ä–∞–∑–±–∏–ª —Å–ª–æ–≤–æ "–ø–æ-—Ä—É—Å—Å–∫–∏" –Ω–∞ ["–ø–æ", "-", "—Ä—É—Å—Å–∫–∏"],
        –º—ã –¥–æ–ª–∂–Ω—ã –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏—Ö –≤ –æ–¥–∏–Ω Token (–∫–∞–∫ –≤ Razdel),
        –≤—ã–±—Ä–∞–≤ –≥–ª–∞–≤–Ω–æ–≥–æ syntactic head.
        """
        result_tokens = []
        dp_cursor = 0

        # –í—Ä–µ–º–µ–Ω–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥: DP index -> Result Token index
        # –ù—É–∂–µ–Ω –¥–ª—è –ø–µ—Ä–µ—Å—á–µ—Ç–∞ head_id, —Ç–∞–∫ –∫–∞–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –º–µ–Ω—è–µ—Ç—Å—è
        dp_to_result_map = {}

        # 1. –ü—Ä–æ—Ö–æ–¥ –ø–æ —Ç–æ–∫–µ–Ω–∞–º Razdel (Target)
        for r_idx, r_tok in enumerate(razdel_tokens):
            r_text = r_tok.text

            # –°–æ–±–∏—Ä–∞–µ–º —Ç–æ–∫–µ–Ω—ã DP, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ø–∞–¥–∞—é—Ç –≤–Ω—É—Ç—Ä—å r_text
            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: —Å–æ–±–∏—Ä–∞–µ–º DP —Ç–æ–∫–µ–Ω—ã –ø–æ–∫–∞ –∏—Ö —Å–∫–ª–µ–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å r_text
            # (–í —Ä–µ–∞–ª—å–Ω–æ–π –∂–∏–∑–Ω–∏ –Ω—É–∂–Ω–æ —á–µ—Å—Ç–Ω–æ–µ —Å–∏–º–≤–æ–ª—å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ, –Ω–æ –¥–ª—è MVP —Ö–≤–∞—Ç–∏—Ç concat)

            buffer_dp_indices = []
            buffer_text = ""

            while dp_cursor < len(dp_tokens_data):
                dp_tok = dp_tokens_data[dp_cursor]

                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (—É DP –º–æ–∂–µ—Ç –±—ã—Ç—å —ë/–µ —Ä–∞–∑–ª–∏—á–∏–µ)
                # –Ω–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –∏–ª–∏ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ

                buffer_dp_indices.append(dp_cursor)
                buffer_text += dp_tok['text']
                dp_cursor += 1

                # –ï—Å–ª–∏ —Å–æ–±—Ä–∞–ª–∏ —Å–ª–æ–≤–æ —Ü–µ–ª–∏–∫–æ–º (–∏–ª–∏ –±–æ–ª—å—à–µ)
                if len(buffer_text) >= len(r_text):
                    break

            # --- Merge Logic ---
            # –£ –Ω–∞—Å –µ—Å—Ç—å N —Ç–æ–∫–µ–Ω–æ–≤ DeepPavlov, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç 1 —Ç–æ–∫–µ–Ω—É Razdel.
            # –ü—Ä–∏–º–µ—Ä: Razdel="–ø–æ-—Ä—É—Å—Å–∫–∏", DP=["–ø–æ", "-", "—Ä—É—Å—Å–∫–∏"]

            # –í—ã–±–∏—Ä–∞–µ–º "–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—è" –¥–ª—è POS –∏ Syntax.
            # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –±–µ—Ä–µ–º —Ç–æ–∫–µ–Ω, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è ROOT-–æ–º –¥–ª—è —ç—Ç–æ–π –≥—Ä—É–ø–ø—ã
            # (—Ç.–µ. –Ω–∞ –∫–æ—Ç–æ—Ä—ã–π —Å—Å—ã–ª–∞—é—Ç—Å—è –¥—Ä—É–≥–∏–µ, –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π/–ø–µ—Ä–≤—ã–π)

            # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –±–µ—Ä–µ–º –ü–û–°–õ–ï–î–ù–ò–ô –∑–Ω–∞—á–∞—â–∏–π —Ç–æ–∫–µ–Ω (—á–∞—Å—Ç–æ –∫–æ—Ä–µ–Ω—å –≤ —Å—É—Ñ—Ñ–∏–∫—Å–µ)
            # –∏–ª–∏ –ü–ï–†–í–´–ô. DeepPavlov –æ–±—ã—á–Ω–æ —Å—Ç–∞–≤–∏—Ç head –Ω–∞ –≥–ª–∞–≤–Ω–æ–µ —Å–ª–æ–≤–æ.

            # –í–æ–∑—å–º–µ–º –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω –∏–∑ –≥—Ä—É–ø–ø—ã –∫–∞–∫ –æ—Å–Ω–æ–≤—É, –Ω–æ –µ—Å–ª–∏ –≥—Ä—É–ø–ø–∞ > 1,
            # —ç—Ç–æ —Å–∏–≥–Ω–∞–ª "Tokenization Mismatch"[cite: 102].

            if not buffer_dp_indices:
                continue  # Edge case

            main_dp_idx = buffer_dp_indices[0]
            # –ú–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å: –Ω–∞–π—Ç–∏ —Ç–æ–∫–µ–Ω –≤ –≥—Ä—É–ø–ø–µ, —É –∫–æ—Ç–æ—Ä–æ–≥–æ head –ª–µ–∂–∏—Ç –í–ù–ï –≥—Ä—É–ø–ø—ã

            dp_token = dp_tokens_data[main_dp_idx]

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –≤—Å–µ—Ö "—Å—ä–µ–¥–µ–Ω–Ω—ã—Ö" —Ç–æ–∫–µ–Ω–æ–≤ DP -> —Ç–µ–∫—É—â–∏–π r_idx
            for dpi in buffer_dp_indices:
                dp_to_result_map[dpi] = r_idx + 1  # 1-based output index

            token = Token(
                id=r_idx + 1,
                text=r_text,
                lemma=dp_token['lemma'],  # –õ–µ–º–º–∞ –æ—Ç DP
                pos=dp_token['pos'],  # POS –æ—Ç DP
                head_id=dp_token['head'],  # –°—Ç–∞—Ä—ã–π head (–ø–æ–∫–∞ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π)
                rel=dp_token['rel'],
                char_start=sent_offset + r_tok.start,
                char_end=sent_offset + r_tok.stop
            )
            result_tokens.append(token)

        # 2. –í—Ç–æ—Ä–æ–π –ø—Ä–æ—Ö–æ–¥: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ HEAD
        # –¢–∞–∫ –∫–∞–∫ –º—ã —Å–∫–ª–µ–∏–ª–∏ —Ç–æ–∫–µ–Ω—ã, –∏–Ω–¥–µ–∫—Å—ã –∏–∑–º–µ–Ω–∏–ª–∏—Å—å. –ù—É–∂–Ω–æ –ø–µ—Ä–µ–∞–¥—Ä–µ—Å–æ–≤–∞—Ç—å head_id.
        for token in result_tokens:
            old_head = token.head_id

            if old_head == 0:
                token.head_id = 0
            elif old_head in dp_to_result_map:
                # –ü–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ ID –Ω–æ–≤–æ–≥–æ —Å–∫–ª–µ–µ–Ω–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
                token.head_id = dp_to_result_map[old_head]

                # –ó–∞—â–∏—Ç–∞ –æ—Ç self-loop (–µ—Å–ª–∏ head —É–∫–∞–∑—ã–≤–∞–ª –Ω–∞ —Å–æ—Å–µ–¥–∞, —Å –∫–æ—Ç–æ—Ä—ã–º –º—ã —Å–∫–ª–µ–∏–ª–∏—Å—å)
                if token.head_id == token.id:
                    # –¢–∞–∫–æ–µ –±—ã–≤–∞–µ—Ç –ø—Ä–∏ –º–µ—Ä–¥–∂–µ. –ò—â–µ–º "–≤–Ω–µ—à–Ω—é—é" —Å–≤—è–∑—å?
                    # –î–ª—è MVP —Å—Ç–∞–≤–∏–º 0 –∏–ª–∏ –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ root –≥—Ä—É–ø–ø—ã.
                    # –û–±—ã—á–Ω–æ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç, —á—Ç–æ –º—ã –≤–∑—è–ª–∏ –∑–∞–≤–∏—Å–∏–º—ã–π —Ç–æ–∫–µ–Ω –∫–∞–∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—è.
                    token.rel = "flat:merged"  # –ü–æ–º–µ—Ç–∫–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            else:
                # –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∞ –≤–µ–¥–µ—Ç –≤ –Ω–∏–∫—É–¥–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, DP —Ç–æ–∫–µ–Ω –±—ã–ª –ø—Ä–æ–ø—É—â–µ–Ω), –æ–±–Ω—É–ª—è–µ–º
                token.head_id = 0

        return result_tokens
    