# src/engines/natasha_engine.py
from typing import List
from natasha import (
    Segmenter,
    NewsEmbedding,
    NewsMorphTagger,
    NewsSyntaxParser,
    Doc,
    MorphVocab
)
from src.core.interfaces import BasePreprocessor
from src.core.data_structures import Token


class NatashaPreprocessor(BasePreprocessor):
    def __init__(self):
        print("üèóÔ∏è –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π Natasha (Slovnet)...")
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (–∫–∞–∫ –≤ –≤–∞—à–µ–º —Å–∫—Ä–∏–ø—Ç–µ)
        self.segmenter = Segmenter()
        self.emb = NewsEmbedding()
        self.morph_tagger = NewsMorphTagger(self.emb)
        self.syntax_parser = NewsSyntaxParser(self.emb)
        self.morph_vocab = MorphVocab()  # –ù—É–∂–µ–Ω –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏
        print("‚úÖ Natasha –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ")

    def process(self, text: str) -> List[List[Token]]:
        doc = Doc(text)

        # 1. –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è (Razdel) - –∑–¥–µ—Å—å –ø–æ—è–≤–ª—è—é—Ç—Å—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã start/stop!
        doc.segment(self.segmenter)

        # 2. –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è
        doc.tag_morph(self.morph_tagger)

        # 3. –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è (Slovnet —Å–∞–º –Ω–µ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç, –Ω—É–∂–Ω–æ –ø—Ä–æ–≥–æ–Ω—è—Ç—å —á–µ—Ä–µ–∑ vocab)
        for token in doc.tokens:
            token.lemmatize(self.morph_vocab)

        # 4. –°–∏–Ω—Ç–∞–∫—Å–∏—Å
        doc.parse_syntax(self.syntax_parser)

        output_sentences = []

        # Natasha —Ö—Ä–∞–Ω–∏—Ç —Ç–æ–∫–µ–Ω—ã –ø–ª–æ—Å–∫–∏–º —Å–ø–∏—Å–∫–æ–º, –Ω–æ –∏–º–µ–µ—Ç spans –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        # –ú—ã –±—É–¥–µ–º –∏—Ç–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º –∏–∑ doc.sents

        for sent in doc.sents:
            converted_sent = []

            # –í–Ω—É—Ç—Ä–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Ç–æ–∫–µ–Ω—ã –∏–º–µ—é—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ ID, –Ω–æ –Ω–∞–º –Ω—É–∂–Ω—ã –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
            # Natasha Token API: token.start, token.stop - —ç—Ç–æ —Å–º–µ—â–µ–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–∞—á–∞–ª–∞ –¢–ï–ö–°–¢–ê

            for idx, n_token in enumerate(sent.tokens, 1):
                # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ head_id (–∏–∑ –≤–∞—à–µ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞)
                head_id = self._parse_head_id(n_token.head_id)

                # –û–±—Ä–∞–±–æ—Ç–∫–∞ Root (Natasha –º–æ–∂–µ—Ç —Å—Ç–∞–≤–∏—Ç—å head_id=id –¥–ª—è root –∏–ª–∏ 0)
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç UD: head=0 –¥–ª—è root.
                # –ü—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ id == head_id, —Ç–æ —ç—Ç–æ –æ—à–∏–±–∫–∞ —Ü–∏–∫–ª–∞ (–∫—Ä–æ–º–µ root),
                # –Ω–æ –æ–±—ã—á–Ω–æ Slovnet —Å—Ç–∞–≤–∏—Ç rel='root'

                token = Token(
                    id=idx,
                    text=n_token.text,
                    lemma=n_token.lemma if n_token.lemma else n_token.text.lower(),
                    pos=n_token.pos if n_token.pos else "X",
                    head_id=head_id,
                    rel=n_token.rel if n_token.rel else "root",

                    # –ì–õ–ê–í–ù–û–ï: –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏–∑ Razdel
                    char_start=n_token.start,
                    char_end=n_token.stop
                )
                converted_sent.append(token)

            output_sentences.append(converted_sent)

        return output_sentences

    def _parse_head_id(self, head_id) -> int:
        """–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –∏–∑ –≤–∞—à–µ–≥–æ parse_syntagrus.py"""
        if not head_id:
            return 0
        try:
            # Natasha –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å "1_5" (sent_token), –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ token part
            if '_' in str(head_id):
                parts = str(head_id).split('_')
                return int(parts[-1])
            return int(head_id)
        except ValueError:
            return 0
        