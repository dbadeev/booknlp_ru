import torch
from typing import List, Dict, Optional
from razdel import sentenize, tokenize as razdel_tokenize

# –ò–º–ø–æ—Ä—Ç—ã –Ω–∞—à–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
from src.core.interfaces import BasePreprocessor
from src.core.data_structures import Token

# –ò–º–ø–æ—Ä—Ç—ã CoBaLD (–∏–∑ –≤–∞—à–∏—Ö —Ñ–∞–π–ª–æ–≤)
#
from src.cobald_parser.modeling_parser import CobaldParser
from src.cobald_parser.configuration import CobaldParserConfig


class CobaldEngine(BasePreprocessor):
    """
    –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è CoBaLD Parser –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ BookNLP-ru.
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—Å –∏ —Å–µ–º–∞–Ω—Ç–∏–∫—É (DeepSlots, SemClasses).
    """

    def __init__(self, model_path: str = "CoBaLD/xlm-roberta-base-cobald-parser-ru", device: str = None):
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device

        print(f"üß† Loading CoBaLD form {model_path} on {self.device}...")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –º–æ–¥–µ–ª–∏
        self.config = CobaldParserConfig.from_pretrained(model_path)
        self.model = CobaldParser.from_pretrained(model_path, config=self.config)
        self.model.to(self.device)
        self.model.eval()

        # –ö—ç—à–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è (ID -> Str)
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–ª–æ–≤–∞—Ä–µ–π –æ–ø–∏—Å–∞–Ω–∞ –≤
        self.vocab = self.config.vocabulary

        print("‚úÖ CoBaLD Engine Ready")

    def process(self, text: str) -> List[List[Token]]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞–π–ø–ª–∞–π–Ω–∞.
        1. –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è (Razdel) -> –ü–æ–ª—É—á–µ–Ω–∏–µ char_start/end
        2. –ò–Ω—Ñ–µ—Ä–µ–Ω—Å CoBaLD
        3. –ú–∞–ø–ø–∏–Ω–≥ —Ç–µ–Ω–∑–æ—Ä–æ–≤ –≤ –æ–±—ä–µ–∫—Ç—ã Token
        """
        doc_sentences = []

        # 1. –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (Razdel)
        for sent_span in sentenize(text):
            # 2. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (Razdel)
            # –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º Razdel –∫–∞–∫ –∏—Å—Ç–æ—á–Ω–∏–∫ "–∏—Å—Ç–∏–Ω—ã" –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
            razdel_tokens = list(razdel_tokenize(sent_span.text))
            if not razdel_tokens:
                continue

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–ª–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
            words = [t.text for t in razdel_tokens]

            # –†–∞—Å—á–µ—Ç –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
            # Razdel –¥–∞–µ—Ç —Å–º–µ—â–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –Ω–∞–º –Ω—É–∂–Ω–æ –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—Å—Ç–∞
            token_metas = []
            for t in razdel_tokens:
                abs_start = sent_span.start + t.start
                abs_end = sent_span.start + t.stop
                token_metas.append((abs_start, abs_end))

            # 3. –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏
            # CoBaLD –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –±–∞—Ç—á —Å–ø–∏—Å–∫–æ–≤ —Å–ª–æ–≤: [["–ú–∞–º–∞", "–º—ã–ª–∞"...]]
            #
            with torch.no_grad():
                outputs = self.model(
                    words=[words],
                    inference_mode=True
                )

            # 4. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ (—Ç–µ–Ω–∑–æ—Ä—ã -> —Ç–æ–∫–µ–Ω—ã)
            tokens = self._decode_sentence_batch(outputs, words, token_metas, batch_idx=0)
            doc_sentences.append(tokens)

        return doc_sentences

    def _decode_sentence_batch(self, outputs: dict, words: List[str], token_metas: List[tuple], batch_idx: int) -> List[
        Token]:
        """
        –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Å—ã—Ä—ã–µ –≤—ã—Ö–æ–¥—ã –º–æ–¥–µ–ª–∏ (Logits/Indices) –≤ –æ–±—ä–µ–∫—Ç—ã Token.
        –õ–æ–≥–∏–∫–∞ –ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∏–∑
        """
        n_words = len(words)
        result_tokens = []

        # --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –∏–∑ —Ç–µ–Ω–∑–æ—Ä–æ–≤ ---

        # 1. Lemma Rules
        lemma_rule_ids = None
        if "lemma_rules" in outputs:
            lemma_rule_ids = outputs["lemma_rules"][batch_idx, :n_words].tolist()

        # 2. POS & Feats (Joint)
        joint_feats_ids = None
        if "joint_feats" in outputs:
            joint_feats_ids = outputs["joint_feats"][batch_idx, :n_words].tolist()

        # 3. –°–µ–º–∞–Ω—Ç–∏–∫–∞ (Misc / DeepSlots / SemClasses) [–ó–∞–¥–∞—á–∞ ENG-003]
        deepslot_ids = outputs["deepslots"][batch_idx, :n_words].tolist() if "deepslots" in outputs else None
        semclass_ids = outputs["semclasses"][batch_idx, :n_words].tolist() if "semclasses" in outputs else None

        # 4. –°–∏–Ω—Ç–∞–∫—Å–∏—Å (Deps UD)
        # deps_ud shape: [N_arcs, 4] -> [batch_idx, head, dep, rel_id]
        #
        deps_ud = outputs.get("deps_ud")
        head_map = {}  # dep_idx (0-based) -> (head_idx (1-based), rel_str)

        if deps_ud is not None:
            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥—É–≥–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (batch_idx)
            current_arcs = deps_ud[deps_ud[:, 0] == batch_idx]

            id2rel = self.vocab.get("ud_deprel", {})

            for arc in current_arcs:
                head_idx = int(arc[1])  # 0 is ROOT
                dep_idx = int(arc[2])  # 1-based index of word
                rel_id = int(arc[3])

                rel_str = id2rel.get(rel_id, "dep")

                # dep_idx - 1, —Ç.–∫. –≤ tokens –º—ã –∏–¥–µ–º —Å 0, –∞ –º–æ–¥–µ–ª—å —Å—á–∏—Ç–∞–µ—Ç —Å 1 (0=CLS)
                #
                token_idx = dep_idx - 1

                if 0 <= token_idx < n_words:
                    head_map[token_idx] = (head_idx, rel_str)

        # --- –°–±–æ—Ä–∫–∞ –¢–æ–∫–µ–Ω–æ–≤ ---

        for i in range(n_words):
            word_text = words[i]
            char_start, char_end = token_metas[i]

            # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
            lemma = word_text.lower()  # Fallback
            if lemma_rule_ids:
                rule_str = self.vocab["lemma_rule"][lemma_rule_ids[i]]
                lemma = self._apply_lemma_rule(word_text, rule_str)

            # POS
            pos = "X"
            feats = {}
            if joint_feats_ids:
                # Format: UPOS#XPOS#Feats
                val = self.vocab["joint_feats"][joint_feats_ids[i]]
                parts = val.split('#')
                pos = parts[0]
                # –ú–æ–∂–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å feats (parts[2]), –µ—Å–ª–∏ –Ω—É–∂–Ω–æ

            # –°–∏–Ω—Ç–∞–∫—Å–∏—Å
            head_id, rel = head_map.get(i, (0, "root"))

            # –°–µ–º–∞–Ω—Ç–∏–∫–∞ (–ó–∞–ø–æ–ª–Ω—è–µ–º misc)
            misc = {}

            if deepslot_ids:
                slot = self.vocab["deepslot"][deepslot_ids[i]]
                if slot != "_":
                    misc["deep_slot"] = slot  # –ù–∞–ø—Ä–∏–º–µ—Ä: "Agent", "Experiencer"

            if semclass_ids:
                s_class = self.vocab["semclass"][semclass_ids[i]]
                if s_class != "_":
                    misc["sem_class"] = s_class  # –ù–∞–ø—Ä–∏–º–µ—Ä: "Person", "Event"

            token = Token(
                id=i + 1,
                text=word_text,
                lemma=lemma,
                pos=pos,
                head_id=head_id,
                rel=rel,
                char_start=char_start,
                char_end=char_end,
                misc=misc
            )
            result_tokens.append(token)

        return result_tokens

    def _apply_lemma_rule(self, word: str, rule_str: str) -> str:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø—Ä–∞–≤–∏–ª–æ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏.
        –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –∏–∑
        Format: cut_prefix=0|cut_suffix=1|append_suffix=–∞
        """
        try:
            # –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ –ø—Ä–∞–≤–∏–ª–∞
            # –ü—Ä–∏–º–µ—Ä: "0|1|–∞" –∏–ª–∏ "cut_prefix=0|..." (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≤–µ—Ä—Å–∏–∏ vocab)
            # –í provided file lemmatize_helper.py —Ñ–æ—Ä–º–∞—Ç —Å–ª–æ–∂–Ω—ã–π, –Ω–æ vocab –æ–±—ã—á–Ω–æ —Ö—Ä–∞–Ω–∏—Ç —É–∂–µ values
            # –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º —Ñ–æ—Ä–º–∞—Ç –∏–∑ lemmatize_helper.py: keys like "cut_prefix=..."

            params = {}
            for part in rule_str.split('|'):
                key, val = part.split('=')
                params[key] = val

            cut_prefix = int(params.get('cut_prefix', 0))
            cut_suffix = int(params.get('cut_suffix', 0))
            append_suffix = params.get('append_suffix', '')

            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ
            lemma = word[cut_prefix:]
            if cut_suffix > 0:
                lemma = lemma[:-cut_suffix]
            lemma += append_suffix

            return lemma
        except Exception:
            # Fallback –µ—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è
            return word.lower()