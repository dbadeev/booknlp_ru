#!/usr/bin/env python3
"""
–õ–æ–∫–∞–ª—å–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è spaCy –ø–∞—Ä—Å–µ—Ä–∞.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    from spacy_wrapper import SpacyParser, SpacyPymorphy3Parser

    # –ë–∞–∑–æ–≤—ã–π –ø–∞—Ä—Å–µ—Ä (—Ç–æ–ª—å–∫–æ spaCy)
    parser = SpacyParser()

    # 4 –≤–∞—Ä–∏–∞–Ω—Ç–∞ –≤—ã–≤–æ–¥–∞ (2 —Ñ–æ—Ä–º–∞—Ç–∞ √ó 2 —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞):
    result = parser.parse_text(text, output_format="native",  tokenizer="internal")
    result = parser.parse_text(text, output_format="native",  tokenizer="razdel")
    result = parser.parse_text(text, output_format="conllu",  tokenizer="internal")
    result = parser.parse_text(text, output_format="conllu",  tokenizer="razdel")

    # –û–±–æ–≥–∞—â—ë–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä (spaCy + pymorphy3), —Ç–æ–ª—å–∫–æ native —Ñ–æ—Ä–º–∞—Ç
    parser_enriched = SpacyPymorphy3Parser()
    result = parser_enriched.parse_text_enriched(
        text,
        tokenizer="razdel",
        include_lexeme=True,
        include_all_parses=True
    )
"""
import logging
import sys
import modal
from typing import List, Dict, Any, Union, Literal

logger = logging.getLogger(__name__)

OutputFormat = Literal["native", "conllu"]
TokenizerType = Literal["internal", "razdel"]


class SpacyParser:
    """
    –ö–ª–∏–µ–Ω—Ç –¥–ª—è spaCy –ø–∞—Ä—Å–µ—Ä–∞, –∑–∞–ø—É—â–µ–Ω–Ω–æ–≥–æ –≤ Modal.

    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - 2 —Ñ–æ—Ä–º–∞—Ç–∞ –≤—ã–≤–æ–¥–∞: native (–ø–æ–ª–Ω—ã–π), conllu (—Å—Ç–∞–Ω–¥–∞—Ä—Ç UD)
    - 2 —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞:   internal (spaCy), razdel (–≤–Ω–µ—à–Ω–∏–π)
    """

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        try:
            self.service = modal.Cls.from_name(
                "booknlp-ru-spacy",
                "SpacyService"
            )()
            self.logger.info("‚úì Connected to SpaCy via Modal.")
        except Exception as e:
            self.logger.error(f"‚ùå Failed to connect to Modal: {e}")
            raise

    def parse_text(
        self,
        text: str,
        output_format: OutputFormat = "native",
        tokenizer: TokenizerType = "internal"
    ) -> Union[List[Dict[str, Any]], str]:
        """
        –ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º spaCy.

        Args:
            text:          –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            output_format: –§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞
                - 'native':  –ø–æ–ª–Ω—ã–π –Ω–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç spaCy (–í–°–ï –ø–æ–ª—è —Ç–æ–∫–µ–Ω–∞)
                - 'conllu':  —Å—Ç—Ä–æ–∫–∞ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ CoNLL-U
            tokenizer: –¢–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
                - 'internal': –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä spaCy
                - 'razdel':   –≤–Ω–µ—à–Ω–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä razdel

        Returns:
            native  ‚Üí List[Dict]  (–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å–æ –≤—Å–µ–º–∏ –ø–æ–ª—è–º–∏ spaCy)
            conllu  ‚Üí str

        –ü–æ–ª—è —Ç–æ–∫–µ–Ω–∞ –≤ native —Ñ–æ—Ä–º–∞—Ç–µ:
        ‚îÄ‚îÄ –ü–æ–∑–∏—Ü–∏—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
          id, start_char, end_char
        ‚îÄ‚îÄ –§–æ—Ä–º–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
          form, norm, lower, shape
        ‚îÄ‚îÄ –õ–µ–º–º–∞ –∏ POS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
          lemma, upos, xpos, feats
        ‚îÄ‚îÄ –°–∏–Ω—Ç–∞–∫—Å–∏—Å ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
          head, deprel, n_lefts, n_rights, children
        ‚îÄ‚îÄ –ò–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
          ent_type, ent_iob
        ‚îÄ‚îÄ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
          is_sent_start, whitespace, misc (SpaceAfter=No)
        ‚îÄ‚îÄ –õ–µ–∫—Å–∏—á–µ—Å–∫–∏–µ —Ñ–ª–∞–≥–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
          is_alpha, is_digit, is_punct, is_space,
          is_stop, is_oov, like_num, like_url, like_email
        ‚îÄ‚îÄ –í–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø–æ–ª—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
          has_vector, cluster
        ‚îÄ‚îÄ –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        –ò–°–ö–õ–Æ–ß–ï–ù–û –ò–ó-–ó–ê –ù–ï–ò–ù–§–û–†–ú–ê–¢–ò–í–ù–û–°–¢–ò
        #   prob
        # ‚îÄ‚îÄ –û–±–æ–≥–∞—â–µ–Ω–∏–µ pymorphy3 (—Ç–æ–ª—å–∫–æ SpacyPymorphy3Parser)
        #   pymorphy3_word, pymorphy3_lemma, pymorphy3_tag,
        #   pymorphy3_score, pymorphy3_is_known,
        #   pymorphy3_methods_stack, pymorphy3_normalized,
        #   pymorphy3_lexeme / pymorphy3_lexeme_count,
        #   pymorphy3_all_parses / pymorphy3_parses_count
        """
        try:
            return self.service.parse.remote(
                text,
                output_format=output_format,
                tokenizer=tokenizer
            )
        except Exception as e:
            self.logger.error(f"‚ùå Error during spaCy parsing: {e}")
            raise

    def parse_batch(
        self,
        texts: List[str],
        output_format: OutputFormat = "native",
        tokenizer: TokenizerType = "internal",
        batch_size: int = 32
    ) -> List[Union[List[Dict[str, Any]], str]]:
        """
        –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤.

        Args:
            texts:         –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            output_format: –§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞
            tokenizer:     –¢–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
            batch_size:    –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 32)
        """
        try:
            return self.service.parse_batch.remote(
                texts,
                output_format=output_format,
                tokenizer=tokenizer,
                batch_size=batch_size
            )
        except Exception as e:
            self.logger.error(f"‚ùå Error during batch parsing: {e}")
            raise


# ============================================================
# –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° PYMORPHY3
# ============================================================
class SpacyPymorphy3Parser:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–∞—Ä—Å–µ—Ä–∞: spaCy (–í–°–ï –ø–æ–ª—è) + pymorphy3 (–æ–±–æ–≥–∞—â–µ–Ω–∏–µ).

    –¢–æ–ª—å–∫–æ —Ñ–æ—Ä–º–∞—Ç native ‚Äî –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–º–æ –∫ conllu.

    –ü–æ–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è pymorphy3:
    ‚îÄ‚îÄ –ë–∞–∑–æ–≤—ã–µ (–≤—Å–µ–≥–¥–∞) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      pymorphy3_word:         —Ñ–æ—Ä–º–∞ –≤ lowercase (p.word)
      pymorphy3_lemma:        –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ (p.normal_form)
      pymorphy3_tag:          –ø–æ–ª–Ω—ã–π —Ç–µ–≥ OpenCorpora (str(p.tag))
      pymorphy3_score:        –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–∞–∑–±–æ—Ä–∞ (p.score)
      pymorphy3_is_known:     —Å–ª–æ–≤–æ –≤ —Å–ª–æ–≤–∞—Ä–µ (p.is_known)
    ‚îÄ‚îÄ –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ (–≤—Å–µ–≥–¥–∞) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      pymorphy3_methods_stack: –º–µ—Ç–æ–¥—ã —Ä–∞–∑–±–æ—Ä–∞ (p.methods_stack)
      pymorphy3_normalized:   {word, tag, score} –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º—ã
    ‚îÄ‚îÄ –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      pymorphy3_lexeme:       –ø–æ–ª–Ω–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ [{word, tag}, ...]
                              (–µ—Å–ª–∏ include_lexeme=True)
      pymorphy3_lexeme_count: –∫–æ–ª-–≤–æ —Ñ–æ—Ä–º –ø–∞—Ä–∞–¥–∏–≥–º—ã
                              (–µ—Å–ª–∏ include_lexeme=False)
      pymorphy3_all_parses:   –≤—Å–µ —Ä–∞–∑–±–æ—Ä—ã [{normal_form, tag, score, is_known}, ...]
                              (–µ—Å–ª–∏ include_all_parses=True –∏ –∫–æ–ª-–≤–æ —Ä–∞–∑–±–æ—Ä–æ–≤ > 1)
      pymorphy3_parses_count: –∫–æ–ª-–≤–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ä–∞–∑–±–æ—Ä–æ–≤
                              (–µ—Å–ª–∏ include_all_parses=True)

    –ö–ª—é—á–µ–≤—ã–µ –æ—Ç–ª–∏—á–∏—è spaCy vs pymorphy3:
      form   (spaCy)        ‚â†  pymorphy3_word   ‚Äî —Ä–µ–≥–∏—Å—Ç—Ä vs lowercase
      lemma  (spaCy)        ‚â†  pymorphy3_lemma  ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç vs –ø–µ—Ä–≤—ã–π —Ä–∞–∑–±–æ—Ä
      feats  (spaCy/UD)     ‚â†  pymorphy3_tag    ‚Äî UD —Ñ–æ—Ä–º–∞—Ç vs OpenCorpora
    """

    def __init__(self):
        import pymorphy3
        self.spacy_parser = SpacyParser()
        self.morph = pymorphy3.MorphAnalyzer()
        self.logger = logging.getLogger(__name__)
        self.logger.info("‚úì SpaCy+Pymorphy3 parser initialized.")

    def parse_text_enriched(
        self,
        text: str,
        tokenizer: TokenizerType = "internal",
        include_lexeme: bool = False,
        include_all_parses: bool = False
    ) -> List[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç –∏ –æ–±–æ–≥–∞—â–∞–µ—Ç –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ pymorphy3.

        Args:
            text:               –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
            tokenizer:          –¢–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ('internal' | 'razdel')
            include_lexeme:     –í–∫–ª—é—á–∞—Ç—å –ø–æ–ª–Ω—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É —Å–ª–æ–≤–∞
            include_all_parses: –í–∫–ª—é—á–∞—Ç—å –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ä–∞–∑–±–æ—Ä–∞ (–¥–ª—è –æ–º–æ–Ω–∏–º–æ–≤)

        Returns:
            List[Dict] ‚Äî –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ native —Ñ–æ—Ä–º–∞—Ç–µ —Å–æ –≤—Å–µ–º–∏ –ø–æ–ª—è–º–∏ spaCy
            –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–æ–ª—è–º–∏ pymorphy3_*
        """
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π native —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç spaCy
        spacy_result = self.spacy_parser.parse_text(
            text,
            output_format="native",
            tokenizer=tokenizer
        )

        # –û–±–æ–≥–∞—â–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –¥–∞–Ω–Ω—ã–º–∏ pymorphy3
        enriched_result = []
        for sent in spacy_result:
            enriched_sent = sent.copy()
            enriched_words = []

            for token_dict in sent.get("words", []):
                form = token_dict.get("form", "")
                all_parses = self.morph.parse(form)
                p = all_parses[0]  # –ù–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Ä–∞–∑–±–æ—Ä

                token_enriched = token_dict.copy()

                # ‚îÄ‚îÄ –ë–∞–∑–æ–≤—ã–µ –ø–æ–ª—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                token_enriched.update({
                    "pymorphy3_word":     p.word,          # –≤—Å–µ–≥–¥–∞ lowercase!
                    "pymorphy3_lemma":    p.normal_form,
                    "pymorphy3_tag":      str(p.tag),
                    "pymorphy3_score":    p.score,
                    "pymorphy3_is_known": p.is_known,
                })

                # ‚îÄ‚îÄ –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–æ–ª—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                token_enriched["pymorphy3_methods_stack"] = p.methods_stack
                token_enriched["pymorphy3_normalized"] = {
                    "word":  p.normalized.word,
                    "tag":   str(p.normalized.tag),
                    "score": p.normalized.score,
                }

                # ‚îÄ‚îÄ –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø–æ–ª—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                if include_lexeme:
                    token_enriched["pymorphy3_lexeme"] = [
                        {"word": lf.word, "tag": str(lf.tag)}
                        for lf in p.lexeme
                    ]
                else:
                    token_enriched["pymorphy3_lexeme_count"] = len(p.lexeme)

                if include_all_parses:
                    token_enriched["pymorphy3_parses_count"] = len(all_parses)
                    if len(all_parses) > 1:
                        token_enriched["pymorphy3_all_parses"] = [
                            {
                                "normal_form": parse.normal_form,
                                "tag":         str(parse.tag),
                                "score":       parse.score,
                                "is_known":    parse.is_known,
                            }
                            for parse in all_parses
                        ]

                enriched_words.append(token_enriched)

            enriched_sent["words"] = enriched_words
            enriched_result.append(enriched_sent)

        return enriched_result


# ============================================================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò –î–õ–Ø –í–´–í–û–î–ê
# ============================================================
def _print_token_full(tok: Dict[str, Any], with_pymorphy3: bool = False):
    """–í—ã–≤–æ–¥–∏—Ç –í–°–ï –ø–æ–ª—è —Ç–æ–∫–µ–Ω–∞ –≤ —á–∏—Ç–∞–µ–º–æ–º –≤–∏–¥–µ."""
    print(f"\n  ‚îÄ‚îÄ –¢–æ–∫–µ–Ω #{tok['id']}: '{tok['form']}' ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")

    print(f"  –ü–û–ó–ò–¶–ò–Ø:")
    print(f"    start_char:    {tok['start_char']}")
    print(f"    end_char:      {tok['end_char']}")

    print(f"  –§–û–†–ú–ê:")
    print(f"    form:          {tok['form']}")
    print(f"    norm:          {tok.get('norm', '‚Äî')}")
    print(f"    lower:         {tok.get('lower', '‚Äî')}")
    print(f"    shape:         {tok.get('shape', '‚Äî')}")

    print(f"  –õ–ï–ú–ú–ê –ò POS:")
    print(f"    lemma:         {tok['lemma']}")
    print(f"    upos:          {tok['upos']}")
    print(f"    xpos:          {tok['xpos']}")
    print(f"    feats:         {tok['feats']}")

    print(f"  –°–ò–ù–¢–ê–ö–°–ò–°:")
    print(f"    head:          {tok['head']}")
    print(f"    deprel:        {tok['deprel']}")
    print(f"    n_lefts:       {tok.get('n_lefts', '‚Äî')}")
    print(f"    n_rights:      {tok.get('n_rights', '‚Äî')}")
    print(f"    children:      {tok.get('children', [])}")

    print(f"  –°–£–©–ù–û–°–¢–ò:")
    print(f"    ent_type:      {tok.get('ent_type') or '‚Äî'}")
    print(f"    ent_iob:       {tok.get('ent_iob') or '‚Äî'}")

    print(f"  –ú–ï–¢–ê–î–ê–ù–ù–´–ï:")
    print(f"    is_sent_start: {tok.get('is_sent_start')}")
    print(f"    whitespace:    '{tok.get('whitespace', '')}'")
    print(f"    misc:          {tok.get('misc', '‚Äî')}")

    print(f"  –§–õ–ê–ì–ò:")
    print(f"    is_alpha:      {tok.get('is_alpha')}")
    print(f"    is_digit:      {tok.get('is_digit')}")
    print(f"    is_punct:      {tok.get('is_punct')}")
    print(f"    is_space:      {tok.get('is_space')}")
    print(f"    is_stop:       {tok.get('is_stop')}")
    print(f"    is_oov:        {tok.get('is_oov')}")
    print(f"    like_num:      {tok.get('like_num')}")
    print(f"    like_url:      {tok.get('like_url')}")
    print(f"    like_email:    {tok.get('like_email')}")

    print(f"  –í–ï–ö–¢–û–†:")
    print(f"    has_vector:    {tok.get('has_vector')}")
    vn = tok.get('vector_norm')
    print(f"    vector_norm:   {vn if vn is not None else '‚Äî'}")

    if with_pymorphy3:
        print(f"\n  üîç PYMORPHY3:")
        print(f"    word (lower):   {tok.get('pymorphy3_word', '‚Äî')}")
        print(f"    lemma:          {tok.get('pymorphy3_lemma', '‚Äî')}")
        print(f"    tag (OpenCorp): {tok.get('pymorphy3_tag', '‚Äî')}")
        print(f"    score:          {tok.get('pymorphy3_score', 0):.4f}")
        print(f"    is_known:       {tok.get('pymorphy3_is_known', '‚Äî')}")
        print(f"    methods_stack:  {tok.get('pymorphy3_methods_stack', '‚Äî')}")

        normalized = tok.get('pymorphy3_normalized', {})
        if normalized:
            print(f"    normalized:")
            print(f"      word:  {normalized.get('word')}")
            print(f"      tag:   {normalized.get('tag')}")
            print(f"      score: {normalized.get('score', 0):.4f}")

        if 'pymorphy3_lexeme' in tok:
            lexeme = tok['pymorphy3_lexeme']
            print(f"    lexeme ({len(lexeme)} —Ñ–æ—Ä–º):")
            for lf in lexeme[:5]:
                print(f"      {lf['word']:<15} [{lf['tag']}]")
            if len(lexeme) > 5:
                print(f"      ... (–µ—â–µ {len(lexeme)-5} —Ñ–æ—Ä–º)")
        else:
            print(f"    lexeme_count:   {tok.get('pymorphy3_lexeme_count', '‚Äî')}")

        if 'pymorphy3_all_parses' in tok:
            parses = tok['pymorphy3_all_parses']
            print(f"    all_parses ({tok.get('pymorphy3_parses_count')}):")
            for i, parse in enumerate(parses, 1):
                print(f"      {i}. {parse['normal_form']:<12} "
                      f"[{parse['tag']:<30}] score={parse['score']:.4f}")
        elif 'pymorphy3_parses_count' in tok:
            print(f"    parses_count:   {tok.get('pymorphy3_parses_count')}")

        # –ö–ª—é—á–µ–≤—ã–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è
        if tok.get('form') != tok.get('pymorphy3_word'):
            print(f"\n    ‚ö†Ô∏è  –†–ï–ì–ò–°–¢–†: '{tok['form']}' vs '{tok['pymorphy3_word']}'")
        if tok.get('lemma') != tok.get('pymorphy3_lemma'):
            print(f"    ‚ö†Ô∏è  –õ–ï–ú–ú–ê:   spaCy='{tok['lemma']}' vs "
                  f"pymorphy3='{tok['pymorphy3_lemma']}'")


# ============================================================
# –¢–ï–°–¢–´
# ============================================================
if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # ‚îÄ‚îÄ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Modal ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    print("=" * 80)
    print("–ü–†–û–í–ï–†–ö–ê –î–û–°–¢–£–ü–ù–û–°–¢–ò MODAL-–°–ï–†–í–ò–°–ê")
    print("=" * 80)
    try:
        parser = SpacyParser()
    except Exception as e:
        print(f"‚ö†Ô∏è  Modal-—Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        print("\n–ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–∏—Å –∫–æ–º–∞–Ω–¥–æ–π:")
        print("  modal deploy src/parsers/spacy_modal.py")
        print("–∏–ª–∏ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏:")
        print("  modal serve src/parsers/spacy_modal.py")
        sys.exit(1)

    parser_enriched = SpacyPymorphy3Parser()

    # –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã
    test_short   = "–ö—Ä—É–∂–∫–∞-—Ç–µ—Ä–º–æ—Å —Å—Ç–æ–∏—Ç 500—Ä."
    test_ner     = "–ú–æ—Å–∫–≤–∞ - —Å—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏."
    test_ambig   = "–º–æ–π –±—Ä–∞—Ç"
    test_complex = "–ó–ª–æ, –∫–æ—Ç–æ—Ä—ã–º –ø—É–≥–∞–µ—à—å, –Ω–µ —Ç–∞–∫ –∑–ª–æ."

    # ========================================================================
    # –í–ê–†–ò–ê–ù–¢ 1: native + internal tokenizer
    # ========================================================================
    print("\n" + "=" * 80)
    print("–í–ê–†–ò–ê–ù–¢ 1: NATIVE + INTERNAL TOKENIZER")
    print("=" * 80)

    result_1 = parser.parse_text(
        test_short,
        output_format="native",
        tokenizer="internal"
    )

    print(f"\n–¢–µ–∫—Å—Ç: '{test_short}'")
    for sent_data in result_1:
        print(f"\n–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: '{sent_data['text']}'")
        if sent_data.get("entities"):
            print(f"–°—É—â–Ω–æ—Å—Ç–∏: {[(e['text'], e['label']) for e in sent_data['entities']]}")
        for tok in sent_data["words"]:
            _print_token_full(tok, with_pymorphy3=False)

    # ========================================================================
    # –í–ê–†–ò–ê–ù–¢ 2: native + razdel tokenizer
    # ========================================================================
    print("\n" + "=" * 80)
    print("–í–ê–†–ò–ê–ù–¢ 2: NATIVE + RAZDEL TOKENIZER")
    print("=" * 80)

    result_2 = parser.parse_text(
        test_short,
        output_format="native",
        tokenizer="razdel"
    )

    print(f"\n–¢–µ–∫—Å—Ç: '{test_short}'")
    print("\n‚ö° –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤:")
    internal_toks = [w['form'] for s in result_1 for w in s['words']]
    razdel_toks   = [w['form'] for s in result_2 for w in s['words']]
    print(f"  Internal: {internal_toks}")
    print(f"  Razdel:   {razdel_toks}")
    if internal_toks != razdel_toks:
        print("  ‚ö†Ô∏è  –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–´ –î–ê–Æ–¢ –†–ê–ó–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´!")

    print(f"\n–í—Å–µ –ø–æ–ª—è —Ç–æ–∫–µ–Ω–æ–≤ (razdel):")
    for sent_data in result_2:
        for tok in sent_data["words"]:
            _print_token_full(tok, with_pymorphy3=False)

    # ========================================================================
    # –í–ê–†–ò–ê–ù–¢ 3: conllu + internal tokenizer
    # ========================================================================
    print("\n" + "=" * 80)
    print("–í–ê–†–ò–ê–ù–¢ 3: CONLL-U + INTERNAL TOKENIZER")
    print("=" * 80)

    result_3 = parser.parse_text(
        test_complex,
        output_format="conllu",
        tokenizer="internal"
    )
    print(f"\n–¢–µ–∫—Å—Ç: '{test_complex}'")
    print(result_3)

    # ========================================================================
    # –í–ê–†–ò–ê–ù–¢ 4: conllu + razdel tokenizer
    # ========================================================================
    print("\n" + "=" * 80)
    print("–í–ê–†–ò–ê–ù–¢ 4: CONLL-U + RAZDEL TOKENIZER")
    print("=" * 80)

    result_4 = parser.parse_text(
        test_complex,
        output_format="conllu",
        tokenizer="razdel"
    )
    print(f"\n–¢–µ–∫—Å—Ç: '{test_complex}'")
    print(result_4)

    # ========================================================================
    # –í–ê–†–ò–ê–ù–¢ 1+2 –° –û–ë–û–ì–ê–©–ï–ù–ò–ï–ú PYMORPHY3 (internal tokenizer)
    # ========================================================================
    print("\n" + "=" * 80)
    print("NATIVE + INTERNAL + PYMORPHY3 –û–ë–û–ì–ê–©–ï–ù–ò–ï")
    print("=" * 80)

    enriched_internal = parser_enriched.parse_text_enriched(
        test_ambig,
        tokenizer="internal",
        include_lexeme=False,
        include_all_parses=True
    )

    print(f"\n–¢–µ–∫—Å—Ç: '{test_ambig}' (—Ç–µ—Å—Ç –Ω–∞ –æ–º–æ–Ω–∏–º—ã)")
    for sent_data in enriched_internal:
        for tok in sent_data["words"]:
            _print_token_full(tok, with_pymorphy3=True)

    # ========================================================================
    # –í–ê–†–ò–ê–ù–¢ 1+2 –° –û–ë–û–ì–ê–©–ï–ù–ò–ï–ú PYMORPHY3 (razdel tokenizer)
    # ========================================================================
    print("\n" + "=" * 80)
    print("NATIVE + RAZDEL + PYMORPHY3 –û–ë–û–ì–ê–©–ï–ù–ò–ï")
    print("=" * 80)

    enriched_razdel = parser_enriched.parse_text_enriched(
        test_ner,
        tokenizer="razdel",
        include_lexeme=True,
        include_all_parses=False
    )

    print(f"\n–¢–µ–∫—Å—Ç: '{test_ner}' (—Ç–µ—Å—Ç NER + –ø–∞—Ä–∞–¥–∏–≥–º–∞)")
    for sent_data in enriched_razdel:
        if sent_data.get("entities"):
            print(f"\n–°—É—â–Ω–æ—Å—Ç–∏:")
            for ent in sent_data["entities"]:
                print(f"  - '{ent['text']}' [{ent['label']}] "
                      f"chars:[{ent['start_char']},{ent['end_char']}]")
        for tok in sent_data["words"]:
            _print_token_full(tok, with_pymorphy3=True)

    # ========================================================================
    print("\n" + "=" * 80)
    print("‚úÖ –í–°–ï –¢–ï–°–¢–´ –ó–ê–í–ï–†–®–ï–ù–´")
    print("=" * 80)
    print("\n4 –≤–∞—Ä–∏–∞–Ω—Ç–∞ –≤—ã–≤–æ–¥–∞:")
    print("  1. native  + internal  ‚Äî –≤—Å–µ –ø–æ–ª—è spaCy")
    print("  2. native  + razdel    ‚Äî –≤—Å–µ –ø–æ–ª—è spaCy, –ª—É—á—à–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä")
    print("  3. conllu  + internal  ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π CoNLL-U")
    print("  4. conllu  + razdel    ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π CoNLL-U, –ª—É—á—à–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä")
    print("\n+ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ pymorphy3 (—Ç–æ–ª—å–∫–æ –∫ native):")
    print("  - –±–∞–∑–æ–≤—ã–µ:    word, lemma, tag, score, is_known")
    print("  - —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ: methods_stack, normalized")
    print("  - –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ: lexeme / lexeme_count, all_parses / parses_count")
