#!/usr/bin/env python3
"""
–õ–æ–∫–∞–ª—å–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è spaCy –ø–∞—Ä—Å–µ—Ä–∞.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    from spacy_wrapper import SpacyParser, SpacyPymorphy3Parser

    # –ë–∞–∑–æ–≤—ã–π –ø–∞—Ä—Å–µ—Ä (—Ç–æ–ª—å–∫–æ spaCy)
    parser = SpacyParser()
    result = parser.parse_text(text, output_format="simplified")
    result = parser.parse_text(text, output_format="native")
    result = parser.parse_text(text, output_format="conllu")

    # –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä (spaCy + pymorphy3)
    parser_enriched = SpacyPymorphy3Parser()
    result = parser_enriched.parse_text_enriched(
        text,
        output_format="simplified",
        include_lexeme=True
    )

    # –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    results = parser.parse_batch(texts, output_format="simplified")
"""
import logging
import modal
from typing import List, Dict, Any, Union, Literal, Optional

logger = logging.getLogger(__name__)

OutputFormat = Literal["simplified", "native", "conllu"]


class SpacyParser:
    """
    –ö–ª–∏–µ–Ω—Ç –¥–ª—è spaCy –ø–∞—Ä—Å–µ—Ä–∞, –∑–∞–ø—É—â–µ–Ω–Ω–æ–≥–æ –≤ Modal.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å ru_core_news_lg (CNN/Tok2Vec –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)
    –¥–ª—è –º–æ—Ä—Ñ–æ-—Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.
    """

    def __init__(self):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞.

        Raises:
            Exception: –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Modal
        """
        self.logger = logging.getLogger(__name__)

        try:
            self.service = modal.Cls.from_name(
                "booknlp-ru-spacy",
                "SpacyService"
            )()
            self.logger.info("‚úì Connected to SpaCy via Modal.")
        except Exception as e:
            self.logger.error(f"‚ùå Failed to connect to Modal: {e}")
            raise e

    def parse_text(
            self,
            text: str,
            output_format: OutputFormat = "simplified"
    ) -> Union[List[List[Dict[str, Any]]], List[Dict[str, Any]], str]:
        """
        –ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º spaCy.

        Args:
            text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            output_format: –§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞
                - 'simplified': —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (—Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π ‚Üí —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤)
                - 'native': –ø–æ–ª–Ω—ã–π –Ω–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç spaCy —Å–æ –≤—Å–µ–º–∏ –∞—Ç—Ä–∏–±—É—Ç–∞–º–∏
                - 'conllu': —Å—Ç—Ä–æ–∫–∞ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ CoNLL-U

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ

        –§–æ—Ä–º–∞—Ç—ã –≤—ã–≤–æ–¥–∞:

        simplified (List[List[Dict]]):
            [
                [  # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ 1
                    {
                        "id": 1,
                        "form": "–ö–æ–ª—è",
                        "lemma": "–∫–æ–ª—è",
                        "upos": "PROPN",
                        "xpos": "PROPN",
                        "feats": "Case=Nom|Gender=Masc|Number=Sing",
                        "head": 2,
                        "deprel": "nsubj",
                        "start_char": 0,
                        "end_char": 4
                    },
                    {...}
                ],
                [...]  # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ 2
            ]

        native (List[Dict]):
            [
                {
                    "text": "–ö–æ–ª—è —Å–∫–∞–∑–∞–ª...",
                    "start_char": 0,
                    "end_char": 20,
                    "words": [
                        {
                            "id": 1,
                            "form": "–ö–æ–ª—è",
                            "lemma": "–∫–æ–ª—è",
                            "upos": "PROPN",
                            "xpos": "PROPN",
                            "feats": "Case=Nom|Gender=Masc|Number=Sing",
                            "head": 2,
                            "deprel": "nsubj",
                            "start_char": 0,
                            "end_char": 4,
                            "ent_type": "PER",
                            "ent_iob": "B",
                            "is_sent_start": True,
                            "whitespace": " ",
                            "shape": "Xxxx",
                            "is_alpha": True,
                            "is_punct": False,
                            "like_num": False
                        },
                        {...}
                    ],
                    "entities": [
                        {
                            "text": "–ö–æ–ª—è",
                            "start": 0,
                            "end": 1,
                            "label": "PER"
                        }
                    ]
                },
                {...}
            ]

        conllu (str):
            # sent_id = 1
            # text = –ö–æ–ª—è —Å–∫–∞–∑–∞–ª...
            1	–ö–æ–ª—è	–∫–æ–ª—è	PROPN	PROPN	Case=Nom|Gender=Masc|Number=Sing	2	nsubj	_	_
            2	—Å–∫–∞–∑–∞–ª	—Å–∫–∞–∑–∞—Ç—å	VERB	VERB	...	0	root	_	SpaceAfter=No
            ...
        """
        try:
            return self.service.parse.remote(text, output_format=output_format)
        except Exception as e:
            self.logger.error(f"‚ùå Error during spaCy parsing: {e}")
            raise e

    def parse_batch(
            self,
            texts: List[str],
            output_format: OutputFormat = "simplified",
            batch_size: int = 32
    ) -> List[Union[List[List[Dict[str, Any]]], List[Dict[str, Any]], str]]:
        """
        –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            output_format: –§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 32)

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ —Ç–æ–º –∂–µ —Ñ–æ—Ä–º–∞—Ç–µ,
            —á—Ç–æ –∏ parse_text()
        """
        try:
            return self.service.parse_batch.remote(
                texts,
                output_format=output_format,
                batch_size=batch_size
            )
        except Exception as e:
            self.logger.error(f"‚ùå Error during batch parsing: {e}")
            raise e


# ========== –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° PYMORPHY3 ==========
class SpacyPymorphy3Parser:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–∞—Ä—Å–µ—Ä–∞ —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏–∑ pymorphy3.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç spaCy –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç
    –ü–û–õ–ù–£–Æ –¥–µ—Ç–∞–ª—å–Ω—É—é –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—é –∏–∑ pymorphy3.

    –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–¥ standalone pymorphy3_wrapper:
    - ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–µ –¥–µ—Ä–µ–≤–æ (–æ—Ç spaCy)
    - ‚úÖ NER (–æ—Ç spaCy)
    - ‚úÖ –ü–æ–ª–Ω–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ —Å–ª–æ–≤–∞ (–æ—Ç pymorphy3)
    - ‚úÖ –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏ –º–µ—Ç–æ–¥—ã —Ä–∞–∑–±–æ—Ä–∞ (–æ—Ç pymorphy3)
    - ‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –∏ –±–µ—Å–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏
    """

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞ —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π pymorphy3."""
        import pymorphy3

        self.spacy_parser = SpacyParser()
        self.morph = pymorphy3.MorphAnalyzer()
        self.logger = logging.getLogger(__name__)
        self.logger.info("‚úì SpaCy+Pymorphy3 parser initialized.")

    def parse_text_enriched(
            self,
            text: str,
            output_format: OutputFormat = "simplified",
            include_lexeme: bool = False,
            include_all_parses: bool = False
    ) -> Union[List[List[Dict[str, Any]]], List[Dict[str, Any]]]:
        """
        –ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç —Å –ø–æ–ª–Ω—ã–º –æ–±–æ–≥–∞—â–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ pymorphy3.

        –î–æ–±–∞–≤–ª—è–µ—Ç –∫ –∫–∞–∂–¥–æ–º—É —Ç–æ–∫–µ–Ω—É –í–°–ï –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø–æ–ª—è pymorphy3:

        –ë–∞–∑–æ–≤—ã–µ –ø–æ–ª—è (–≤—Å–µ–≥–¥–∞):
        - pymorphy3_word: —Ñ–æ—Ä–º–∞ —Å–ª–æ–≤–∞ –∏–∑ pymorphy3 (lowercase!)
        - pymorphy3_lemma: –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –∏–∑ pymorphy3
        - pymorphy3_tag: –ø–æ–ª–Ω—ã–π —Ç–µ–≥ OpenCorpora
        - pymorphy3_score: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–±–æ—Ä–∞ (0.0-1.0)
        - pymorphy3_is_known: True –µ—Å–ª–∏ —Å–ª–æ–≤–æ –µ—Å—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ

        –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–æ–ª—è (–≤—Å–µ–≥–¥–∞):
        - pymorphy3_methods_stack: —Å–ø–∏—Å–æ–∫ –º–µ—Ç–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ pymorphy3
          –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª –¥–ª—è —Ä–∞–∑–±–æ—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, [('DictAnalyzer', 1.0)])
        - pymorphy3_normalized: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ
          {word, tag, score}

        –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø–æ–ª—è:
        - pymorphy3_lexeme: –ø–æ–ª–Ω–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ —Å–ª–æ–≤–∞ - –≤—Å–µ —Å–ª–æ–≤–æ—Ñ–æ—Ä–º—ã
          (–µ—Å–ª–∏ include_lexeme=True, –º–æ–∂–µ—Ç –±—ã—Ç—å 10-30+ —Ñ–æ—Ä–º)
        - pymorphy3_all_parses: –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ä–∞–∑–±–æ—Ä—ã —Å–ª–æ–≤–∞
          (–µ—Å–ª–∏ include_all_parses=True, –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –æ–º–æ–Ω–∏–º–æ–≤)

        Args:
            text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
            output_format: –§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ ('simplified' –∏–ª–∏ 'native')
                           'conllu' –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
            include_lexeme: –í–∫–ª—é—á–∞—Ç—å –ª–∏ –ø–æ–ª–Ω—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É —Å–ª–æ–≤–∞
            include_all_parses: –í–∫–ª—é—á–∞—Ç—å –ª–∏ –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ä–∞–∑–±–æ—Ä–∞

        Returns:
            –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞

        –ü—Ä–∏–º–µ—á–∞–Ω–∏—è:
            –û—Ç–ª–∏—á–∏—è –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ–ª–µ–π spaCy vs pymorphy3:
            - lemma (spaCy) ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
            - pymorphy3_lemma ‚Äî –ø–µ—Ä–≤—ã–π (–Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–π) —Ä–∞–∑–±–æ—Ä –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            - form (spaCy) ‚Äî –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–≥–∏—Å—Ç—Ä
            - pymorphy3_word ‚Äî –≤—Å–µ–≥–¥–∞ lowercase
            - feats (spaCy) ‚Äî —Ñ–æ—Ä–º–∞—Ç Universal Dependencies
            - pymorphy3_tag ‚Äî —Ñ–æ—Ä–º–∞—Ç OpenCorpora
        """
        # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç spaCy
        spacy_result = self.spacy_parser.parse_text(
            text,
            output_format=output_format
        )

        # –î–ª—è CoNLL-U –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        if output_format == "conllu":
            self.logger.warning(
                "CoNLL-U format doesn't support enrichment. "
                "Returning original spaCy result."
            )
            return spacy_result

        # –û–±–æ–≥–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ pymorphy3
        enriched_result = []
        for sent in spacy_result:
            enriched_sent = []

            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞)
            words_list = sent if output_format == "simplified" else sent.get("words", [])

            for token_dict in words_list:
                form = token_dict.get("form", "")

                # –ü–æ–ª—É—á–∞–µ–º –í–°–ï —Ä–∞–∑–±–æ—Ä—ã
                all_parses = self.morph.parse(form)
                p = all_parses[0]  # –ù–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–π

                # –ö–æ–ø–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω –∏ –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—è pymorphy3
                token_dict_enriched = token_dict.copy()

                # ========== –ë–ê–ó–û–í–´–ï –ü–û–õ–Ø ==========
                token_dict_enriched.update({
                    # –í–ê–ñ–ù–û: p.word –≤—Å–µ–≥–¥–∞ lowercase!
                    "pymorphy3_word": p.word,
                    "pymorphy3_lemma": p.normal_form,
                    "pymorphy3_tag": str(p.tag),
                    "pymorphy3_score": p.score,
                    "pymorphy3_is_known": p.is_known,
                })

                # ========== –†–ê–°–®–ò–†–ï–ù–ù–´–ï –ü–û–õ–Ø ==========
                # methods_stack –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–∞–∫ –∏–º–µ–Ω–Ω–æ —Å–ª–æ–≤–æ –±—ã–ª–æ —Ä–∞–∑–æ–±—Ä–∞–Ω–æ
                # –ü—Ä–∏–º–µ—Ä—ã:
                # - [('DictAnalyzer', 1.0)] - –Ω–∞–π–¥–µ–Ω–æ –≤ —Å–ª–æ–≤–∞—Ä–µ
                # - [('FakeDictionary', 0.1)] - –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ —Å–ª–æ–≤–æ
                # - [('KnownPrefixAnalyzer', 0.5)] - –ø–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–º –ø—Ä–µ—Ñ–∏–∫—Å–∞–º
                token_dict_enriched["pymorphy3_methods_stack"] = p.methods_stack

                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ
                token_dict_enriched["pymorphy3_normalized"] = {
                    "word": p.normalized.word,
                    "tag": str(p.normalized.tag),
                    "score": p.normalized.score
                }

                # ========== –û–ü–¶–ò–û–ù–ê–õ–¨–ù–´–ï –ü–û–õ–Ø ==========

                # –ü–æ–ª–Ω–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, —Ç–∞–∫ –∫–∞–∫ –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª—å—à–æ–π)
                if include_lexeme:
                    # lexeme —Å–æ–¥–µ—Ä–∂–∏—Ç –í–°–ï —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞
                    # –ù–∞–ø—Ä–∏–º–µ—Ä –¥–ª—è "–∏–¥—Ç–∏": –∏–¥—É, –∏–¥–µ—à—å, –∏–¥–µ—Ç, —à–µ–ª, —à–ª–∞, –ø–æ—à—ë–ª ...
                    token_dict_enriched["pymorphy3_lexeme"] = [
                        {
                            "word": form.word,
                            "tag": str(form.tag)
                        }
                        for form in p.lexeme
                    ]
                else:
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ—Ä–º –≤ –ø–∞—Ä–∞–¥–∏–≥–º–µ
                    token_dict_enriched["pymorphy3_lexeme_count"] = len(p.lexeme)

                # –í—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ä–∞–∑–±–æ—Ä—ã (–¥–ª—è –æ–º–æ–Ω–∏–º–æ–≤)
                if include_all_parses and len(all_parses) > 1:
                    token_dict_enriched["pymorphy3_all_parses"] = [
                        {
                            "normal_form": parse.normal_form,
                            "tag": str(parse.tag),
                            "score": parse.score,
                            "is_known": parse.is_known
                        }
                        for parse in all_parses
                    ]
                    token_dict_enriched["pymorphy3_parses_count"] = len(all_parses)

                enriched_sent.append(token_dict_enriched)

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞
            if output_format == "native":
                sent_copy = sent.copy()
                sent_copy["words"] = enriched_sent
                enriched_result.append(sent_copy)
            else:
                enriched_result.append(enriched_sent)

        return enriched_result


# ========== –¢–ï–°–¢–û–í–´–ï –ü–†–ò–ú–ï–†–´ ==========
if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã
    test_text = "–ó–ª–æ, –∫–æ—Ç–æ—Ä—ã–º —Ç—ã –º–µ–Ω—è –ø—É–≥–∞–µ—à—å, –≤–æ–≤—Å–µ –Ω–µ —Ç–∞–∫ –∑–ª–æ, –∫–∞–∫ —Ç—ã –∑–ª–æ —É—Ö–º—ã–ª—è–µ—à—å—Å—è."
    test_ambiguous = "–º–æ–π –±—Ä–∞—Ç"  # –æ–º–æ–Ω–∏–º: –º–æ–π (–º–µ—Å—Ç–æ–∏–º–µ–Ω–∏–µ) vs –º—ã—Ç—å
    test_name = "–ú–æ—Å–∫–≤–∞ - —Å—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏."

    print("=" * 80)
    print("–¢–ï–°–¢ 1: –ë–ê–ó–û–í–´–ô SPACY PARSER")
    print("=" * 80)

    parser = SpacyParser()

    # 1.1 Simplified format
    print("\n1.1 SIMPLIFIED FORMAT:")
    print("-" * 80)
    result_simple = parser.parse_text(test_text, output_format="simplified")

    print(f"–¢–µ–∫—Å—Ç: '{test_text}'")
    print(f"–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {len(result_simple)}\n")

    for sent in result_simple:
        print(f"–¢–æ–∫–µ–Ω–æ–≤ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏: {len(sent)}\n")
        print("ID\tFORM\t\tLEMMA\t\tUPOS\tDEPREL")
        print("-" * 80)
        for tok in sent[:7]:
            print(f"{tok['id']}\t{tok['form']:<12}\t{tok['lemma']:<12}\t"
                  f"{tok['upos']}\t{tok['deprel']}")
        if len(sent) > 7:
            print(f"... (–≤—Å–µ–≥–æ {len(sent)} —Ç–æ–∫–µ–Ω–æ–≤)")

    # 1.2 Native format
    print("\n1.2 NATIVE FORMAT —Å NER:")
    print("-" * 80)
    result_native = parser.parse_text(test_name, output_format="native")

    for sent_data in result_native:
        print(f"\n–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: '{sent_data['text']}'")
        print(f"–ì—Ä–∞–Ω–∏—Ü—ã: [{sent_data['start_char']}, {sent_data['end_char']}]")

        if "entities" in sent_data and sent_data["entities"]:
            print("\n–ò–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏:")
            for ent in sent_data["entities"]:
                print(f"  - '{ent['text']}' [{ent['label']}]")
        else:
            print("\n–ò–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏: –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

        print("\n–ü–µ—Ä–≤—ã–µ 3 —Ç–æ–∫–µ–Ω–∞ (–¥–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è):")
        for tok in sent_data["words"]:
        # for tok in sent_data["words"][:3]:
            print(f"\n  –¢–æ–∫–µ–Ω: '{tok['form']}'")
            print(f"    Lemma: {tok['lemma']}")
            print(f"    POS: {tok['upos']} / {tok['xpos']}")
            print(f"    Feats: {tok['feats']}")
            print(f"    Head: {tok['head']}, Deprel: {tok['deprel']}")
            print(f"    Shape: {tok['shape']}, Alpha: {tok['is_alpha']}, Punct: {tok['is_punct']}")
            if tok.get('ent_type'):
                print(f"    Entity: {tok['ent_type']} ({tok['ent_iob']})")
            if tok.get('misc'):
                print(f"    Misc: {tok['misc']}")

    # 1.3 CoNLL-U format
    print("\n1.3 CONLL-U FORMAT:")
    print("-" * 80)
    result_conllu = parser.parse_text(test_text, output_format="conllu")
    print("\n–í—ã–≤–æ–¥ –≤ —Ñ–æ—Ä–º–∞—Ç–µ CoNLL-U (–ø–µ—Ä–≤—ã–µ 800 —Å–∏–º–≤–æ–ª–æ–≤):")
    print(result_conllu[:800])
    if len(result_conllu) > 800:
        print("... (–æ–±—Ä–µ–∑–∞–Ω–æ)")

    # ========================================================================
    print("\n" + "=" * 80)
    print("–¢–ï–°–¢ 2: SPACY + PYMORPHY3 (–ë–ê–ó–û–í–û–ï –û–ë–û–ì–ê–©–ï–ù–ò–ï)")
    print("=" * 80)

    parser_enriched = SpacyPymorphy3Parser()
    result_enriched = parser_enriched.parse_text_enriched(
        test_text,
        output_format="simplified",
        include_lexeme=False
    )

    print(f"\n–¢–µ–∫—Å—Ç: '{test_text}'\n")
    print("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ spaCy –∏ pymorphy3 –¥–ª—è –ø–µ—Ä–≤—ã—Ö 3 —Ç–æ–∫–µ–Ω–æ–≤:")
    print("-" * 80)

    for sent in result_enriched:
        for tok in sent:
        # for tok in sent[:3]:
            print(f"\n{'=' * 70}")
            print(f"–¢–æ–∫–µ–Ω: '{tok['form']}'")
            print(f"{'=' * 70}")

            print(f"\nüìä SPACY (–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑):")
            print(f"  Form: {tok['form']}")
            print(f"  Lemma: {tok['lemma']}")
            print(f"  POS: {tok['upos']} (Universal)")
            print(f"  Feats: {tok['feats']}")
            print(f"  Head: {tok['head']}, Deprel: {tok['deprel']}")

            print(f"\nüîç PYMORPHY3 (–±–µ—Å–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑):")
            print(f"  Word: {tok['pymorphy3_word']} ‚Üê (–≤—Å–µ–≥–¥–∞ lowercase!)")
            print(f"  Lemma: {tok['pymorphy3_lemma']}")
            print(f"  Tag OpenCorpora: {tok['pymorphy3_tag']}")
            print(f"  Score: {tok['pymorphy3_score']:.4f}")
            print(f"  Is known: {tok['pymorphy3_is_known']}")

            print(f"\n‚öôÔ∏è  –ú–ï–¢–û–î–´ –†–ê–ó–ë–û–†–ê:")
            print(f"  Methods stack: {tok['pymorphy3_methods_stack']}")
            if tok['pymorphy3_methods_stack']:
                method_name = tok['pymorphy3_methods_stack'][0][0]
                if method_name == 'DictAnalyzer':
                    print(f"  ‚Ü≥ –ù–∞–π–¥–µ–Ω–æ –≤ —Å–ª–æ–≤–∞—Ä–µ pymorphy3")
                elif method_name == 'FakeDictionary':
                    print(f"  ‚Ü≥ –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ —Å–ª–æ–≤–æ, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ —ç–≤—Ä–∏—Å—Ç–∏–∫–∞")
                elif method_name == 'KnownPrefixAnalyzer':
                    print(f"  ‚Ü≥ –†–∞–∑–±–æ—Ä –ø–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–º –ø—Ä–µ—Ñ–∏–∫—Å–∞–º")

            print(f"\nüìù –ù–û–†–ú–ê–õ–ò–ó–û–í–ê–ù–ù–ê–Ø –§–û–†–ú–ê:")
            normalized = tok['pymorphy3_normalized']
            print(f"  Word: {normalized['word']}")
            print(f"  Tag: {normalized['tag']}")
            print(f"  Score: {normalized['score']:.4f}")

            print(f"\nüìö –ü–ê–†–ê–î–ò–ì–ú–ê:")
            print(f"  –í—Å–µ–≥–æ —Ñ–æ—Ä–º: {tok['pymorphy3_lexeme_count']}")

            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ª–µ–º–º
            if tok['lemma'] != tok['pymorphy3_lemma']:
                print(f"\n‚ö†Ô∏è  –†–ê–°–•–û–ñ–î–ï–ù–ò–ï –í –õ–ï–ú–ú–ê–•:")
                print(f"  spaCy:      '{tok['lemma']}' (—Å —É—á—ë—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)")
                print(f"  pymorphy3:  '{tok['pymorphy3_lemma']}' (–ø–µ—Ä–≤—ã–π —Ä–∞–∑–±–æ—Ä)")

    # ========================================================================
    print("\n" + "=" * 80)
    print("–¢–ï–°–¢ 3: –ü–û–õ–ù–ê–Ø –ü–ê–†–ê–î–ò–ì–ú–ê (lexeme)")
    print("=" * 80)

    test_word = "–∫–Ω–∏–≥–∞"
    result_lexeme = parser_enriched.parse_text_enriched(
        test_word,
        output_format="simplified",
        include_lexeme=True
    )

    print(f"\n–¢–µ–∫—Å—Ç: '{test_word}'\n")

    for sent in result_lexeme:
        for tok in sent:
            print(f"–¢–æ–∫–µ–Ω: '{tok['form']}'")
            print(f"Lemma (spaCy): {tok['lemma']}")
            print(f"Lemma (pymorphy3): {tok['pymorphy3_lemma']}")
            print(f"\n–ü–æ–ª–Ω–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ ({len(tok['pymorphy3_lexeme'])} —Ñ–æ—Ä–º):")
            print("-" * 60)

            # –í—ã–≤–æ–¥–∏–º –≤—Å–µ —Ñ–æ—Ä–º—ã
            for i, form in enumerate(tok['pymorphy3_lexeme'], 1):
                print(f"  {i:2d}. {form['word']:<15} [{form['tag']}]")

    # ========================================================================
    print("\n" + "=" * 80)
    print("–¢–ï–°–¢ 4: –û–ú–û–ù–ò–ú–´ (–≤—Å–µ —Ä–∞–∑–±–æ—Ä—ã)")
    print("=" * 80)

    result_ambiguous = parser_enriched.parse_text_enriched(
        test_ambiguous,
        output_format="simplified",
        include_all_parses=True
    )

    print(f"\n–¢–µ–∫—Å—Ç: '{test_ambiguous}'\n")
    print("–ê–Ω–∞–ª–∏–∑ –æ–º–æ–Ω–∏–º–æ–≤:")
    print("-" * 80)

    for sent in result_ambiguous:
        for tok in sent:
            print(f"\n–¢–æ–∫–µ–Ω: '{tok['form']}'")

            # –†–µ–∑—É–ª—å—Ç–∞—Ç spaCy (–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π)
            print(f"\n  ‚úÖ spaCy –≤—ã–±—Ä–∞–ª (—Å —É—á—ë—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞):")
            print(f"     Lemma: {tok['lemma']}, POS: {tok['upos']}")

            # –†–µ–∑—É–ª—å—Ç–∞—Ç pymorphy3 (–ø–µ—Ä–≤—ã–π —Ä–∞–∑–±–æ—Ä)
            print(f"\n  üìä pymorphy3 –ø—Ä–µ–¥–ª–æ–∂–∏–ª (–ø–µ—Ä–≤—ã–π –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏):")
            print(f"     Lemma: {tok['pymorphy3_lemma']}")
            print(f"     Tag: {tok['pymorphy3_tag']}")
            print(f"     Score: {tok['pymorphy3_score']:.4f}")

            # –í—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ä–∞–∑–±–æ—Ä—ã
            if 'pymorphy3_all_parses' in tok:
                print(f"\n  üîç –í—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ä–∞–∑–±–æ—Ä—ã ({tok['pymorphy3_parses_count']}):")
                for i, parse in enumerate(tok['pymorphy3_all_parses'], 1):
                    print(f"     {i}. {parse['normal_form']:<12} "
                          f"[{parse['tag']:<30}] score={parse['score']:.4f}")

            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
            if tok['lemma'] != tok['pymorphy3_lemma']:
                print(f"\n  ‚ö†Ô∏è  –ö–û–ù–¢–ï–ö–°–¢ –ü–û–ú–û–ì: spaCy –≤—ã–±—Ä–∞–ª '{tok['lemma']}' –≤–º–µ—Å—Ç–æ '{tok['pymorphy3_lemma']}'")

    # ========================================================================
    print("\n" + "=" * 80)
    print("–¢–ï–°–¢ 5: –†–ï–ì–ò–°–¢–† (form vs pymorphy3_word)")
    print("=" * 80)

    result_case = parser_enriched.parse_text_enriched(
        test_name,
        output_format="simplified"
    )

    print(f"\n–¢–µ–∫—Å—Ç: '{test_name}'\n")
    print("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–≥–∏—Å—Ç—Ä–∞:")
    print("-" * 80)

    for sent in result_case:
        for tok in sent:
        # for tok in sent[:3]:
            print(f"\n–¢–æ–∫–µ–Ω:")
            print(f"  form (spaCy):          '{tok['form']}'  ‚Üê –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–≥–∏—Å—Ç—Ä")
            print(f"  pymorphy3_word:        '{tok['pymorphy3_word']}'  ‚Üê –≤—Å–µ–≥–¥–∞ lowercase")

            if tok['form'] != tok['pymorphy3_word']:
                print(f"  ‚ö†Ô∏è  –†–ê–ó–õ–ò–ß–ò–ï –í –†–ï–ì–ò–°–¢–†–ï!")

    # ========================================================================
    print("\n" + "=" * 80)
    print("–¢–ï–°–¢ 6: BATCH PROCESSING")
    print("=" * 80)

    test_texts_batch = [
        # "–ú–æ—Å–∫–≤–∞ - —Å—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏.",
        # "–ü–µ—Ç—Ä –∫—É–ø–∏–ª –∫–Ω–∏–≥—É –≤ –º–∞–≥–∞–∑–∏–Ω–µ.",
        # "–û–Ω–∞ —á–∏—Ç–∞–µ—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω—É—é –≥–∞–∑–µ—Ç—É.",
        "–ó–ª–æ, –∫–æ—Ç–æ—Ä—ã–º —Ç—ã –º–µ–Ω—è –ø—É–≥–∞–µ—à—å, –≤–æ–≤—Å–µ –Ω–µ —Ç–∞–∫ –∑–ª–æ, –∫–∞–∫ —Ç—ã –∑–ª–æ —É—Ö–º—ã–ª—è–µ—à—å—Å—è."

    ]

    print("\n6.1 Batch Processing (spaCy):")
    print("-" * 80)
    results_batch = parser.parse_batch(
        test_texts_batch,
        output_format="simplified",
        batch_size=32
    )

    print(f"\n–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: {len(results_batch)}\n")
    for i, text_result in enumerate(results_batch):
        print(f"–¢–µ–∫—Å—Ç {i + 1}: '{test_texts_batch[i]}'")
        for sent in text_result:
            tokens_str = " ‚Üí ".join([
                f"{tok['form']}({tok['upos']})"
                for tok in sent
                # for tok in sent[:3]
            ])
            print(f"  –¢–æ–∫–µ–Ω–æ–≤: {len(sent)}, –ü–µ—Ä–≤—ã–µ 3: {tokens_str}")

    # ========================================================================
    print("\n" + "=" * 80)
    print("‚úÖ –í–°–ï –¢–ï–°–¢–´ –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–´!")
    print("=" * 80)

    print("\nüìä –°–í–û–î–ö–ê:")
    print("-" * 80)
    print("‚úì SpaCy parser: 3 —Ñ–æ—Ä–º–∞—Ç–∞ (simplified, native, conllu)")
    print("‚úì SpaCy+Pymorphy3: –ø–æ–ª–Ω–æ–µ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ")
    print("  - –ë–∞–∑–æ–≤—ã–µ –ø–æ–ª—è: lemma, tag, score, is_known")
    print("  - –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ: methods_stack, normalized")
    print("  - –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ: lexeme, all_parses")
    print("‚úì Batch processing: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
    print("‚úì –í—ã—è–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π: –∫–æ–Ω—Ç–µ–∫—Å—Ç, —Ä–µ–≥–∏—Å—Ç—Ä, –æ–º–æ–Ω–∏–º—ã")
    print("-" * 80)

    print("\nüí° –ö–õ–Æ–ß–ï–í–´–ï –û–¢–õ–ò–ß–ò–Ø spaCy vs pymorphy3:")
    print("-" * 80)
    print("1. –õ–ï–ú–ú–ê:")
    print("   - spaCy: –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º–∞—è (—Ç–æ—á–Ω–µ–µ –¥–ª—è –æ–º–æ–Ω–∏–º–æ–≤)")
    print("   - pymorphy3: –ø–µ—Ä–≤—ã–π –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (–±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)")
    print("\n2. –†–ï–ì–ò–°–¢–†:")
    print("   - spaCy (form): –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–≥–∏—Å—Ç—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞")
    print("   - pymorphy3 (word): –≤—Å–µ–≥–¥–∞ lowercase")
    print("\n3. –ú–û–†–§–û–õ–û–ì–ò–Ø:")
    print("   - spaCy (feats): —Ñ–æ—Ä–º–∞—Ç Universal Dependencies")
    print("   - pymorphy3 (tag): —Ñ–æ—Ä–º–∞—Ç OpenCorpora")
    print("\n4. –£–ù–ò–ö–ê–õ–¨–ù–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò pymorphy3:")
    print("   - –ü–æ–ª–Ω–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ —Å–ª–æ–≤–∞ (–≤—Å–µ —Ñ–æ—Ä–º—ã)")
    print("   - –í—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ä–∞–∑–±–æ—Ä–∞ (–¥–ª—è –æ–º–æ–Ω–∏–º–æ–≤)")
    print("   - –ú–µ—Ç–æ–¥—ã —Ä–∞–∑–±–æ—Ä–∞ (–∫–∞–∫ —Å–ª–æ–≤–æ –±—ã–ª–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ)")
    print("-" * 80)
