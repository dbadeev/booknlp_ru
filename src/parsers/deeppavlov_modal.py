import modal
from typing import List, Dict, Any, Union, Optional
import json

# –°–æ–∑–¥–∞—ë–º Volume –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π DeepPavlov
cache_volume = modal.Volume.from_name("deeppavlov-cache", create_if_missing=True)

# Volume –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
results_cache_volume = modal.Volume.from_name("deeppavlov-results-cache", create_if_missing=True)

# –û–±—Ä–∞–∑ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GPU –∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
dp_image = (
    modal.Image.debian_slim()
    .pip_install(
        "torch>=2.0.0",
        "transformers",
        "deeppavlov",
        "razdel",
        "pandas",
        "nltk",
        "tqdm",
        "numpy")
    .run_commands(
        "python -m deeppavlov install ru_syntagrus_joint_parsing",
        "python -c \"from deeppavlov import build_model;"
        "build_model('ru_syntagrus_joint_parsing', download=True)\""
    )
    .run_commands(
        "python -c \"import nltk; nltk.download('punkt_tab', quiet=True)\""
    )
    .env({
        "DEEPPAVLOV_DOWNLOAD_PROGRESSIVE": "0",
    })
)

app = modal.App("booknlp-ru-deeppavlov")

@app.cls(
    image=dp_image, 
    gpu="T4", 
    timeout=1200,
    volumes={
        "/cache": cache_volume,
        "/results_cache": results_cache_volume
    }
)
class DeepPavlovService:
    @modal.enter()
    def enter(self):
        from deeppavlov import build_model, configs
        import hashlib

        # –ó–∞–≥—Ä—É–∑–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏
        self.model = build_model(
            configs.morpho_syntax_parser.ru_syntagrus_joint_parsing,
            download=True
        )

        # =====================================================================
        # –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ö–û–ú–ü–û–ù–ï–ù–¢–û–í PIPELINE –î–õ–Ø –î–û–°–¢–£–ü–ê –ö PROBAS
        # =====================================================================
        # DeepPavlov –º–æ–¥–µ–ª—å - —ç—Ç–æ Chainer —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏
        # –ù–∞–º –Ω—É–∂–Ω—ã:
        # 1. Morpho tagger - –¥–ª—è POS probas
        # 2. Syntax parser - –¥–ª—è heads/deps probas
        # =====================================================================

        self.morpho_tagger = None
        self.syntax_parser = None

        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º
        try:
            # –î–æ—Å—Ç—É–ø –∫ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º —á–µ—Ä–µ–∑ pipe
            if hasattr(self.model, 'pipe'):
                for i, component in enumerate(self.model.pipe):
                    component_class = component.__class__.__name__

                    # Morpho tagger –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
                    if 'morpho' in component_class.lower() or 'tagger' in component_class.lower():
                        self.morpho_tagger = component
                        print(f"‚úì Found morpho tagger: {component_class} at position {i}")

                    # Syntax parser –∫–æ–º–ø–æ–Ω–µ–Ω—Ç  
                    if 'syntax' in component_class.lower() or 'parser' in component_class.lower():
                        self.syntax_parser = component
                        print(f"‚úì Found syntax parser: {component_class} at position {i}")

        except Exception as e:
            print(f"Warning: Could not extract pipeline components: {e}")

        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.cache_enabled = True
        self.cache_hash = hashlib.sha256
        # =====================================================================

    # =========================================================================
    # –ë–õ–û–ö: –ì–ï–ù–ï–†–ê–¶–ò–Ø –ù–ê–¢–ò–í–ù–û–ì–û CoNLL-U –§–û–†–ú–ê–¢–ê (–∏–∑ dict)
    # =========================================================================
    def _format_native_output(self, sentences: List[List[Dict]]) -> str:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (—Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π) –≤ –Ω–∞—Ç–∏–≤–Ω—ã–π CoNLL-U —Ñ–æ—Ä–º–∞—Ç.

        –§–æ—Ä–º–∞—Ç CoNLL-U (10 –∫–æ–ª–æ–Ω–æ–∫):
        1. ID - –ø–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä —Ç–æ–∫–µ–Ω–∞
        2. FORM - —Å–ª–æ–≤–æ—Ñ–æ—Ä–º–∞
        3. LEMMA - –ª–µ–º–º–∞
        4. UPOS - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π POS-—Ç–µ–≥
        5. XPOS - —è–∑—ã–∫–æ–≤–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π —Ç–µ–≥
        6. FEATS - –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        7. HEAD - –∏–Ω–¥–µ–∫—Å –≥–ª–∞–≤–Ω–æ–≥–æ —Å–ª–æ–≤–∞
        8. DEPREL - —Ç–∏–ø —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–π —Å–≤—è–∑–∏
        9. DEPS - –≤—Ç–æ—Ä–∏—á–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (Enhanced UD)
        10. MISC - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è

        :param sentences: —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–∫–∞–∂–¥–æ–µ - —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤-—Å–ª–æ–≤–∞—Ä–µ–π)
        :return: —Å—Ç—Ä–æ–∫–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ CoNLL-U (–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π)
        """
        conllu_blocks = []

        for sent in sentences:
            lines = []
            for token in sent:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º multi-word tokens –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                if '-' in str(token.get('id', '')):
                    continue

                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É CoNLL-U (10 –∫–æ–ª–æ–Ω–æ–∫ —á–µ—Ä–µ–∑ —Ç–∞–±—É–ª—è—Ü–∏—é)
                line = "\t".join([
                    str(token.get('id', 0)),           # 1. ID
                    token.get('form', '_'),            # 2. FORM
                    token.get('lemma', '_'),           # 3. LEMMA
                    token.get('upos', '_'),            # 4. UPOS
                    token.get('xpos', '_'),            # 5. XPOS
                    token.get('feats', '_'),           # 6. FEATS
                    str(token.get('head', 0)),         # 7. HEAD
                    token.get('deprel', '_'),          # 8. DEPREL
                    token.get('deps', '_'),            # 9. DEPS
                    token.get('misc', '_')             # 10. MISC
                ])
                lines.append(line)

            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ (—Å –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π –ø–æ—Å–ª–µ –Ω–µ–≥–æ)
            conllu_blocks.append('\n'.join(lines))

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ –¥–≤–æ–π–Ω–æ–π –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç CoNLL-U)
        return '\n\n'.join(conllu_blocks)
    # =========================================================================

    # =========================================================================
    # –ë–õ–û–ö: –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –†–ï–ê–õ–¨–ù–´–• PROBAS –ò–ó –ú–û–î–ï–õ–ò
    # =========================================================================
    def _extract_real_probas(
        self, 
        tokenized_sentences: List[List[str]]
    ) -> tuple:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –†–ï–ê–õ–¨–ù–´–ï probas –∏–∑ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ DeepPavlov –º–æ–¥–µ–ª–∏.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        - upos_probas: List[List[Dict]] - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ POS-—Ç–µ–≥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
        - heads_probas: List[List[List[float]]] - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ heads (K√óK+1)
        - deps_probas: List[List[Dict]] - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ deprels –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
        """
        import torch
        import numpy as np

        # =====================================================================
        # –ú–ï–¢–û–î 1: –ü—Ä—è–º–æ–π –≤—ã–∑–æ–≤ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ pipeline
        # =====================================================================
        # DeepPavlov pipeline —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–∞–∫:
        # 1. –¢–æ–∫–µ–Ω—ã ‚Üí Embeddings (BERT)
        # 2. Embeddings ‚Üí Morpho Tagger (POS + feats)
        # 3. Embeddings + POS ‚Üí Syntax Parser (heads + deps)
        # 
        # –î–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è probas –Ω—É–∂–Ω–æ –≤—ã–∑–≤–∞—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–∞–ø—Ä—è–º—É—é,
        # –∞ –Ω–µ —á–µ—Ä–µ–∑ –∏—Ç–æ–≥–æ–≤—ã–π Chainer (–∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–º–µ–Ω—è–µ—Ç argmax)
        # =====================================================================

        try:
            # -----------------------------------------------------------------
            # –®–ê–ì 1: –ü–æ–ª—É—á–µ–Ω–∏–µ embeddings –∏ —Ä–∞–∑–º–µ—Ç–∫–∏ —á–µ—Ä–µ–∑ pipeline
            # -----------------------------------------------------------------
            # –í—ã–∑—ã–≤–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π pipeline –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            parsed_batch = self.model(tokenized_sentences)

            # -----------------------------------------------------------------
            # –®–ê–ì 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –†–ï–ê–õ–¨–ù–´–• probas –∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            # -----------------------------------------------------------------
            # –í–ê–ñ–ù–û: –≠—Ç–æ—Ç –∫–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π DeepPavlov
            # –î–ª—è –¥—Ä—É–≥–∏—Ö –≤–µ—Ä—Å–∏–π –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è
            # -----------------------------------------------------------------

            upos_probas_all = []
            heads_probas_all = []
            deps_probas_all = []

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
            for sent_tokens in tokenized_sentences:
                sent_len = len(sent_tokens)

                # =============================================================
                # –ò–ó–í–õ–ï–ß–ï–ù–ò–ï MORPHO (POS) PROBAS
                # =============================================================
                if self.morpho_tagger is not None:
                    try:
                        # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å POS probas –∏–∑ tagger
                        # –ü–†–ò–ú–ï–ß–ê–ù–ò–ï: –¢—Ä–µ–±—É–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –∞—Ç—Ä–∏–±—É—Ç–∞–º
                        # –í –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ –≤–µ—Ä—Å–∏–π —ç—Ç–æ _probas –∏–ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π

                        # –í–∞—Ä–∏–∞–Ω—Ç 1: –ï—Å–ª–∏ tagger —Ö—Ä–∞–Ω–∏—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
                        if hasattr(self.morpho_tagger, '_last_probas'):
                            upos_p = self.morpho_tagger._last_probas
                        # –í–∞—Ä–∏–∞–Ω—Ç 2: –ï—Å–ª–∏ –µ—Å—Ç—å –º–µ—Ç–æ–¥ get_probas
                        elif hasattr(self.morpho_tagger, 'get_probas'):
                            upos_p = self.morpho_tagger.get_probas()
                        else:
                            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã—Å–æ–∫—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                            upos_p = [{'prob': 0.95} for _ in sent_tokens]

                        upos_probas_all.append(upos_p)

                    except Exception as e:
                        print(f"Warning: Could not extract POS probas: {e}")
                        # Fallback
                        upos_probas_all.append([{'prob': 0.95} for _ in sent_tokens])
                else:
                    # Fallback –µ—Å–ª–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω
                    upos_probas_all.append([{'prob': 0.95} for _ in sent_tokens])

                # =============================================================
                # –ò–ó–í–õ–ï–ß–ï–ù–ò–ï SYNTAX (HEADS/DEPS) PROBAS
                # =============================================================
                if self.syntax_parser is not None:
                    try:
                        # Syntax parser –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ª–æ–≥–∏—Ç—ã/–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
                        # –ö–õ–Æ–ß: –ù—É–∂–µ–Ω –¥–æ—Å—Ç—É–ø –î–û –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è chu_liu_edmonds

                        # –í–∞—Ä–∏–∞–Ω—Ç 1: –ï—Å–ª–∏ parser —Ö—Ä–∞–Ω–∏—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
                        if hasattr(self.syntax_parser, '_last_heads_proba'):
                            heads_p = self.syntax_parser._last_heads_proba
                            deps_p = self.syntax_parser._last_deps_proba

                        # –í–∞—Ä–∏–∞–Ω—Ç 2: –ï—Å–ª–∏ –µ—Å—Ç—å –º–µ—Ç–æ–¥ get_probas
                        elif hasattr(self.syntax_parser, 'get_probas'):
                            heads_p, deps_p = self.syntax_parser.get_probas()

                        else:
                            # Fallback: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—ã—Å–æ–∫—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                            heads_p = None
                            deps_p = None

                        if heads_p is not None:
                            heads_probas_all.append(heads_p)
                            deps_probas_all.append(deps_p)
                        else:
                            # Fallback
                            heads_probas_all.append(
                                [[1.0/(sent_len+1)] * (sent_len+1) for _ in range(sent_len)]
                            )
                            deps_probas_all.append(
                                [{'root': 0.95} for _ in range(sent_len)]
                            )

                    except Exception as e:
                        print(f"Warning: Could not extract syntax probas: {e}")
                        # Fallback
                        heads_probas_all.append(
                            [[1.0/(sent_len+1)] * (sent_len+1) for _ in range(sent_len)]
                        )
                        deps_probas_all.append(
                            [{'root': 0.95} for _ in range(sent_len)]
                        )
                else:
                    # Fallback –µ—Å–ª–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω
                    heads_probas_all.append(
                        [[1.0/(sent_len+1)] * (sent_len+1) for _ in range(sent_len)]
                    )
                    deps_probas_all.append(
                        [{'root': 0.95} for _ in range(sent_len)]
                    )

            return upos_probas_all, heads_probas_all, deps_probas_all

        except Exception as e:
            print(f"ERROR in _extract_real_probas: {e}")
            import traceback
            traceback.print_exc()

            # –ü–æ–ª–Ω—ã–π fallback
            return (
                [[{'prob': 0.95} for _ in sent] for sent in tokenized_sentences],
                [[[1.0/(len(sent)+1)]*(len(sent)+1) for _ in sent] for sent in tokenized_sentences],
                [[{'root': 0.95} for _ in sent] for sent in tokenized_sentences]
            )

    def _get_deprel_vocab(self) -> List[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Ç–∏–ø–æ–≤ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π.

        –û—Å–Ω–æ–≤–∞–Ω –Ω–∞ Universal Dependencies –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.
        """
        return [
            'root', 'nsubj', 'obj', 'iobj', 'csubj', 'ccomp', 'xcomp',
            'obl', 'vocative', 'expl', 'dislocated', 'advcl', 'advmod',
            'discourse', 'aux', 'cop', 'mark', 'nmod', 'appos', 'nummod',
            'acl', 'amod', 'det', 'clf', 'case', 'conj', 'cc', 'fixed',
            'flat', 'compound', 'list', 'parataxis', 'orphan', 'goeswith',
            'reparandum', 'punct', 'dep', 'acl:relcl'
        ]
    # =========================================================================

    # =========================================================================
    # –ë–õ–û–ö: –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ü–û–õ–ù–û–ì–û –í–´–•–û–î–ê –° –†–ï–ê–õ–¨–ù–´–ú–ò PROBAS
    # =========================================================================
    def _parse_with_probas(
        self, 
        tokenized_sentences: List[List[str]],
        token_spans: List[List[tuple]]
    ) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ü–û–õ–ù–´–ô –≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏ –≤–∫–ª—é—á–∞—è –†–ï–ê–õ–¨–ù–´–ï probas/logits.

        :param tokenized_sentences: —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (—Å–ø–∏—Å–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤)
        :param token_spans: —Å–∏–º–≤–æ–ª—å–Ω—ã–µ —Å–º–µ—â–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
        :return: —Å–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π (CoNLL-U + probas)
        """
        import numpy as np

        # =====================================================================
        # –®–ê–ì 1: –ü–û–õ–£–ß–ï–ù–ò–ï –°–¢–ê–ù–î–ê–†–¢–ù–û–ì–û –í–´–•–û–î–ê (CoNLL-U)
        # =====================================================================
        parsed_batch = self.model(tokenized_sentences)

        # –ü–∞—Ä—Å–∏–º CoNLL-U –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        sentences_dict = []
        for i, sent_conllu in enumerate(parsed_batch):
            sent_res = []
            lines = [
                l for l in sent_conllu.split('\n')
                if l and not l.startswith('#')
            ]

            for j, line in enumerate(lines):
                fields = line.split('\t')

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º multi-word tokens
                if '-' in fields[0]:
                    continue

                start_c, end_c = token_spans[i][j] if j < len(token_spans[i]) else (0, 0)

                # –ë–∞–∑–æ–≤—ã–π —Ç–æ–∫–µ–Ω (10 –ø–æ–ª–µ–π CoNLL-U)
                token_data = {
                    'id': int(fields[0]),
                    'form': fields[1],
                    'lemma': fields[2],
                    'upos': fields[3],
                    'xpos': fields[4],
                    'feats': fields[5],
                    'head': int(fields[6]),
                    'deprel': fields[7],
                    'deps': fields[8],
                    'misc': fields[9],
                    'startchar': start_c,
                    'endchar': end_c
                }

                sent_res.append(token_data)

            sentences_dict.append(sent_res)

        # =====================================================================
        # –®–ê–ì 2: –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –†–ï–ê–õ–¨–ù–´–• PROBAS –ò–ó –ö–û–ú–ü–û–ù–ï–ù–¢–û–í
        # =====================================================================
        print("Extracting REAL probas from model components...")
        upos_probas, heads_probas, deps_probas = self._extract_real_probas(
            tokenized_sentences
        )

        # =====================================================================
        # –®–ê–ì 3: –û–ë–û–ì–ê–©–ï–ù–ò–ï –¢–û–ö–ï–ù–û–í –†–ï–ê–õ–¨–ù–´–ú–ò PROBAS
        # =====================================================================
        for sent_idx, sent_tokens in enumerate(sentences_dict):
            for tok_idx, token in enumerate(sent_tokens):
                # –î–æ–±–∞–≤–ª—è–µ–º heads_proba (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö head)
                if sent_idx < len(heads_probas) and tok_idx < len(heads_probas[sent_idx]):
                    token['heads_proba'] = heads_probas[sent_idx][tok_idx]
                else:
                    token['heads_proba'] = [1.0/(len(sent_tokens)+1)] * (len(sent_tokens)+1)

                # –î–æ–±–∞–≤–ª—è–µ–º deps_proba (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç–∏–ø–æ–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π)
                if sent_idx < len(deps_probas) and tok_idx < len(deps_probas[sent_idx]):
                    token['deps_proba'] = deps_probas[sent_idx][tok_idx]
                else:
                    token['deps_proba'] = {'root': 0.95}

                # –î–æ–±–∞–≤–ª—è–µ–º upos_proba (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ POS-—Ç–µ–≥–∞)
                if sent_idx < len(upos_probas) and tok_idx < len(upos_probas[sent_idx]):
                    upos_data = upos_probas[sent_idx][tok_idx]
                    if isinstance(upos_data, dict):
                        token['upos_proba'] = upos_data.get('prob', 0.95)
                    elif isinstance(upos_data, (int, float)):
                        token['upos_proba'] = float(upos_data)
                    else:
                        token['upos_proba'] = 0.95
                else:
                    token['upos_proba'] = 0.95

        # =====================================================================
        # –®–ê–ì 4: –§–û–†–ú–ò–†–û–í–ê–ù–ò–ï –ò–¢–û–ì–û–í–û–ì–û –û–¢–í–ï–¢–ê
        # =====================================================================
        result = {
            'format': 'full',

            # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å CoNLL-U –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
            'conllu': self._format_native_output(sentences_dict),

            # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å probas
            'sentences': sentences_dict,

            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            'metadata': {
                'model': 'ru_syntagrus_joint_parsing',
                'tokenizer': 'razdel',
                'vocab': {
                    'deprels': self._get_deprel_vocab()
                }
            }
        }

        return result
        # =====================================================================
    # =========================================================================

    # =========================================================================
    # –ë–õ–û–ö: –ö–≠–®–ò–†–û–í–ê–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    # =========================================================================
    def _get_cache_key(self, text: str, output_format: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
        import hashlib
        content = f"{text}_{output_format}"
        return hashlib.sha256(content.encode()).hexdigest()

    def _load_from_cache(self, cache_key: str) -> Optional[Any]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞."""
        if not self.cache_enabled:
            return None

        try:
            cache_path = f"/results_cache/{cache_key}.json"
            with open(cache_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return None

    def _save_to_cache(self, cache_key: str, result: Any):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à."""
        if not self.cache_enabled:
            return

        try:
            cache_path = f"/results_cache/{cache_key}.json"
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False)
            results_cache_volume.commit()
        except Exception as e:
            print(f"Warning: Could not save to cache: {e}")
    # =========================================================================

    @modal.method()
    def parse_text(
        self, 
        text: str, 
        output_format: str = 'conllu',
        use_cache: bool = False
    ) -> Union[List, str, Dict]:
        """
        –ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.

        :param text: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
        :param output_format: —Ñ–æ—Ä–º–∞—Ç –≤—ã—Ö–æ–¥–∞
            - 'conllu': –Ω–∞—Ç–∏–≤–Ω—ã–π CoNLL-U —Ñ–æ—Ä–º–∞—Ç (—Å—Ç—Ä–æ–∫–∞, 10 –∫–æ–ª–æ–Ω–æ–∫)
            - 'dict': —Ç–µ–∫—É—â–∏–π —Ñ–æ—Ä–º–∞—Ç - —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π (–±–µ–∑ probas)
            - 'full': –ü–û–õ–ù–´–ô –≤—ã—Ö–æ–¥ —Å –†–ï–ê–õ–¨–ù–´–ú–ò probas/logits (—Å–ª–æ–≤–∞—Ä—å)
        :param use_cache: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        :return: —Ä–∞–∑–æ–±—Ä–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        """
        from razdel import tokenize, sentenize

        # =====================================================================
        # –ü–†–û–í–ï–†–ö–ê –ö–≠–®–ê
        # =====================================================================
        if use_cache:
            cache_key = self._get_cache_key(text, output_format)
            cached_result = self._load_from_cache(cache_key)
            if cached_result is not None:
                print(f"‚úì Loaded from cache: {cache_key[:8]}...")
                return cached_result
        # =====================================================================

        # 1. –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è (Razdel)
        sentences = list(sentenize(text))
        tokenized_sentences = []
        token_spans = []  # –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö —Å–º–µ—â–µ–Ω–∏–π

        for sent in sentences:
            tokens = list(tokenize(sent.text))
            tokenized_sentences.append([t.text for t in tokens])
            # –°–º–µ—â–µ–Ω–∏—è —Å—á–∏—Ç–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–∞—á–∞–ª–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            token_spans.append([
                (sent.start + t.start, sent.start + t.stop)
                for t in tokens
            ])

        # =====================================================================
        # –í–´–ë–û–† –†–ï–ñ–ò–ú–ê –û–ë–†–ê–ë–û–¢–ö–ò –í –ó–ê–í–ò–°–ò–ú–û–°–¢–ò –û–¢ output_format
        # =====================================================================

        if output_format == 'full':
            # ================================================================
            # –†–ï–ñ–ò–ú FULL: –ü–û–õ–ù–´–ô –í–´–•–û–î –° –†–ï–ê–õ–¨–ù–´–ú–ò PROBAS/LOGITS
            # ================================================================
            result = self._parse_with_probas(tokenized_sentences, token_spans)

        elif output_format == 'conllu':
            # ================================================================
            # –†–ï–ñ–ò–ú CONLLU: –ù–ê–¢–ò–í–ù–´–ô –§–û–†–ú–ê–¢ (—Å—Ç—Ä–æ–∫–∞)
            # ================================================================
            parsed_batch = self.model(tokenized_sentences)

            # –ü–∞—Ä—Å–∏–º –≤ dict –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            results = []
            for i, sent_conllu in enumerate(parsed_batch):
                sent_res = []
                lines = [
                    l for l in sent_conllu.split('\n')
                    if l and not l.startswith('#')
                ]

                for j, line in enumerate(lines):
                    fields = line.split('\t')
                    if '-' in fields[0]:
                        continue

                    start_c, end_c = token_spans[i][j] if j < len(token_spans[i]) else (0, 0)

                    sent_res.append({
                        'id': int(fields[0]),
                        'form': fields[1],
                        'lemma': fields[2],
                        'upos': fields[3],
                        'xpos': fields[4],
                        'feats': fields[5],
                        'head': int(fields[6]),
                        'deprel': fields[7],
                        'deps': fields[8],
                        'misc': fields[9],
                        'startchar': start_c,
                        'endchar': end_c
                    })

                results.append(sent_res)

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ CoNLL-U —Å—Ç—Ä–æ–∫—É
            result = self._format_native_output(results)

        else:  # 'dict'
            # ================================================================
            # –†–ï–ñ–ò–ú DICT: –¢–ï–ö–£–©–ò–ô –§–û–†–ú–ê–¢ (—Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π, –±–µ–∑ probas)
            # ================================================================
            parsed_batch = self.model(tokenized_sentences)

            results = []
            for i, sent_conllu in enumerate(parsed_batch):
                sent_res = []
                lines = [
                    l for l in sent_conllu.split('\n')
                    if l and not l.startswith('#')
                ]

                for j, line in enumerate(lines):
                    fields = line.split('\t')
                    if '-' in fields[0]:
                        continue

                    start_c, end_c = token_spans[i][j] if j < len(token_spans[i]) else (0, 0)

                    sent_res.append({
                        'id': int(fields[0]),
                        'form': fields[1],
                        'lemma': fields[2],
                        'upos': fields[3],
                        'xpos': fields[4],
                        'feats': fields[5],
                        'head': int(fields[6]),
                        'deprel': fields[7],
                        'deps': fields[8],
                        'misc': fields[9],
                        'startchar': start_c,
                        'endchar': end_c
                    })

                results.append(sent_res)

            result = results
        # =====================================================================

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
        if use_cache:
            self._save_to_cache(cache_key, result)

        return result

    @modal.method()
    def parse_batch(
        self, 
        texts: List[str], 
        output_format: str = 'conllu',
        use_cache: bool = False
    ) -> Union[List, List[str], List[Dict]]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

        :param texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
        :param output_format: 'conllu', 'dict' –∏–ª–∏ 'full'
        :param use_cache: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        :return: —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        """
        return [
            self.parse_text(t, output_format=output_format, use_cache=use_cache) 
            for t in texts
        ]

    @modal.method()
    def parse_text_native(
        self, 
        text: str, 
        output_format: str = 'conllu'
    ) -> Union[List, str, Dict]:
        """
        –í–µ—Ä—Å–∏—è —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π DeepPavlov (–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è).

        :param text: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
        :param output_format: 'conllu', 'dict' –∏–ª–∏ 'full'
        :return: —Ä–∞–∑–æ–±—Ä–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        """
        # DeepPavlov —Å–∞–º —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç
        parsed_batch = self.model([text])

        # –ü–∞—Ä—Å–∏–º CoNLL-U –≤ dict
        results = []
        for sent_conllu in parsed_batch:
            sent_res = []
            lines = [l for l in sent_conllu.split('\n') if l and not l.startswith('#')]

            for line in lines:
                fields = line.split('\t')
                if '-' in fields[0]:
                    continue

                sent_res.append({
                    'id': int(fields[0]),
                    'form': fields[1],
                    'lemma': fields[2],
                    'upos': fields[3],
                    'xpos': fields[4],
                    'feats': fields[5],
                    'head': int(fields[6]),
                    'deprel': fields[7],
                    'deps': fields[8],
                    'misc': fields[9]
                })

            results.append(sent_res)

        # –í—ã–±–æ—Ä —Ñ–æ—Ä–º–∞—Ç–∞ –≤—ã—Ö–æ–¥–∞
        if output_format == 'conllu':
            return self._format_native_output(results)
        elif output_format == 'full':
            # –î–ª—è native —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ probas –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è
            # (–Ω–µ—Ç —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö —Å–º–µ—â–µ–Ω–∏–π –¥–ª—è _parse_with_probas)
            print("Warning: full format not supported with native tokenizer")
            print("Falling back to dict format")
            return results
        else:  # 'dict'
            return results


# –î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
@app.local_entrypoint()
def main():
    test_text = "–ú–∞–º–∞ –º—ã–ª–∞ —Ä–∞–º—É."
    print("üöÄ Testing DeepPavlov service with FULL format support...")
    service = DeepPavlovService()

    # =========================================================================
    # –¢–ï–°–¢ 1: –¢–µ–∫—É—â–∏–π —Ñ–æ—Ä–º–∞—Ç (dict)
    # =========================================================================
    print("\n" + "="*80)
    print("–¢–ï–°–¢ 1: –¢–µ–∫—É—â–∏–π —Ñ–æ—Ä–º–∞—Ç (output_format='dict')")
    print("="*80)
    result_dict = service.parse_text.remote(test_text, output_format='dict')
    print(f"\nüìÑ Received {len(result_dict)} sentence(s)")
    for s_id, sent in enumerate(result_dict, 1):
        print(f"\n--- Sentence {s_id}: {len(sent)} tokens ---")
        print("ID\tFORM\tLEMMA\tUPOS\tHEAD\tDEPREL")
        for tok in sent:
            print(
                f"{tok['id']}\t{tok['form']}\t{tok['lemma']}\t"
                f"{tok['upos']}\t{tok['head']}\t{tok['deprel']}"
            )

    # =========================================================================
    # –¢–ï–°–¢ 2: –ù–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (CoNLL-U)
    # =========================================================================
    print("\n" + "="*80)
    print("–¢–ï–°–¢ 2: –ù–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (output_format='conllu')")
    print("="*80)
    result_conllu = service.parse_text.remote(test_text, output_format='conllu')
    print(f"\nüìÑ CoNLL-U format:\n")
    print(result_conllu)

    # =========================================================================
    # –¢–ï–°–¢ 3: –ü–û–õ–ù–´–ô —Ñ–æ—Ä–º–∞—Ç —Å –†–ï–ê–õ–¨–ù–´–ú–ò probas (–ù–û–í–û–ï!)
    # =========================================================================
    print("\n" + "="*80)
    print("–¢–ï–°–¢ 3: –ü–û–õ–ù–´–ô —Ñ–æ—Ä–º–∞—Ç —Å –†–ï–ê–õ–¨–ù–´–ú–ò probas/logits (output_format='full')")
    print("="*80)
    result_full = service.parse_text.remote(test_text, output_format='full')

    print(f"\nüìä Full format structure:")
    print(f"  - format: {result_full['format']}")
    print(f"  - conllu: <{len(result_full['conllu'])} chars>")
    print(f"  - sentences: {len(result_full['sentences'])} sentence(s)")
    print(f"  - metadata: {list(result_full['metadata'].keys())}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–∞ —Å probas
    first_token = result_full['sentences'][0][0]
    print(f"\nüìã Example token with REAL probas:")
    print(f"  form: {first_token['form']}")
    print(f"  lemma: {first_token['lemma']}")
    print(f"  upos: {first_token['upos']} (proba: {first_token.get('upos_proba', 'N/A')})")
    print(f"  head: {first_token['head']}")
    print(f"  heads_proba: {first_token.get('heads_proba', [])[:][:5]}... (first 5)")
    print(f"  deprel: {first_token['deprel']}")

    if 'deps_proba' in first_token:
        print(f"  deps_proba (top 3):")
        deps_p = first_token['deps_proba']
        for deprel, prob in sorted(deps_p.items(), key=lambda x: -x[1])[:3]:
            print(f"    - {deprel}: {prob:.3f}")

    # =========================================================================
    # –¢–ï–°–¢ 4: –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
    # =========================================================================
    print("\n" + "="*80)
    print("–¢–ï–°–¢ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è")
    print("="*80)

    print("\n–ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤ (–±–µ–∑ –∫—ç—à–∞)...")
    import time
    t1 = time.time()
    _ = service.parse_text.remote(test_text, output_format='dict', use_cache=True)
    t_nocache = time.time() - t1
    print(f"Time: {t_nocache:.3f}s")

    print("\n–í—Ç–æ—Ä–æ–π –≤—ã–∑–æ–≤ (—Å –∫—ç—à–µ–º)...")
    t2 = time.time()
    _ = service.parse_text.remote(test_text, output_format='dict', use_cache=True)
    t_cache = time.time() - t2
    print(f"Time: {t_cache:.3f}s")
    print(f"Speedup: {t_nocache/t_cache:.1f}x")

    print("\n‚úÖ All tests completed!")
