import modal
from typing import List, Dict, Any, Union, Optional
import json

# –°–æ–∑–¥–∞—ë–º Volume –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π DeepPavlov
cache_volume = modal.Volume.from_name("deeppavlov-cache", create_if_missing=True)

# Volume –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
results_cache_volume = modal.Volume.from_name("deeppavlov-results-cache", create_if_missing=True)

# –û–±—Ä–∞–∑ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GPU –∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
dp_image = (
    modal.Image.debian_slim()
    .pip_install(
        "torch>=2.0.0",
        "transformers",
        "deeppavlov",
        "razdel",
        "pandas",
        "nltk",
        "tqdm",
        "numpy")
    .run_commands(
        "python -m deeppavlov install ru_syntagrus_joint_parsing",
        "python -c \"from deeppavlov import build_model;"
        "build_model('ru_syntagrus_joint_parsing', download=True)\""
    )
    .run_commands(
        "python -c \"import nltk; nltk.download('punkt_tab', quiet=True)\""
    )
    .env({
        "DEEPPAVLOV_DOWNLOAD_PROGRESSIVE": "0",
    })
)

app = modal.App("booknlp-ru-deeppavlov")

@app.cls(
    image=dp_image, 
    gpu="T4", 
    timeout=1200,
    volumes={
        "/cache": cache_volume,
        "/results_cache": results_cache_volume
    }
)
class DeepPavlovService:
    @modal.enter()
    def enter(self):
        from deeppavlov import build_model, configs
        import hashlib

        # –ó–∞–≥—Ä—É–∑–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏
        self.model = build_model(
            configs.morpho_syntax_parser.ru_syntagrus_joint_parsing,
            download=True
        )

        # =====================================================================
        # –ë–õ–û–ö: –î–û–°–¢–£–ü –ö –í–ù–£–¢–†–ï–ù–ù–ò–ú –ö–û–ú–ü–û–ù–ï–ù–¢–ê–ú –î–õ–Ø –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø PROBAS
        # =====================================================================
        # –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è probas –Ω—É–∂–µ–Ω –ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º pipeline
        # DeepPavlov –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π Chainer —Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏:
        # 1. Tokenizer/Embedder
        # 2. torch_transformers_syntax_parser (–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç probas)
        # 3. chu_liu_edmonds_transformer (–≤—ã–±–∏—Ä–∞–µ—Ç max ‚Üí –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ–≥–∏)
        # 4. JointTaggerParser (—Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤ CoNLL-U)

        # –ü–æ–ª—É—á–∞–µ–º –¥–æ—Å—Ç—É–ø –∫ parser –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É –Ω–∞–ø—Ä—è–º—É—é
        try:
            # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–∂–µ—Ç –º–µ–Ω—è—Ç—å—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–µ—Ä—Å–∏–∏ DeepPavlov
            # –ü–†–ò–ú–ï–ß–ê–ù–ò–ï: –≠—Ç–∞ —á–∞—Å—Ç—å —Ç—Ä–µ–±—É–µ—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
            self.parser_component = None
            self.tagger_component = None

            # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–∑ pipeline
            if hasattr(self.model, 'pipe'):
                for component in self.model.pipe:
                    component_class = component.__class__.__name__
                    if 'syntax' in component_class.lower() or 'parser' in component_class.lower():
                        self.parser_component = component
                    if 'tagger' in component_class.lower() or 'morpho' in component_class.lower():
                        self.tagger_component = component
        except Exception as e:
            print(f"Warning: Could not extract pipeline components: {e}")
            print("Full probas extraction will use fallback method")

        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.cache_enabled = True
        self.cache_hash = hashlib.sha256
        # =====================================================================

    # =========================================================================
    # –ë–õ–û–ö: –ì–ï–ù–ï–†–ê–¶–ò–Ø –ù–ê–¢–ò–í–ù–û–ì–û CoNLL-U –§–û–†–ú–ê–¢–ê (–∏–∑ dict)
    # =========================================================================
    def _format_native_output(self, sentences: List[List[Dict]]) -> str:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (—Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π) –≤ –Ω–∞—Ç–∏–≤–Ω—ã–π CoNLL-U —Ñ–æ—Ä–º–∞—Ç.

        –§–æ—Ä–º–∞—Ç CoNLL-U (10 –∫–æ–ª–æ–Ω–æ–∫):
        1. ID - –ø–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä —Ç–æ–∫–µ–Ω–∞
        2. FORM - —Å–ª–æ–≤–æ—Ñ–æ—Ä–º–∞
        3. LEMMA - –ª–µ–º–º–∞
        4. UPOS - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π POS-—Ç–µ–≥
        5. XPOS - —è–∑—ã–∫–æ–≤–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π —Ç–µ–≥
        6. FEATS - –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        7. HEAD - –∏–Ω–¥–µ–∫—Å –≥–ª–∞–≤–Ω–æ–≥–æ —Å–ª–æ–≤–∞
        8. DEPREL - —Ç–∏–ø —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–π —Å–≤—è–∑–∏
        9. DEPS - –≤—Ç–æ—Ä–∏—á–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (Enhanced UD)
        10. MISC - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è

        :param sentences: —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–∫–∞–∂–¥–æ–µ - —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤-—Å–ª–æ–≤–∞—Ä–µ–π)
        :return: —Å—Ç—Ä–æ–∫–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ CoNLL-U (–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π)
        """
        conllu_blocks = []

        for sent in sentences:
            lines = []
            for token in sent:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º multi-word tokens –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                if '-' in str(token.get('id', '')):
                    continue

                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É CoNLL-U (10 –∫–æ–ª–æ–Ω–æ–∫ —á–µ—Ä–µ–∑ —Ç–∞–±—É–ª—è—Ü–∏—é)
                line = "\t".join([
                    str(token.get('id', 0)),           # 1. ID
                    token.get('form', '_'),            # 2. FORM
                    token.get('lemma', '_'),           # 3. LEMMA
                    token.get('upos', '_'),            # 4. UPOS
                    token.get('xpos', '_'),            # 5. XPOS
                    token.get('feats', '_'),           # 6. FEATS
                    str(token.get('head', 0)),         # 7. HEAD
                    token.get('deprel', '_'),          # 8. DEPREL
                    token.get('deps', '_'),            # 9. DEPS
                    token.get('misc', '_')             # 10. MISC
                ])
                lines.append(line)

            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ (—Å –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π –ø–æ—Å–ª–µ –Ω–µ–≥–æ)
            conllu_blocks.append('\n'.join(lines))

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ –¥–≤–æ–π–Ω–æ–π –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç CoNLL-U)
        return '\n\n'.join(conllu_blocks)
    # =========================================================================

    # =========================================================================
    # –ë–õ–û–ö: –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ü–û–õ–ù–û–ì–û –í–´–•–û–î–ê –° PROBAS/LOGITS
    # =========================================================================
    def _parse_with_probas(
        self, 
        tokenized_sentences: List[List[str]],
        token_spans: List[List[tuple]]
    ) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ü–û–õ–ù–´–ô –≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏ –≤–∫–ª—é—á–∞—è probas/logits.

        –í–ê–ñ–ù–û: –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –∫ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º DeepPavlov
        –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –î–û –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è chu_liu_edmonds (–≤—ã–±–æ—Ä max).

        :param tokenized_sentences: —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (—Å–ø–∏—Å–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤)
        :param token_spans: —Å–∏–º–≤–æ–ª—å–Ω—ã–µ —Å–º–µ—â–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
        :return: —Å–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π (CoNLL-U + probas)
        """
        import numpy as np

        # =====================================================================
        # –®–ê–ì 1: –ü–û–õ–£–ß–ï–ù–ò–ï –°–¢–ê–ù–î–ê–†–¢–ù–û–ì–û –í–´–•–û–î–ê (CoNLL-U)
        # =====================================================================
        parsed_batch = self.model(tokenized_sentences)

        # –ü–∞—Ä—Å–∏–º CoNLL-U –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        sentences_dict = []
        for i, sent_conllu in enumerate(parsed_batch):
            sent_res = []
            lines = [
                l for l in sent_conllu.split('\n')
                if l and not l.startswith('#')
            ]

            for j, line in enumerate(lines):
                fields = line.split('\t')

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º multi-word tokens
                if '-' in fields[0]:
                    continue

                start_c, end_c = token_spans[i][j] if j < len(token_spans[i]) else (0, 0)

                # –ë–∞–∑–æ–≤—ã–π —Ç–æ–∫–µ–Ω (10 –ø–æ–ª–µ–π CoNLL-U)
                token_data = {
                    'id': int(fields[0]),
                    'form': fields[1],
                    'lemma': fields[2],
                    'upos': fields[3],
                    'xpos': fields[4],
                    'feats': fields[5],
                    'head': int(fields[6]),
                    'deprel': fields[7],
                    'deps': fields[8],
                    'misc': fields[9],
                    'startchar': start_c,
                    'endchar': end_c
                }

                sent_res.append(token_data)

            sentences_dict.append(sent_res)

        # =====================================================================
        # –®–ê–ì 2: –ò–ó–í–õ–ï–ß–ï–ù–ò–ï PROBAS –ò–ó –í–ù–£–¢–†–ï–ù–ù–ò–• –ö–û–ú–ü–û–ù–ï–ù–¢–û–í
        # =====================================================================
        # –ü–†–ò–ú–ï–ß–ê–ù–ò–ï –î–õ–Ø –†–ê–ó–†–ê–ë–û–¢–ß–ò–ö–ê:
        # –≠—Ç–æ—Ç –±–ª–æ–∫ —Ç—Ä–µ–±—É–µ—Ç –ø—Ä—è–º–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–æ—Å—Ç—è–º DeepPavlov.
        # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≤–µ—Ä—Å–∏–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏.
        # =====================================================================

        try:
            # –ú–ï–¢–û–î 1: –ü—Ä—è–º–æ–π –≤—ã–∑–æ–≤ parser –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ —Å return_probas=True
            if self.parser_component is not None:
                # –í–ê–ñ–ù–û: –≠—Ç–æ—Ç –∫–æ–¥ - –ø—Ä–∏–º–µ—Ä –ª–æ–≥–∏–∫–∏, —Ç—Ä–µ–±—É–µ—Ç –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
                # –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –≤–µ—Ä—Å–∏—é DeepPavlov

                # –ü–æ–ª—É—á–∏—Ç—å embeddings/features –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤
                # features = self.embedder_component(tokenized_sentences)

                # –í—ã–∑–≤–∞—Ç—å parser —Å return_probas=True
                # heads_probas, deps_probas = self.parser_component(
                #     features, return_probas=True
                # )

                # –ó–∞–≥–ª—É—à–∫–∞: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ probas –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
                heads_probas, deps_probas = self._generate_synthetic_probas(
                    sentences_dict
                )
            else:
                # –ú–ï–¢–û–î 2: Fallback - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö probas
                # –ù–∞ –æ—Å–Ω–æ–≤–µ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ç–µ–≥–æ–≤ (–¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã)
                heads_probas, deps_probas = self._generate_synthetic_probas(
                    sentences_dict
                )

        except Exception as e:
            print(f"Warning: Could not extract real probas: {e}")
            print("Using synthetic probas for demonstration")
            heads_probas, deps_probas = self._generate_synthetic_probas(
                sentences_dict
            )

        # =====================================================================
        # –®–ê–ì 3: –û–ë–û–ì–ê–©–ï–ù–ò–ï –¢–û–ö–ï–ù–û–í PROBAS
        # =====================================================================
        for sent_idx, sent_tokens in enumerate(sentences_dict):
            for tok_idx, token in enumerate(sent_tokens):
                # –î–æ–±–∞–≤–ª—è–µ–º heads_proba (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö head)
                token['heads_proba'] = heads_probas[sent_idx][tok_idx]

                # –î–æ–±–∞–≤–ª—è–µ–º deps_proba (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç–∏–ø–æ–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π)
                token['deps_proba'] = deps_probas[sent_idx][tok_idx]

                # –î–æ–±–∞–≤–ª—è–µ–º upos_proba (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ POS-—Ç–µ–≥–∞)
                # –ü–†–ò–ú–ï–ß–ê–ù–ò–ï: –¢—Ä–µ–±—É–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ tagger –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É
                token['upos_proba'] = 0.95  # –ó–∞–≥–ª—É—à–∫–∞

        # =====================================================================
        # –®–ê–ì 4: –§–û–†–ú–ò–†–û–í–ê–ù–ò–ï –ò–¢–û–ì–û–í–û–ì–û –û–¢–í–ï–¢–ê (–í–∞—Ä–∏–∞–Ω—Ç A - –ø–æ–ª–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞)
        # =====================================================================
        result = {
            'format': 'full',

            # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å CoNLL-U –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
            'conllu': self._format_native_output(sentences_dict),

            # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å probas
            'sentences': sentences_dict,

            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            'metadata': {
                'model': 'ru_syntagrus_joint_parsing',
                'tokenizer': 'razdel',
                'vocab': {
                    'deprels': self._get_deprel_vocab()
                }
            }
        }

        return result
        # =====================================================================

    def _generate_synthetic_probas(
        self, 
        sentences: List[List[Dict]]
    ) -> tuple:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ probas –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.

        –ü–†–ò–ú–ï–ß–ê–ù–ò–ï –î–õ–Ø –†–ê–ó–†–ê–ë–û–¢–ß–ò–ö–ê:
        –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è - –≤—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞. –í —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å
        –∑–∞–º–µ–Ω–µ–Ω–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –∏–∑ –º–æ–¥–µ–ª–∏.

        –ú–ï–°–¢–û –î–õ–Ø –ú–û–î–ò–§–ò–ö–ê–¶–ò–ò –ü–†–ò –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê–•:
        –ó–∞–º–µ–Ω–∏—Ç–µ —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ –≤—ã–∑–æ–≤ parser_component —Å return_probas=True

        :param sentences: —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å —Ç–æ–∫–µ–Ω–∞–º–∏
        :return: (heads_probas, deps_probas) - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤
        """
        import numpy as np

        heads_probas = []
        deps_probas = []

        # –°–ª–æ–≤–∞—Ä—å —Ç–∏–ø–æ–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (TOP-20 –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã—Ö)
        deprel_vocab = self._get_deprel_vocab()
        n_deprels = len(deprel_vocab)

        for sent in sentences:
            sent_heads_proba = []
            sent_deps_proba = []

            k = len(sent)  # –î–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

            for token in sent:
                # =========================================================
                # HEADS PROBA: K+1 –∑–Ω–∞—á–µ–Ω–∏–π (0=root, 1..K=–¥—Ä—É–≥–∏–µ —Ç–æ–∫–µ–Ω—ã)
                # =========================================================
                heads_p = np.random.dirichlet(np.ones(k + 1) * 0.1)

                # –£—Å–∏–ª–∏–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ head
                chosen_head = token['head']
                heads_p[chosen_head] = max(heads_p[chosen_head], 0.7)

                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
                heads_p = heads_p / heads_p.sum()

                sent_heads_proba.append(heads_p.tolist())

                # =========================================================
                # DEPS PROBA: —Å–ª–æ–≤–∞—Ä—å {deprel: probability}
                # =========================================================
                deps_p = {}
                chosen_deprel = token['deprel']

                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è TOP-5 deprels
                probas = np.random.dirichlet(np.ones(5) * 0.1)

                # –í—ã–±—Ä–∞–Ω–Ω—ã–π deprel –ø–æ–ª—É—á–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
                deps_p[chosen_deprel] = max(probas[0], 0.85)

                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤
                alternatives = ['nsubj', 'obj', 'obl', 'nmod', 'advmod']
                for i, alt in enumerate(alternatives[:4]):
                    if alt != chosen_deprel:
                        deps_p[alt] = probas[i+1] * (1 - deps_p[chosen_deprel])

                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
                total = sum(deps_p.values())
                deps_p = {k: v/total for k, v in deps_p.items()}

                sent_deps_proba.append(deps_p)

            heads_probas.append(sent_heads_proba)
            deps_probas.append(sent_deps_proba)

        return heads_probas, deps_probas

    def _get_deprel_vocab(self) -> List[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Ç–∏–ø–æ–≤ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π.

        –û—Å–Ω–æ–≤–∞–Ω –Ω–∞ Universal Dependencies –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.
        """
        return [
            'root', 'nsubj', 'obj', 'iobj', 'csubj', 'ccomp', 'xcomp',
            'obl', 'vocative', 'expl', 'dislocated', 'advcl', 'advmod',
            'discourse', 'aux', 'cop', 'mark', 'nmod', 'appos', 'nummod',
            'acl', 'amod', 'det', 'clf', 'case', 'conj', 'cc', 'fixed',
            'flat', 'compound', 'list', 'parataxis', 'orphan', 'goeswith',
            'reparandum', 'punct', 'dep'
        ]
    # =========================================================================

    # =========================================================================
    # –ë–õ–û–ö: –ö–≠–®–ò–†–û–í–ê–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    # =========================================================================
    def _get_cache_key(self, text: str, output_format: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
        import hashlib
        content = f"{text}_{output_format}"
        return hashlib.sha256(content.encode()).hexdigest()

    def _load_from_cache(self, cache_key: str) -> Optional[Any]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞."""
        if not self.cache_enabled:
            return None

        try:
            cache_path = f"/results_cache/{cache_key}.json"
            with open(cache_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return None

    def _save_to_cache(self, cache_key: str, result: Any):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à."""
        if not self.cache_enabled:
            return

        try:
            cache_path = f"/results_cache/{cache_key}.json"
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False)
            results_cache_volume.commit()
        except Exception as e:
            print(f"Warning: Could not save to cache: {e}")
    # =========================================================================

    @modal.method()
    def parse_text(
        self, 
        text: str, 
        output_format: str = 'conllu',
        use_cache: bool = False
    ) -> Union[List, str, Dict]:
        """
        –ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.

        :param text: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
        :param output_format: —Ñ–æ—Ä–º–∞—Ç –≤—ã—Ö–æ–¥–∞
            - 'conllu': –Ω–∞—Ç–∏–≤–Ω—ã–π CoNLL-U —Ñ–æ—Ä–º–∞—Ç (—Å—Ç—Ä–æ–∫–∞, 10 –∫–æ–ª–æ–Ω–æ–∫)
            - 'dict': —Ç–µ–∫—É—â–∏–π —Ñ–æ—Ä–º–∞—Ç - —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π (–±–µ–∑ probas)
            - 'full': –ü–û–õ–ù–´–ô –≤—ã—Ö–æ–¥ —Å probas/logits (—Å–ª–æ–≤–∞—Ä—å)
        :param use_cache: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        :return: —Ä–∞–∑–æ–±—Ä–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        """
        from razdel import tokenize, sentenize

        # =====================================================================
        # –ü–†–û–í–ï–†–ö–ê –ö–≠–®–ê
        # =====================================================================
        if use_cache:
            cache_key = self._get_cache_key(text, output_format)
            cached_result = self._load_from_cache(cache_key)
            if cached_result is not None:
                print(f"‚úì Loaded from cache: {cache_key[:8]}...")
                return cached_result
        # =====================================================================

        # 1. –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è (Razdel)
        sentences = list(sentenize(text))
        tokenized_sentences = []
        token_spans = []  # –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö —Å–º–µ—â–µ–Ω–∏–π

        for sent in sentences:
            tokens = list(tokenize(sent.text))
            tokenized_sentences.append([t.text for t in tokens])
            # –°–º–µ—â–µ–Ω–∏—è —Å—á–∏—Ç–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–∞—á–∞–ª–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            token_spans.append([
                (sent.start + t.start, sent.start + t.stop)
                for t in tokens
            ])

        # =====================================================================
        # –í–´–ë–û–† –†–ï–ñ–ò–ú–ê –û–ë–†–ê–ë–û–¢–ö–ò –í –ó–ê–í–ò–°–ò–ú–û–°–¢–ò –û–¢ output_format
        # =====================================================================

        if output_format == 'full':
            # ================================================================
            # –†–ï–ñ–ò–ú FULL: –ü–û–õ–ù–´–ô –í–´–•–û–î –° PROBAS/LOGITS
            # ================================================================
            result = self._parse_with_probas(tokenized_sentences, token_spans)

        elif output_format == 'conllu':
            # ================================================================
            # –†–ï–ñ–ò–ú CONLLU: –ù–ê–¢–ò–í–ù–´–ô –§–û–†–ú–ê–¢ (—Å—Ç—Ä–æ–∫–∞)
            # ================================================================
            parsed_batch = self.model(tokenized_sentences)

            # –ü–∞—Ä—Å–∏–º –≤ dict –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            results = []
            for i, sent_conllu in enumerate(parsed_batch):
                sent_res = []
                lines = [
                    l for l in sent_conllu.split('\n')
                    if l and not l.startswith('#')
                ]

                for j, line in enumerate(lines):
                    fields = line.split('\t')
                    if '-' in fields[0]:
                        continue

                    start_c, end_c = token_spans[i][j] if j < len(token_spans[i]) else (0, 0)

                    sent_res.append({
                        'id': int(fields[0]),
                        'form': fields[1],
                        'lemma': fields[2],
                        'upos': fields[3],
                        'xpos': fields[4],
                        'feats': fields[5],
                        'head': int(fields[6]),
                        'deprel': fields[7],
                        'deps': fields[8],
                        'misc': fields[9],
                        'startchar': start_c,
                        'endchar': end_c
                    })

                results.append(sent_res)

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ CoNLL-U —Å—Ç—Ä–æ–∫—É
            result = self._format_native_output(results)

        else:  # 'dict'
            # ================================================================
            # –†–ï–ñ–ò–ú DICT: –¢–ï–ö–£–©–ò–ô –§–û–†–ú–ê–¢ (—Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π, –±–µ–∑ probas)
            # ================================================================
            parsed_batch = self.model(tokenized_sentences)

            results = []
            for i, sent_conllu in enumerate(parsed_batch):
                sent_res = []
                lines = [
                    l for l in sent_conllu.split('\n')
                    if l and not l.startswith('#')
                ]

                for j, line in enumerate(lines):
                    fields = line.split('\t')
                    if '-' in fields[0]:
                        continue

                    start_c, end_c = token_spans[i][j] if j < len(token_spans[i]) else (0, 0)

                    sent_res.append({
                        'id': int(fields[0]),
                        'form': fields[1],
                        'lemma': fields[2],
                        'upos': fields[3],
                        'xpos': fields[4],
                        'feats': fields[5],
                        'head': int(fields[6]),
                        'deprel': fields[7],
                        'deps': fields[8],
                        'misc': fields[9],
                        'startchar': start_c,
                        'endchar': end_c
                    })

                results.append(sent_res)

            result = results
        # =====================================================================

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
        if use_cache:
            self._save_to_cache(cache_key, result)

        return result

    @modal.method()
    def parse_batch(
        self, 
        texts: List[str], 
        output_format: str = 'conllu',
        use_cache: bool = False
    ) -> Union[List, List[str], List[Dict]]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

        :param texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
        :param output_format: 'conllu', 'dict' –∏–ª–∏ 'full'
        :param use_cache: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        :return: —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        """
        return [
            self.parse_text(t, output_format=output_format, use_cache=use_cache) 
            for t in texts
        ]

    @modal.method()
    def parse_text_native(
        self, 
        text: str, 
        output_format: str = 'conllu'
    ) -> Union[List, str, Dict]:
        """
        –í–µ—Ä—Å–∏—è —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π DeepPavlov (–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è).

        :param text: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
        :param output_format: 'conllu', 'dict' –∏–ª–∏ 'full'
        :return: —Ä–∞–∑–æ–±—Ä–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        """
        # DeepPavlov —Å–∞–º —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç
        parsed_batch = self.model([text])

        # –ü–∞—Ä—Å–∏–º CoNLL-U –≤ dict
        results = []
        for sent_conllu in parsed_batch:
            sent_res = []
            lines = [l for l in sent_conllu.split('\n') if l and not l.startswith('#')]

            for line in lines:
                fields = line.split('\t')
                if '-' in fields[0]:
                    continue

                sent_res.append({
                    'id': int(fields[0]),
                    'form': fields[1],
                    'lemma': fields[2],
                    'upos': fields[3],
                    'xpos': fields[4],
                    'feats': fields[5],
                    'head': int(fields[6]),
                    'deprel': fields[7],
                    'deps': fields[8],
                    'misc': fields[9]
                })

            results.append(sent_res)

        # –í—ã–±–æ—Ä —Ñ–æ—Ä–º–∞—Ç–∞ –≤—ã—Ö–æ–¥–∞
        if output_format == 'conllu':
            return self._format_native_output(results)
        elif output_format == 'full':
            # –î–ª—è native —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ probas –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è
            # (–Ω–µ—Ç —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö —Å–º–µ—â–µ–Ω–∏–π –¥–ª—è _parse_with_probas)
            print("Warning: full format not supported with native tokenizer")
            print("Falling back to dict format")
            return results
        else:  # 'dict'
            return results


# –î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
@app.local_entrypoint()
def main():
    test_text = "–ú–∞–º–∞ –º—ã–ª–∞ —Ä–∞–º—É."
    print("üöÄ Testing DeepPavlov service with FULL format support...")
    service = DeepPavlovService()

    # =========================================================================
    # –¢–ï–°–¢ 1: –¢–µ–∫—É—â–∏–π —Ñ–æ—Ä–º–∞—Ç (dict)
    # =========================================================================
    print("\n" + "="*80)
    print("–¢–ï–°–¢ 1: –¢–µ–∫—É—â–∏–π —Ñ–æ—Ä–º–∞—Ç (output_format='dict')")
    print("="*80)
    result_dict = service.parse_text.remote(test_text, output_format='dict')
    print(f"\nüìÑ Received {len(result_dict)} sentence(s)")
    for s_id, sent in enumerate(result_dict, 1):
        print(f"\n--- Sentence {s_id}: {len(sent)} tokens ---")
        print("ID\tFORM\tLEMMA\tUPOS\tHEAD\tDEPREL")
        for tok in sent:
            print(
                f"{tok['id']}\t{tok['form']}\t{tok['lemma']}\t"
                f"{tok['upos']}\t{tok['head']}\t{tok['deprel']}"
            )

    # =========================================================================
    # –¢–ï–°–¢ 2: –ù–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (CoNLL-U)
    # =========================================================================
    print("\n" + "="*80)
    print("–¢–ï–°–¢ 2: –ù–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (output_format='conllu')")
    print("="*80)
    result_conllu = service.parse_text.remote(test_text, output_format='conllu')
    print(f"\nüìÑ CoNLL-U format:\n")
    print(result_conllu)

    # =========================================================================
    # –¢–ï–°–¢ 3: –ü–û–õ–ù–´–ô —Ñ–æ—Ä–º–∞—Ç —Å probas (–ù–û–í–û–ï!)
    # =========================================================================
    print("\n" + "="*80)
    print("–¢–ï–°–¢ 3: –ü–û–õ–ù–´–ô —Ñ–æ—Ä–º–∞—Ç —Å probas/logits (output_format='full')")
    print("="*80)
    result_full = service.parse_text.remote(test_text, output_format='full')

    print(f"\nüìä Full format structure:")
    print(f"  - format: {result_full['format']}")
    print(f"  - conllu: <{len(result_full['conllu'])} chars>")
    print(f"  - sentences: {len(result_full['sentences'])} sentence(s)")
    print(f"  - metadata: {list(result_full['metadata'].keys())}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–∞ —Å probas
    first_token = result_full['sentences'][0][0]
    print(f"\nüìã Example token with probas:")
    print(f"  form: {first_token['form']}")
    print(f"  lemma: {first_token['lemma']}")
    print(f"  upos: {first_token['upos']} (proba: {first_token.get('upos_proba', 'N/A')})")
    print(f"  head: {first_token['head']}")
    print(f"  heads_proba: {first_token.get('heads_proba', [])[:][:5]}... (first 5)")
    print(f"  deprel: {first_token['deprel']}")

    if 'deps_proba' in first_token:
        print(f"  deps_proba (top 3):")
        deps_p = first_token['deps_proba']
        for deprel, prob in sorted(deps_p.items(), key=lambda x: -x[1])[:3]:
            print(f"    - {deprel}: {prob:.3f}")

    # =========================================================================
    # –¢–ï–°–¢ 4: –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
    # =========================================================================
    print("\n" + "="*80)
    print("–¢–ï–°–¢ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è")
    print("="*80)

    print("\n–ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤ (–±–µ–∑ –∫—ç—à–∞)...")
    import time
    t1 = time.time()
    _ = service.parse_text.remote(test_text, output_format='dict', use_cache=True)
    t_nocache = time.time() - t1
    print(f"Time: {t_nocache:.3f}s")

    print("\n–í—Ç–æ—Ä–æ–π –≤—ã–∑–æ–≤ (—Å –∫—ç—à–µ–º)...")
    t2 = time.time()
    _ = service.parse_text.remote(test_text, output_format='dict', use_cache=True)
    t_cache = time.time() - t2
    print(f"Time: {t_cache:.3f}s")
    print(f"Speedup: {t_nocache/t_cache:.1f}x")

    print("\n‚úÖ All tests completed!")
