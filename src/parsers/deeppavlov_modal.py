import modal
from typing import List, Dict, Any


# –°–æ–∑–¥–∞—ë–º Volume –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π DeepPavlov
cache_volume = modal.Volume.from_name("deeppavlov-cache", create_if_missing=True)

# –û–±—Ä–∞–∑ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GPU –∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
dp_image = (
    modal.Image.debian_slim()
    .pip_install(
        "torch>=2.0.0",
        "transformers",
        "deeppavlov",
        "razdel",
        "pandas",
        "nltk",
        "tqdm")

    .run_commands(
        "python -m deeppavlov install ru_syntagrus_joint_parsing",
        "python -c \"from deeppavlov import build_model;"
        "build_model('ru_syntagrus_joint_parsing', download=True)\""
    )

    .run_commands(
        "python -c \"import nltk; nltk.download('punkt_tab', quiet=True)\""
    )

    .env({
        # –û—Ç–∫–ª—é—á–∞–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ö–µ—à–µ–π
        "DEEPPAVLOV_DOWNLOAD_PROGRESSIVE": "0",
    })
)

app = modal.App("booknlp-ru-deeppavlov")


@app.cls(image=dp_image, gpu="T4", timeout=1200)
class DeepPavlovService:

    @modal.enter()
    def enter(self):
        from deeppavlov import build_model, configs
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è joint_parsing –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç SOTA-—Ç–æ—á–Ω–æ—Å—Ç—å (LAS ~93.4%)
        # –∏ –ø–æ–ª–Ω—É—é —Ä–∞–∑–º–µ—Ç–∫—É –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.model = build_model(
            configs.morpho_syntax_parser.ru_syntagrus_joint_parsing,
            download=True
        )

    @modal.method()
    def parse_text(self, text: str) -> List:
        from razdel import tokenize, sentenize

        # 1. –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è (Razdel)
        sentences = list(sentenize(text))

        tokenized_sentences = []
        token_spans = []  # –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö —Å–º–µ—â–µ–Ω–∏–π

        for sent in sentences:
            tokens = list(tokenize(sent.text))
            tokenized_sentences.append([t.text for t in tokens])
            # –°–º–µ—â–µ–Ω–∏—è —Å—á–∏—Ç–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–∞—á–∞–ª–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            token_spans.append([
                (sent.start + t.start, sent.start + t.stop)
                for t in tokens
            ])

        # 2. –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ä–∞–∑–±–æ—Ä–∞
        # DeepPavlov –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ CoNLL-U (10 –ø–æ–ª–µ–π)
        parsed_batch = self.model(tokenized_sentences)

        results = []
        for i, sent_conllu in enumerate(parsed_batch):
            sent_res = []
            # –†–∞–∑–±–∏—Ä–∞–µ–º CoNLL-U –≤—ã–≤–æ–¥
            lines = [
                l for l in sent_conllu.split('\n')
                if l and not l.startswith('#')
            ]

            for j, line in enumerate(lines):
                fields = line.split('\t')

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ multi-word token
                if '-' in fields[0]:
                    continue

                start_c, end_c = token_spans[i][j] if j < len(token_spans[i]) else (0, 0)

                # –ü–û–õ–ù–´–ô CoNLL-U —Ñ–æ—Ä–º–∞—Ç (10 –ø–æ–ª–µ–π)
                sent_res.append({
                    'id': int(fields[0]),  # ID (1-based)
                    'form': fields[1],  # –°–ª–æ–≤–æ—Ñ–æ—Ä–º–∞
                    'lemma': fields[2],  # –õ–µ–º–º–∞
                    'upos': fields[3],  # Universal POS
                    'xpos': fields[4],  # Language-specific POS (–º–æ–∂–µ—Ç –±—ã—Ç—å "_")
                    'feats': fields[5],  # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                    'head': int(fields[6]),  # –ì–ª–∞–≤–Ω–æ–µ —Å–ª–æ–≤–æ
                    'deprel': fields[7],  # –¢–∏–ø —Å–≤—è–∑–∏
                    'deps': fields[8],  # Enhanced dependencies (–æ–±—ã—á–Ω–æ "_")
                    'misc': fields[9],  # MISC (–æ–±—ã—á–Ω–æ "_")
                    'startchar': start_c,  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: –ø–æ–∑–∏—Ü–∏—è –Ω–∞—á–∞–ª–∞
                    'endchar': end_c  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: –ø–æ–∑–∏—Ü–∏—è –∫–æ–Ω—Ü–∞
                })

            results.append(sent_res)

        return results

    @modal.method()
    def parse_batch(self, texts: List[str]) -> List:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ GPU"""
        return [self.parse_text(t) for t in texts]

    @modal.method()
    def parse_text_native(self, text: str) -> List:
        """
        –í–µ—Ä—Å–∏—è —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π DeepPavlov (–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è).
        """
        # DeepPavlov —Å–∞–º —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç
        parsed_batch = self.model([text])

        results = []
        for sent_conllu in parsed_batch:
            sent_res = []
            lines = [l for l in sent_conllu.split('\n') if l and not l.startswith('#')]

            for line in lines:
                fields = line.split('\t')
                sent_res.append({
                    'id': int(fields[0]),
                    'form': fields[1],
                    'lemma': fields[2],
                    'upos': fields[3],
                    'xpos': fields[4],
                    'feats': fields[5],
                    'head': int(fields[6]),
                    'deprel': fields[7],
                    'deps': fields[8],
                    'misc': fields[9]
                })

            results.append(sent_res)

        return results


# –î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
@app.local_entrypoint()
def main():
    test_text = "–ú–∞–º–∞ –º—ã–ª–∞ —Ä–∞–º—É."
    print("üöÄ Testing DeepPavlov service...")

    service = DeepPavlovService()
    result = service.parse_text.remote(test_text)

    print(f"\nReceived {len(result)} sentence(s)")
    for s_id, sent in enumerate(result, 1):
        print(f"\n--- Sentence {s_id} ---")
        print("ID\tFORM\tLEMMA\tUPOS\tXPOS\tFEATS\tHEAD\tDEPREL")
        for tok in sent:
            print(
                f"{tok['id']}\t{tok['form']}\t{tok['lemma']}\t"
                f"{tok['upos']}\t{tok['xpos']}\t{tok['feats']}\t"
                f"{tok['head']}\t{tok['deprel']}"
            )

    print("\n‚úÖ Test completed!")

