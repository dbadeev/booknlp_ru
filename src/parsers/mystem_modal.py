import modal
import logging
import re

# –û–±—Ä–∞–∑: Python + pymystem3
image = (
    modal.Image.debian_slim()
    .pip_install("pymystem3")
    # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –±–∏–Ω–∞—Ä–Ω–∏–∫–∞ Mystem
    .run_commands("python -c 'from pymystem3 import Mystem; Mystem()'")
)

app = modal.App("booknlp-ru-mystem")

# –ú–∞–ø–ø–∏–Ω–≥ Mystem POS -> Universal Dependencies UPOS
MYSTEM_TO_UPOS = {
    'S': 'NOUN', 'A': 'ADJ', 'V': 'VERB', 'ADV': 'ADV',
    'SPRO': 'PRON', 'PR': 'ADP', 'CONJ': 'CCONJ',
    'PART': 'PART', 'INTJ': 'INTJ', 'NUM': 'NUM',
    'COM': 'X', 'APRO': 'DET', 'ANUM': 'ADJ', 'ADVPRO': 'ADV'
}


@app.cls(image=image, timeout=600)
class MystemService:

    @modal.enter()
    def setup(self):
        from pymystem3 import Mystem
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger("MystemService")
        # entire_input=False —É–±–∏—Ä–∞–µ—Ç –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏–∑ –≤—ã–≤–æ–¥–∞
        # –° disambiguation=False –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ë–ï–ó —É—á–µ—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ –≤ –∫–æ—Ä–ø—É—Å–µ
        # –° disambiguation=True –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –° —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        self.mystem = Mystem(entire_input=False, disambiguation=True)
        self.logger.info("Mystem initialized!")

    @modal.method()
    def parse_batch(self, batch_texts: list, output_format: str = "simplified"):
        """
        batch_texts: –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (list[str] –∏–ª–∏ list[list[str]]).
        output_format: –§–æ—Ä–º–∞—Ç –≤—ã—Ö–æ–¥–∞ - "simplified" (—Ç–µ–∫—É—â–∏–π) –∏–ª–∏ "native" (–Ω–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç Mystem).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: List[List[List[Dict]]].
        """
        results = []

        for text_obj in batch_texts:
            try:
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–∞
                if isinstance(text_obj, list):
                    text = " ".join([str(t) for t in text_obj])
                elif isinstance(text_obj, str):
                    text = text_obj
                else:
                    text = str(text_obj) if text_obj else ""

                if not isinstance(text, str):
                    self.logger.error(f"Text is not string: {type(text)}")
                    results.append([[]])
                    continue

                if not text.strip():
                    results.append([[]])
                    continue

                # Mystem –∞–Ω–∞–ª–∏–∑
                analysis = self.mystem.analyze(text)

                # ============================================================
                # –ë–õ–û–ö: –í—ã–±–æ—Ä —Ñ–æ—Ä–º–∞—Ç–∞ –≤—ã—Ö–æ–¥–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
                # ============================================================
                if output_format == "native":
                    # –ù–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ª–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É JSON –æ—Ç Mystem
                    sent_res = self._process_native(analysis)
                else:
                    # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (—Ç–µ–∫—É—â–∞—è –ª–æ–≥–∏–∫–∞): –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–∫–µ–Ω—ã —Å –±–∞–∑–æ–≤—ã–º–∏ –ø–æ–ª—è–º–∏
                    sent_res = self._process_simplified(analysis)

                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º [sent_res] - —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
                results.append([sent_res] if sent_res else [[]])

            except Exception as e:
                self.logger.error(f"Mystem error: {e}")
                import traceback
                self.logger.error(traceback.format_exc())
                results.append([[]])

        return results

    # ============================================================
    # –ë–õ–û–ö: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–∞—Ç–∏–≤–Ω–æ–≥–æ –≤—ã—Ö–æ–¥–∞ –º–æ–¥–µ–ª–∏ Mystem
    # ============================================================
    def _process_native(self, analysis: list) -> list:
        """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–∞—Ç–∏–≤–Ω—ã–π –≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏ Mystem.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É JSON, –∫–æ—Ç–æ—Ä—É—é –æ—Ç–¥–∞–µ—Ç Mystem:
    - text: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–æ–∫–µ–Ω
    - analysis: —Å–ø–∏—Å–æ–∫ –æ–º–æ–Ω–∏–º–æ–≤ (–≥–∏–ø–æ—Ç–µ–∑ —Ä–∞–∑–±–æ—Ä–∞)
      - lex: –ª–µ–º–º–∞
      - gr: –ø–æ–ª–Ω–∞—è –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä–æ–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "S,–∂–µ–Ω,–Ω–µ–æ–¥=–≤–∏–Ω,–µ–¥")
      - wt: –≤–µ—Å (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å) –≥–∏–ø–æ—Ç–µ–∑—ã
      - qual: –º–∞—Ä–∫–µ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ (–û–ü–¶–ò–û–ù–ê–õ–¨–ù–û–ï –ø–æ–ª–µ, –ø–æ—è–≤–ª—è–µ—Ç—Å—è –¢–û–õ–¨–ö–û –¥–ª—è –Ω–µ—Å–ª–æ–≤–∞—Ä–Ω—ã—Ö —Å–ª–æ–≤)
              –í–æ–∑–º–æ–∂–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: "bastard" (–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ —Å–ª–æ–≤–æ), "sob", "prefixoid"
              –î–ª—è –æ–±—ã—á–Ω—ã—Ö —Å–ª–æ–≤–∞—Ä–Ω—ã—Ö —Å–ª–æ–≤ –ø–æ–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç

        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            analysis (list): –ù–∞—Ç–∏–≤–Ω—ã–π –≤—ã–≤–æ–¥ –æ—Ç mystem.analyze()

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            list: –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ —Å –ø–æ–ª–Ω–æ–π –Ω–∞—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
        """
        sent_res = []

        for i, item in enumerate(analysis):
            token_text = item.get('text', '')

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã –∏ —á–∏—Å—Ç—ã–µ –ø—Ä–æ–±–µ–ª—ã
            if not token_text:
                continue

            token_clean = token_text.strip()
            if not token_clean and token_text:
                # –≠—Ç–æ –ø—Ä–æ–±–µ–ª –∏–ª–∏ whitespace - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                continue

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–∏—â–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
            token_text = token_clean

            # ============================================================
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—É—é –Ω–∞—Ç–∏–≤–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É Mystem
            # ============================================================
            native_token = {
                "id": len(sent_res) + 1,  # ID –¥–æ–±–∞–≤–ª—è–µ–º –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ (–Ω–µ —è–≤–ª—è–µ—Ç—Å—è –Ω–∞—Ç–∏–≤–Ω—ã–º –ø–æ–ª–µ–º)
                "text": token_text,  # –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–æ–∫–µ–Ω
                "analysis": item.get('analysis', [])  # –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≥–∏–ø–æ—Ç–µ–∑ —Ä–∞–∑–±–æ—Ä–∞ (–æ–º–æ–Ω–∏–º–æ–≤)
            }

            sent_res.append(native_token)

        return sent_res

    # ============================================================
    # –ë–õ–û–ö: –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (—Ç–µ–∫—É—â–∞—è –ª–æ–≥–∏–∫–∞ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    # ============================================================
    def _process_simplified(self, analysis: list) -> list:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –≤—ã—Ö–æ–¥ (—Ç–µ–∫—É—â–∏–π —Ñ–æ—Ä–º–∞—Ç).

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã —Å –±–∞–∑–æ–≤—ã–º–∏ –ø–æ–ª—è–º–∏: id, form, lemma, upos.
        –ë–µ—Ä–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞ –∏–∑ analysis.

        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            analysis (list): –ù–∞—Ç–∏–≤–Ω—ã–π –≤—ã–≤–æ–¥ –æ—Ç mystem.analyze()

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            list: –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ —Å —É–ø—Ä–æ—â–µ–Ω–Ω—ã–º–∏ –ø–æ–ª—è–º–∏
        """
        sent_res = []

        # ===== –ò–°–ü–†–ê–í–õ–ï–ù–û: –û–ë–†–ê–ë–û–¢–ö–ê –ü–£–ù–ö–¢–£–ê–¶–ò–ò =====
        for i, item in enumerate(analysis):
            token_text = item.get('text', '')

            # –ù–û–í–û–ï: –ù–ï –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å—Ä–∞–∑—É
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–±–µ–ª
            if not token_text:
                continue

            # –ò–°–ü–†–ê–í–õ–ï–ù–û: strip() –º–æ–∂–µ—Ç —É–¥–∞–ª–∏—Ç—å –∑–Ω–∞—á–∏–º—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç, –µ—Å–ª–∏ –æ–Ω –Ω–µ –ø—É—Å—Ç–æ–π –ø–æ—Å–ª–µ strip
            token_clean = token_text.strip()
            if not token_clean and token_text:
                # –≠—Ç–æ –ø—Ä–æ–±–µ–ª –∏–ª–∏ whitespace - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                continue

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–∏—â–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
            token_text = token_clean
            # ===== –ö–û–ù–ï–¶ –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø =====

            lemma = token_text.lower()
            upos = "X"

            # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
            if 'analysis' in item and item['analysis']:
                lex_entry = item['analysis'][0]
                lemma = lex_entry.get('lex', token_text.lower())
                gr_full = lex_entry.get('gr', '')

                # –ò–∑–≤–ª–µ–∫–∞–µ–º POS –∏–∑ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏
                gr_pos = re.split('[,=]', gr_full)[0]
                upos = MYSTEM_TO_UPOS.get(gr_pos, 'X')

            # –ù–û–í–û–ï: –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
            if token_text in '.!?,;:‚Äî‚Äì-"¬´¬ª()[]{}':
                upos = 'PUNCT'

            sent_res.append({
                "id": len(sent_res) + 1,  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –Ω—É–º–µ—Ä–∞—Ü–∏—è —Å 1
                "form": token_text,
                "lemma": lemma,
                "upos": upos
            })

        return sent_res


@app.local_entrypoint()
def main():
    test_texts = [
        "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ.",
        ["–°–ø–∏—Å–æ–∫", "—Ç–æ–∫–µ–Ω–æ–≤", "–¥–ª—è", "—Ç–µ—Å—Ç–∞"],
        "–ú–∞–º–∞ –º—ã–ª–∞ —Ä–∞–º—É."
    ]

    print("üöÄ Testing Mystem service...")
    service = MystemService()

    # ============================================================
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –≤ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    # ============================================================
    print("\n" + "=" * 60)
    print("–£–ü–†–û–©–ï–ù–ù–´–ô –§–û–†–ú–ê–¢ (simplified):")
    print("=" * 60)
    results = service.parse_batch.remote(test_texts, output_format="simplified")

    for i, doc in enumerate(results):
        print(f"\nüìÑ Document {i + 1}: {test_texts[i]}")
        if not doc or not doc[0]:
            print("  [Empty result]")
            continue

        sent = doc[0]
        print(f"  Tokens: {len(sent)}")
        for tok in sent:
            print(f"  {tok['id']}\t{tok['form']} -> {tok['lemma']} ({tok.get('upos', 'X')})")

    # ============================================================
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –≤ –Ω–∞—Ç–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
    # ============================================================
    print("\n" + "=" * 60)
    print("–ù–ê–¢–ò–í–ù–´–ô –§–û–†–ú–ê–¢ (native):")
    print("=" * 60)
    results_native = service.parse_batch.remote(test_texts[:1], output_format="native")

    for i, doc in enumerate(results_native):
        print(f"\nüìÑ Document {i + 1}: {test_texts[i]}")
        if not doc or not doc[0]:
            print("  [Empty result]")
            continue

        sent = doc[0]
        print(f"  Tokens: {len(sent)}")
        for tok in sent[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 —Ç–æ–∫–µ–Ω–∞
            print(f"  Token: {tok['text']}")
            print(f"    Analysis variants: {len(tok['analysis'])}")
            for j, variant in enumerate(tok['analysis'][:2]):  # –ü–µ—Ä–≤—ã–µ 2 –≥–∏–ø–æ—Ç–µ–∑—ã
                print(f"      [{j+1}] lex={variant.get('lex')}, gr={variant.get('gr')}, wt={variant.get('wt')}")

    print("\n‚úÖ Test completed!")


