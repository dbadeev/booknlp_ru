import modal
import logging
import sys
import os

LOCALCOBALDDIR = "src/cobald_parser"
REMOTEROOT = "/root/booknlp_ru"
REMOTESRC = f"{REMOTEROOT}/src"

# –û–±—Ä–∞–∑ –¥–ª—è CoBaLD
image = (
    modal.Image.debian_slim(python_version="3.12")
    .pip_install(
        "huggingface_hub",
        "numpy",
        "razdel",
        "torch==2.10.0",
        "transformers==4.35.2",
    )
    .env({
        "PYTHONPATH": f"{REMOTEROOT}:{REMOTESRC}:$PYTHONPATH",
        "ACCELERATE_DISABLE_MAPPING": "1",
        "ACCELERATE_USE_CPU": "0",
    })
    .add_local_dir(LOCALCOBALDDIR, remote_path=f"{REMOTESRC}/cobald_parser", copy=True)
)

app = modal.App("booknlp-ru-cobald")

@app.cls(image=image, gpu="T4", timeout=600)
class CobaldService:
    @modal.enter()
    def setup(self):
        import torch
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger("CobaldService")

        if REMOTEROOT not in sys.path:
            sys.path.append(REMOTEROOT)
        if REMOTESRC not in sys.path:
            sys.path.append(REMOTESRC)

        from src.cobald_parser.modeling_parser import CobaldParser
        from src.cobald_parser.configuration import CobaldParserConfig
        from src.cobald_parser.pipeline import ConlluTokenClassificationPipeline
        from razdel import tokenize as razdel_tokenize, sentenize

        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        model_name = "CoBaLD/xlm-roberta-base-cobald-parser-ru"

        config = CobaldParserConfig.from_pretrained(model_name)
        model = CobaldParser.from_pretrained(model_name, config=config)
        model.to(self.device)
        model.eval()

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Pipeline –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.pipeline = ConlluTokenClassificationPipeline(
            model=model,
            tokenizer=lambda text: [tok.text for tok in razdel_tokenize(text)],
            sentenizer=lambda text: [sent.text for sent in sentenize(text)]
        )

        self.vocab = config.vocabulary
        self.logger.info(f"CoBaLD pipeline loaded on {self.device}!")

    # ============================================================================
    # –ë–õ–û–ö –ü–û–î–ì–û–¢–û–í–ö–ò –ù–ê–¢–ò–í–ù–û–ì–û –í–´–•–û–î–ê –ú–û–î–ï–õ–ò (CoNLL-Plus —Ñ–æ—Ä–º–∞—Ç)
    # ============================================================================
    def _format_native_output(self, sentence_data: dict) -> str:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—ã—Ö–æ–¥ pipeline –≤ –Ω–∞—Ç–∏–≤–Ω—ã–π CoNLL-Plus —Ñ–æ—Ä–º–∞—Ç (12 –∫–æ–ª–æ–Ω–æ–∫).

        –§–æ—Ä–º–∞—Ç CoNLL-Plus:
        1. ID - –ø–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä —Ç–æ–∫–µ–Ω–∞
        2. FORM - —Å–ª–æ–≤–æ—Ñ–æ—Ä–º–∞
        3. LEMMA - –ª–µ–º–º–∞
        4. UPOS - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π POS-—Ç–µ–≥
        5. XPOS - —è–∑—ã–∫–æ–≤–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π —Ç–µ–≥
        6. FEATS - –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        7. HEAD - –∏–Ω–¥–µ–∫—Å –≥–ª–∞–≤–Ω–æ–≥–æ —Å–ª–æ–≤–∞
        8. DEPREL - —Ç–∏–ø —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–π —Å–≤—è–∑–∏
        9. DEPS - –≤—Ç–æ—Ä–∏—á–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (Enhanced UD)
        10. MISC - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        11. SC (Semantic Class) - —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å
        12. DS (Deep Slot) - –≥–ª—É–±–∏–Ω–Ω—ã–π —Å–ª–æ—Ç

        :param sentence_data: —Å–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ—Ç pipeline
        :return: —Å—Ç—Ä–æ–∫–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ CoNLL-Plus (—Ç–∞–±–ª–∏—Ü–∞ —Å —Ç–∞–±—É–ª—è—Ü–∏–µ–π)
        """
        lines = []

        # ===== –°–û–ó–î–ê–Å–ú –ú–ê–ü–ü–ò–ù–ì –°–¢–ê–†–´–• ID -> –ù–û–í–´–• ID (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ dict-—Ñ–æ—Ä–º–∞—Ç—É) =====
        # –°—Ç–∞—Ä—ã–µ ID: 1 ([CLS]), 2 (–ú–∞–º–∞), 3 (–º—ã–ª–∞), ...
        # –ù–æ–≤—ã–µ ID: 1 (–ú–∞–º–∞), 2 (–º—ã–ª–∞), 3 (—Ä–∞–º—É), ...
        id_mapping = {}  # —Å—Ç–∞—Ä—ã–π_id -> –Ω–æ–≤—ã–π_id
        new_id = 0

        for i, word_id in enumerate(sentence_data['ids']):
            word = sentence_data['words'][i]
            if word == '[CLS]':
                # [CLS] –º–∞–ø–ø–∏—Ç—Å—è –Ω–∞ 0 (root)
                id_mapping[str(word_id)] = 0
            else:
                new_id += 1
                id_mapping[str(word_id)] = new_id
        # =============================================================================

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω
        for i, word_id in enumerate(sentence_data['ids']):
            word = sentence_data['words'][i]

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–π —Ç–æ–∫–µ–Ω [CLS]
            if word == '[CLS]':
                continue

            # –ö–æ–ª–æ–Ω–∫–∞ 1: ID (–Ω–æ–≤—ã–π ID –∏–∑ –º–∞–ø–ø–∏–Ω–≥–∞)
            token_id = id_mapping[str(word_id)]

            # –ö–æ–ª–æ–Ω–∫–∞ 2: FORM
            form = word

            # –ö–æ–ª–æ–Ω–∫–∞ 3: LEMMA
            lemma = sentence_data.get('lemmas', ['_'] * len(sentence_data['words']))[i] or '_'

            # –ö–æ–ª–æ–Ω–∫–∞ 4: UPOS
            upos = sentence_data.get('upos', ['_'] * len(sentence_data['words']))[i]

            # –ö–æ–ª–æ–Ω–∫–∞ 5: XPOS
            xpos = sentence_data.get('xpos', ['_'] * len(sentence_data['words']))[i]

            # –ö–æ–ª–æ–Ω–∫–∞ 6: FEATS
            feats = sentence_data.get('feats', ['_'] * len(sentence_data['words']))[i]

            # –ö–æ–ª–æ–Ω–∫–∏ 7-8: HEAD –∏ DEPREL (–±–∞–∑–æ–≤—ã–π UD)
            head = 0
            deprel = '_'
            if 'deps_ud' in sentence_data:
                for arc_from, arc_to, rel in sentence_data['deps_ud']:
                    if arc_to == word_id:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ HEAD
                        head = id_mapping.get(str(arc_from), 0)
                        deprel = rel
                        break

            # –ö–æ–ª–æ–Ω–∫–∞ 9: DEPS (Enhanced UD)
            deps = '_'
            if 'deps_eud' in sentence_data:
                eud_list = []
                for arc_from, arc_to, rel in sentence_data['deps_eud']:
                    if arc_to == word_id:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ HEAD
                        eud_head = id_mapping.get(str(arc_from), 0)
                        eud_list.append(f"{eud_head}:{rel}")
                if eud_list:
                    deps = '|'.join(eud_list)

            # –ö–æ–ª–æ–Ω–∫–∞ 10: MISC
            misc = sentence_data.get('miscs', ['_'] * len(sentence_data['words']))[i] if 'miscs' in sentence_data else '_'

            # –ö–æ–ª–æ–Ω–∫–∞ 11: SC (Semantic Class) - –Ω–∞—Ç–∏–≤–Ω–æ–µ –ø–æ–ª–µ CoBaLD
            sc = sentence_data.get('semclasses', ['_'] * len(sentence_data['words']))[i] if 'semclasses' in sentence_data else '_'

            # –ö–æ–ª–æ–Ω–∫–∞ 12: DS (Deep Slot) - –Ω–∞—Ç–∏–≤–Ω–æ–µ –ø–æ–ª–µ CoBaLD
            ds = sentence_data.get('deepslots', ['_'] * len(sentence_data['words']))[i] if 'deepslots' in sentence_data else '_'

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É (12 –∫–æ–ª–æ–Ω–æ–∫ —á–µ—Ä–µ–∑ —Ç–∞–±—É–ª—è—Ü–∏—é)
            line = f"{token_id}\t{form}\t{lemma}\t{upos}\t{xpos}\t{feats}\t{head}\t{deprel}\t{deps}\t{misc}\t{sc}\t{ds}"
            lines.append(line)

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –∫–∞–∫ –µ–¥–∏–Ω—É—é —Å—Ç—Ä–æ–∫—É (—Å—Ç—Ä–æ–∫–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã \n)
        return '\n'.join(lines)
    # ============================================================================

    @modal.method()
    def parse_batch(self, batch_tokens: list[list[str]], output_format: str = 'dict'):
        """
        batch_tokens: —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–∫–∞–∂–¥–æ–µ ‚Äî —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤).
        output_format: —Ñ–æ—Ä–º–∞—Ç –≤—ã—Ö–æ–¥–∞ - 'dict' (—Ç–µ–∫—É—â–∏–π) –∏–ª–∏ 'native' (CoNLL-Plus).

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        - –ï—Å–ª–∏ output_format='dict': —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–Ω—ã–º —Ä–∞–∑–±–æ—Ä–æ–º (—Ç–µ–∫—É—â–∏–π —Ñ–æ—Ä–º–∞—Ç).
        - –ï—Å–ª–∏ output_format='native': —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ –≤ –Ω–∞—Ç–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ CoNLL-Plus.
        """
        if not batch_tokens:
            return []

        all_results = []

        for tokens in batch_tokens:
            if not tokens:
                all_results.append([] if output_format == 'dict' else '')
                continue

            try:
                # –°–∫–ª–µ–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç –¥–ª—è pipeline
                text = " ".join(tokens)

                # Pipeline –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç List[Dict] - —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
                decoded_sentences = self.pipeline(text, output_format='list')

                # –ë–µ—Ä—ë–º –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
                if not decoded_sentences:
                    all_results.append([] if output_format == 'dict' else '')
                    continue

                sentence_data = decoded_sentences[0]

                # ========================================================================
                # –í–´–ë–û–† –§–û–†–ú–ê–¢–ê –í–´–•–û–î–ê: –Ω–∞—Ç–∏–≤–Ω—ã–π (CoNLL-Plus) –∏–ª–∏ —Ç–µ–∫—É—â–∏–π (dict)
                # ========================================================================
                if output_format == 'native':
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–∞—Ç–∏–≤–Ω—ã–π CoNLL-Plus —Ñ–æ—Ä–º–∞—Ç
                    native_output = self._format_native_output(sentence_data)
                    all_results.append(native_output)
                else:
                    # –¢–µ–∫—É—â–∞—è –ª–æ–≥–∏–∫–∞ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è dict (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
                    # ===== –ù–û–í–û–ï: –°–û–ó–î–ê–Å–ú –ú–ê–ü–ü–ò–ù–ì –°–¢–ê–†–´–• ID -> –ù–û–í–´–• ID =====
                    # –°—Ç–∞—Ä—ã–µ ID: "1" ([CLS]), "2" (–ú–∞–º–∞), "3" (–º—ã–ª–∞), ...
                    # –ù–æ–≤—ã–µ ID: "1" (–ú–∞–º–∞), "2" (–º—ã–ª–∞), "3" (—Ä–∞–º—É), ...
                    id_mapping = {}  # —Å—Ç–∞—Ä—ã–π_id -> –Ω–æ–≤—ã–π_id
                    new_id = 0

                    for i, word_id in enumerate(sentence_data['ids']):
                        word = sentence_data['words'][i]
                        if word == '[CLS]':
                            # [CLS] (id=1) –º–∞–ø–ø–∏—Ç—Å—è –Ω–∞ 0 (root)
                            id_mapping['1'] = '0'
                        else:
                            new_id += 1
                            id_mapping[str(word_id)] = str(new_id)
                    # =========================================================

                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç —Ç–æ–∫–µ–Ω–æ–≤
                    sent_tokens = []

                    for i, word_id in enumerate(sentence_data['ids']):
                        word = sentence_data['words'][i]

                        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–π [CLS] —Ç–æ–∫–µ–Ω
                        if word == '[CLS]':
                            continue

                        # ===== –ù–û–í–û–ï: –ò–°–ü–û–õ–¨–ó–£–ï–ú –ù–û–í–´–ô ID =====
                        new_token_id = id_mapping[str(word_id)]
                        # =====================================

                        token = {
                            'id': new_token_id,  # –ò–°–ü–†–ê–í–õ–ï–ù–û
                            'form': word,
                            'lemma': sentence_data.get('lemmas', [''] * len(sentence_data['words']))[i] or '_',
                            'upos': sentence_data.get('upos', ['_'] * len(sentence_data['words']))[i],
                            'xpos': sentence_data.get('xpos', ['_'] * len(sentence_data['words']))[i],
                            'feats': sentence_data.get('feats', ['_'] * len(sentence_data['words']))[i],
                            'head': 0,
                            'deprel': '_',
                            'deps': '_',
                            'misc': sentence_data.get('miscs', ['_'] * len(sentence_data['words']))[
                                i] if 'miscs' in sentence_data else '_',
                        }

                        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω—Ç–∞–∫—Å–∏—Å –∏–∑ deps_ud
                        if 'deps_ud' in sentence_data:
                            for arc_from, arc_to, deprel in sentence_data['deps_ud']:
                                if arc_to == word_id:
                                    # ===== –ù–û–í–û–ï: –ò–°–ü–û–õ–¨–ó–£–ï–ú –ú–ê–ü–ü–ò–ù–ì =====
                                    old_head = str(arc_from)
                                    new_head = id_mapping.get(old_head, '0')
                                    token['head'] = int(new_head)
                                    # ======================================
                                    token['deprel'] = deprel
                                    break

                        # Enhanced deps
                        if 'deps_eud' in sentence_data:
                            eud_list = []
                            for arc_from, arc_to, deprel in sentence_data['deps_eud']:
                                if arc_to == word_id:
                                    # ===== –ù–û–í–û–ï: –ò–°–ü–û–õ–¨–ó–£–ï–ú –ú–ê–ü–ü–ò–ù–ì =====
                                    old_head = str(arc_from)
                                    new_head = id_mapping.get(old_head, '0')
                                    eud_list.append(f"{new_head}:{deprel}")
                                    # ======================================
                            if eud_list:
                                token['deps'] = '|'.join(eud_list)

                        # –°–µ–º–∞–Ω—Ç–∏–∫–∞
                        if 'deepslots' in sentence_data and i < len(sentence_data['deepslots']):
                            token['deepslot'] = sentence_data['deepslots'][i]
                        if 'semclasses' in sentence_data and i < len(sentence_data['semclasses']):
                            token['semclass'] = sentence_data['semclasses'][i]

                        sent_tokens.append(token)

                    all_results.append(sent_tokens)
                # ========================================================================

            except Exception as e:
                self.logger.error(f"CoBaLD error: {e}")
                import traceback
                self.logger.error(traceback.format_exc())
                all_results.append([] if output_format == 'dict' else '')

        return all_results

    @modal.method()
    def parse(self, tokens: list[str], output_format: str = 'dict'):
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.

        :param tokens: —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
        :param output_format: 'dict' –∏–ª–∏ 'native'
        :return: —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–∑–±–æ—Ä–∞ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        """
        batch_result = self.parse_batch.remote([tokens], output_format=output_format)
        return batch_result[0] if batch_result else ([] if output_format == 'dict' else '')

@app.local_entrypoint()
def main():
    test_tokens = [
        ["–ú–∞–º–∞", "–º—ã–ª–∞", "—Ä–∞–º—É", "."],
        ["CoBaLD", "—Ä–∞–±–æ—Ç–∞–µ—Ç", "–Ω–∞", "GPU", "."],
    ]

    print("üöÄ Testing CoBaLD service...")
    service = CobaldService()

    # –¢–µ—Å—Ç 1: –¢–µ–∫—É—â–∏–π —Ñ–æ—Ä–º–∞—Ç (dict)
    print("\n" + "="*80)
    print("–¢–ï–°–¢ 1: –¢–µ–∫—É—â–∏–π —Ñ–æ—Ä–º–∞—Ç (output_format='dict')")
    print("="*80)
    results_dict = service.parse_batch.remote(test_tokens, output_format='dict')
    for i, sent in enumerate(results_dict):
        print(f"\nüìÑ Sentence {i + 1}: {' '.join(test_tokens[i])}")
        if not sent:
            print("  [Empty result]")
            continue
        print(f"  Tokens: {len(sent)}")
        for tok in sent:
            print(
                f"  {tok['id']}\t{tok['form']}\t{tok['lemma']}\t{tok['upos']}\t"
                f"{tok.get('xpos', '_')}\t{tok.get('feats', '_')}\t"
                f"{tok['head']}\t{tok['deprel']}"
            )

    # –¢–µ—Å—Ç 2: –ù–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (CoNLL-Plus)
    print("\n" + "="*80)
    print("–¢–ï–°–¢ 2: –ù–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (output_format='native')")
    print("="*80)
    results_native = service.parse_batch.remote(test_tokens, output_format='native')
    for i, sent_native in enumerate(results_native):
        print(f"\nüìÑ Sentence {i + 1}: {' '.join(test_tokens[i])}")
        if not sent_native:
            print("  [Empty result]")
            continue
        print("  CoNLL-Plus format (12 columns):")
        print(sent_native)

    print("\n‚úÖ Test completed!")
