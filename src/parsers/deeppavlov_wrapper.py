#!/usr/bin/env python3
"""
–û–±—ë—Ä—Ç–∫–∞ –¥–ª—è DeepPavlov (—á–µ—Ä–µ–∑ Modal) —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ü–û–õ–ù–û–ì–û –≤—ã—Ö–æ–¥–∞.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç—Ä–∏ —Ä–µ–∂–∏–º–∞:
1. output_format='dict' - —Ç–µ–∫—É—â–∏–π —Ñ–æ—Ä–º–∞—Ç (—Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π)
2. output_format='conllu' - –Ω–∞—Ç–∏–≤–Ω—ã–π CoNLL-U —Ñ–æ—Ä–º–∞—Ç (—Å—Ç—Ä–æ–∫–∞)
3. output_format='full' - –ü–û–õ–ù–´–ô –≤—ã—Ö–æ–¥ —Å probas/logits (—Å–ª–æ–≤–∞—Ä—å)

–¢–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–≤–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞:
- 'razdel' (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è) - –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å —Å–∏–º–≤–æ–ª—å–Ω—ã–º–∏ —Å–º–µ—â–µ–Ω–∏—è–º–∏
- 'native' - –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä DeepPavlov
"""

import logging
import modal
from typing import List, Dict, Any, Literal, Union

logger = logging.getLogger(__name__)


class DeepPavlovParser:
    """
    –ö–ª–∏–µ–Ω—Ç –¥–ª—è DeepPavlov, –∑–∞–ø—É—â–µ–Ω–Ω–æ–≥–æ –≤ Modal.

    Args:
        tokenizer: 'razdel' –∏–ª–∏ 'native'
            - 'razdel' (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è): –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Razdel –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
            - 'native': –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –ø—Ä–æ—Å—Ç–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä DeepPavlov
    """

    def __init__(self, tokenizer: Literal['razdel', 'native'] = 'razdel'):
        self.logger = logging.getLogger(__name__)
        self.tokenizer_type = tokenizer

        try:
            self.service = modal.Cls.from_name("booknlp-ru-deeppavlov", "DeepPavlovService")()
            self.logger.info(f"Connected to DeepPavlov via Modal (tokenizer: {tokenizer}).")
        except Exception as e:
            self.logger.error(f"Failed to connect to Modal app: {e}")
            raise e

    def parse_text(
        self, 
        text: str, 
        output_format: str = 'dict',
        use_cache: bool = False
    ) -> Union[List[List[Dict[str, Any]]], str, Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.

        Args:
            text: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è —Ä–∞–∑–±–æ—Ä–∞
            output_format: —Ñ–æ—Ä–º–∞—Ç –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                - 'dict' (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é): —Ç–µ–∫—É—â–∏–π —Ñ–æ—Ä–º–∞—Ç - —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π,
                  –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ - —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø–æ–ª—è–º–∏:
                  id, form, lemma, upos, xpos, feats, head, deprel, deps, misc,
                  startchar, endchar (–¥–ª—è razdel —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞)

                - 'conllu': –Ω–∞—Ç–∏–≤–Ω—ã–π CoNLL-U —Ñ–æ—Ä–º–∞—Ç - —Ç–µ–∫—Å—Ç–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞
                  —Å 10 –∫–æ–ª–æ–Ω–∫–∞–º–∏ (ID, FORM, LEMMA, UPOS, XPOS, FEATS, HEAD,
                  DEPREL, DEPS, MISC). –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π.

                - 'full': –ü–û–õ–ù–´–ô –≤—ã—Ö–æ–¥ —Å probas/logits - —Å–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π:
                  {
                      'format': 'full',
                      'conllu': <CoNLL-U —Å—Ç—Ä–æ–∫–∞>,
                      'sentences': [
                          [
                              {
                                  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–æ–ª—è CoNLL-U
                                  'id': 1, 'form': '–ú–∞–º–∞', 'lemma': '–º–∞–º–∞',
                                  'upos': 'NOUN', 'head': 2, 'deprel': 'nsubj',
                                  ...
                                  # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û: probas/logits
                                  'heads_proba': [0.05, 0.88, 0.03, ...],
                                  'deps_proba': {'nsubj': 0.92, 'obj': 0.05, ...},
                                  'upos_proba': 0.98
                              },
                              ...
                          ]
                      ],
                      'metadata': {
                          'model': 'ru_syntagrus_joint_parsing',
                          'tokenizer': 'razdel',
                          'vocab': {'deprels': [...]}
                      }
                  }

            use_cache: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (—É—Å–∫–æ—Ä—è–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã)

        Returns:
            - –ï—Å–ª–∏ output_format='dict': List[List[Dict]] - —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            - –ï—Å–ª–∏ output_format='conllu': str - —Å—Ç—Ä–æ–∫–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ CoNLL-U
            - –ï—Å–ª–∏ output_format='full': Dict - –ø–æ–ª–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å probas

        Examples:
            >>> parser = DeepPavlovParser(tokenizer='razdel')

            >>> # –¢–µ–∫—É—â–∏–π —Ñ–æ—Ä–º–∞—Ç (dict)
            >>> result = parser.parse_text("–ú–∞–º–∞ –º—ã–ª–∞ —Ä–∞–º—É.", output_format='dict')
            >>> print(result[0][0]['form'])  # '–ú–∞–º–∞'

            >>> # –ù–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (CoNLL-U)
            >>> result = parser.parse_text("–ú–∞–º–∞ –º—ã–ª–∞ —Ä–∞–º—É.", output_format='conllu')
            >>> print(result)  # "1\t–ú–∞–º–∞\t..."

            >>> # –ü–æ–ª–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å probas
            >>> result = parser.parse_text("–ú–∞–º–∞ –º—ã–ª–∞ —Ä–∞–º—É.", output_format='full')
            >>> token = result['sentences'][0][0]
            >>> print(token['form'])  # '–ú–∞–º–∞'
            >>> print(token['heads_proba'])  # [0.05, 0.88, 0.03, 0.04]
            >>> print(token['deps_proba'])  # {'nsubj': 0.92, 'obj': 0.05, ...}
        """
        try:
            # ====================================================================
            # –£–°–õ–û–í–ù–´–ô –í–´–ó–û–í –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∞ –≤—ã—Ö–æ–¥–∞
            # ====================================================================
            if self.tokenizer_type == 'razdel':
                results = self.service.parse_text.remote(
                    text, 
                    output_format=output_format,
                    use_cache=use_cache
                )
            elif self.tokenizer_type == 'native':
                if output_format == 'full':
                    self.logger.warning(
                        "Full format not supported with native tokenizer. "
                        "Falling back to dict format."
                    )
                    output_format = 'dict'

                results = self.service.parse_text_native.remote(
                    text, 
                    output_format=output_format
                )
            else:
                raise ValueError(f"Unknown tokenizer: {self.tokenizer_type}")
            # ====================================================================

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if output_format == 'dict':
                return results if results else []
            elif output_format == 'conllu':
                return results if results else ''
            else:  # 'full'
                return results if results else {
                    'format': 'full',
                    'conllu': '',
                    'sentences': [],
                    'metadata': {}
                }

        except Exception as e:
            self.logger.error(f"Error during DeepPavlov parsing: {e}")
            raise e

    def parse_batch(
        self, 
        texts: List[str], 
        output_format: str = 'dict',
        use_cache: bool = False
    ) -> Union[List[List[List[Dict[str, Any]]]], List[str], List[Dict[str, Any]]]:
        """
        –ü–∞—Ä—Å–∏—Ç –±–∞—Ç—á —Ç–µ–∫—Å—Ç–æ–≤ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º.

        Args:
            texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            output_format: 'dict', 'conllu' –∏–ª–∏ 'full'
            use_cache: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        """
        try:
            if self.tokenizer_type == 'razdel':
                return self.service.parse_batch.remote(
                    texts, 
                    output_format=output_format,
                    use_cache=use_cache
                )
            elif self.tokenizer_type == 'native':
                if output_format == 'full':
                    self.logger.warning(
                        "Full format not supported with native tokenizer. "
                        "Falling back to dict format."
                    )
                    output_format = 'dict'

                # –î–ª—è native - –≤—ã–∑—ã–≤–∞–µ–º parse_text_native –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                return [
                    self.service.parse_text_native.remote(text, output_format=output_format) 
                    for text in texts
                ]
            else:
                raise ValueError(f"Unknown tokenizer: {self.tokenizer_type}")
        except Exception as e:
            self.logger.error(f"Error during DeepPavlov batch parsing: {e}")
            raise e


if __name__ == "__main__":
    import pandas as pd
    import argparse
    import json

    logging.basicConfig(level=logging.INFO)

    # =========================================================================
    # –ü–ê–†–°–ò–ù–ì –ê–†–ì–£–ú–ï–ù–¢–û–í –ö–û–ú–ê–ù–î–ù–û–ô –°–¢–†–û–ö–ò
    # =========================================================================
    parser_args = argparse.ArgumentParser(
        description='Test DeepPavlov parser with different tokenizers and formats'
    )
    parser_args.add_argument(
        '--tokenizer',
        type=str,
        choices=['razdel', 'native'],
        default='razdel',
        help='Choose tokenizer: razdel (recommended) or native'
    )
    parser_args.add_argument(
        '--output-format',
        type=str,
        choices=['dict', 'conllu', 'full'],
        default='dict',
        help='Choose output format: dict (default), conllu, or full (with probas)'
    )
    parser_args.add_argument(
        '--use-cache',
        action='store_true',
        help='Enable caching for faster repeated queries'
    )
    args = parser_args.parse_args()

    test_text = "–ó–ª–æ, –∫–æ—Ç–æ—Ä—ã–º —Ç—ã –º–µ–Ω—è –ø—É–≥–∞–µ—à—å, –≤–æ–≤—Å–µ –Ω–µ —Ç–∞–∫ –∑–ª–æ, –∫–∞–∫ —Ç—ã –∑–ª–æ —É—Ö–º—ã–ª—è–µ—à—å—Å—è."

    print(f"{'=' * 70}")
    print(f"üöÄ Testing DeepPavlov with {args.tokenizer.upper()} tokenizer")
    print(f"   Output format: {args.output_format.upper()}")
    print(f"   Caching: {'ENABLED' if args.use_cache else 'DISABLED'}")
    print(f"{'=' * 70}")

    try:
        # –°–æ–∑–¥–∞—ë–º parser —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º
        parser = DeepPavlovParser(tokenizer=args.tokenizer)

        # ====================================================================
        # –í–´–ó–û–í –° –£–ö–ê–ó–ê–ù–ò–ï–ú –§–û–†–ú–ê–¢–ê –í–´–•–û–î–ê
        # ====================================================================
        result = parser.parse_text(
            test_text, 
            output_format=args.output_format,
            use_cache=args.use_cache
        )
        # ====================================================================

        # ====================================================================
        # –û–ë–†–ê–ë–û–¢–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–ê –í –ó–ê–í–ò–°–ò–ú–û–°–¢–ò –û–¢ –§–û–†–ú–ê–¢–ê
        # ====================================================================
        if args.output_format == 'conllu':
            # ================================================================
            # CoNLL-U –§–û–†–ú–ê–¢ - –ø—Ä–æ—Å—Ç–æ –≤—ã–≤–æ–¥–∏–º —Å—Ç—Ä–æ–∫—É
            # ================================================================
            print(f"\n--- DeepPavlov CoNLL-U Output ({args.tokenizer}) ---\n")
            print(result)

        elif args.output_format == 'full':
            # ================================================================
            # FULL –§–û–†–ú–ê–¢ - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –ø—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–∞ —Å probas
            # ================================================================
            print(f"\n--- DeepPavlov FULL Output ({args.tokenizer}) ---\n")

            print(f"üìä Structure:")
            print(f"  format: {result['format']}")
            print(f"  conllu: <{len(result['conllu'])} chars>")
            print(f"  sentences: {len(result['sentences'])} sentence(s)")
            print(f"  metadata: {list(result['metadata'].keys())}")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º CoNLL-U —á–∞—Å—Ç—å
            print(f"\nüìÑ CoNLL-U representation:")
            print(result['conllu'])

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–∞ —Å probas
            if result['sentences']:
                print(f"\nüìã Example token with probas/logits:")
                first_token = result['sentences'][0][0]

                print(f"\n  Basic fields:")
                print(f"    form: {first_token['form']}")
                print(f"    lemma: {first_token['lemma']}")
                print(f"    upos: {first_token['upos']}")
                print(f"    head: {first_token['head']}")
                print(f"    deprel: {first_token['deprel']}")

                print(f"\n  Probabilities:")
                print(f"    upos_proba: {first_token.get('upos_proba', 'N/A')}")

                if 'heads_proba' in first_token:
                    heads_p = first_token['heads_proba']
                    print(f"    heads_proba (length={len(heads_p)}): {heads_p[:5]}... (showing first 5)")
                    print(f"      ‚Üí probability for chosen head ({first_token['head']}): "
                          f"{heads_p[first_token['head']]:.3f}")

                if 'deps_proba' in first_token:
                    deps_p = first_token['deps_proba']
                    print(f"    deps_proba (top 5):")
                    for deprel, prob in sorted(deps_p.items(), key=lambda x: -x[1])[:5]:
                        marker = " ‚Üê CHOSEN" if deprel == first_token['deprel'] else ""
                        print(f"      - {deprel}: {prob:.4f}{marker}")

                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Å—å —Ç–æ–∫–µ–Ω –≤ JSON –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã
                print(f"\n  Full token as JSON:")
                print(json.dumps(first_token, indent=4, ensure_ascii=False))

        else:  # 'dict'
            # ================================================================
            # DICT –§–û–†–ú–ê–¢ - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏
            # ================================================================
            sentences = result
            print(f"\nReceived {len(sentences)} sentence(s)")

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame
            all_tokens = [token for sent in sentences for token in sent]
            df = pd.DataFrame(all_tokens)

            print(f"\n--- DeepPavlov Joint Parsing ({args.tokenizer}) ---")
            if not df.empty:
                cols = ['id', 'form', 'lemma', 'upos', 'head', 'deprel']
                available_cols = [col for col in cols if col in df.columns]
                print(df[available_cols].to_string(index=False))

                if 'feats' in df.columns:
                    print(f"\n--- Morphological Features ---")
                    print(df[['form', 'feats']].to_string(index=False))

                # Character offsets —Ç–æ–ª—å–∫–æ –¥–ª—è razdel
                if 'startchar' in df.columns and args.tokenizer == 'razdel':
                    print(f"\n--- Character Offsets (Razdel) ---")
                    print(df[['form', 'startchar', 'endchar']].to_string(index=False))
            else:
                print("Empty result")
        # ====================================================================

        print(f"\n‚úÖ Test completed!")

    except Exception as e:
        print(f"\n‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
