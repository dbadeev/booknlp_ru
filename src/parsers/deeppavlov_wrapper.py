#!/usr/bin/env python3
"""
–û–±—ë—Ä—Ç–∫–∞ –¥–ª—è DeepPavlov (—á–µ—Ä–µ–∑ Modal) —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ü–û–õ–ù–û–ì–û –≤—ã—Ö–æ–¥–∞.
"""

import logging
import modal
from typing import List, Dict, Any, Literal, Union
import json

logger = logging.getLogger(__name__)


class DeepPavlovParser:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è DeepPavlov, –∑–∞–ø—É—â–µ–Ω–Ω–æ–≥–æ –≤ Modal."""

    def __init__(self, tokenizer: Literal['razdel', 'native'] = 'razdel'):
        self.logger = logging.getLogger(__name__)
        self.tokenizer_type = tokenizer

        try:
            self.service = modal.Cls.from_name("booknlp-ru-deeppavlov", "DeepPavlovService")()
            self.logger.info(f"Connected to DeepPavlov via Modal (tokenizer: {tokenizer}).")
        except Exception as e:
            self.logger.error(f"Failed to connect to Modal app: {e}")
            raise e

    def parse_text(
        self, 
        text: str, 
        output_format: str = 'dict',
        use_cache: bool = False
    ) -> Union[List[List[Dict[str, Any]]], str, Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º."""
        try:
            if self.tokenizer_type == 'razdel':
                results = self.service.parse_text.remote(
                    text, 
                    output_format=output_format,
                    use_cache=use_cache
                )
            elif self.tokenizer_type == 'native':
                if output_format == 'full':
                    self.logger.warning("Full format not supported with native tokenizer.")
                    output_format = 'dict'
                results = self.service.parse_text_native.remote(text, output_format=output_format)
            else:
                raise ValueError(f"Unknown tokenizer: {self.tokenizer_type}")

            return results if results else ([] if output_format == 'dict' else '')
        except Exception as e:
            self.logger.error(f"Error during DeepPavlov parsing: {e}")
            raise e

    def parse_batch(
        self, 
        texts: List[str], 
        output_format: str = 'dict',
        use_cache: bool = False
    ) -> Union[List, List[str], List[Dict]]:
        """–ü–∞—Ä—Å–∏—Ç –±–∞—Ç—á —Ç–µ–∫—Å—Ç–æ–≤."""
        try:
            if self.tokenizer_type == 'razdel':
                return self.service.parse_batch.remote(texts, output_format=output_format, use_cache=use_cache)
            elif self.tokenizer_type == 'native':
                if output_format == 'full':
                    self.logger.warning("Full format not supported with native tokenizer.")
                    output_format = 'dict'
                return [
                    self.service.parse_text_native.remote(text, output_format=output_format) 
                    for text in texts
                ]
            else:
                raise ValueError(f"Unknown tokenizer: {self.tokenizer_type}")
        except Exception as e:
            self.logger.error(f"Error during DeepPavlov batch parsing: {e}")
            raise e


if __name__ == "__main__":
    import pandas as pd
    import argparse

    logging.basicConfig(level=logging.INFO)

    parser_args = argparse.ArgumentParser(description='Test DeepPavlov parser')
    parser_args.add_argument('--tokenizer', type=str, choices=['razdel', 'native'], default='razdel')
    parser_args.add_argument('--output-format', type=str, choices=['dict', 'conllu', 'full', 'both'], default='both')
    parser_args.add_argument('--use-cache', action='store_true')
    args = parser_args.parse_args()

    test_text = "–ó–ª–æ, –∫–æ—Ç–æ—Ä—ã–º —Ç—ã –º–µ–Ω—è –ø—É–≥–∞–µ—à—å, –≤–æ–≤—Å–µ –Ω–µ —Ç–∞–∫ –∑–ª–æ, –∫–∞–∫ —Ç—ã –∑–ª–æ —É—Ö–º—ã–ª—è–µ—à—å—Å—è."

    print(f"{'=' * 70}")
    print(f"üöÄ Testing DeepPavlov with {args.tokenizer.upper()} tokenizer")
    print(f"   Output format: {args.output_format.upper()}")
    print(f"   Caching: {'ENABLED' if args.use_cache else 'DISABLED'}")
    print(f"{'=' * 70}")

    try:
        parser = DeepPavlovParser(tokenizer=args.tokenizer)

        # ====================================================================
        # BOTH: –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±–∞ —Ñ–æ—Ä–º–∞—Ç–∞ (standard dict + full)
        # ====================================================================
        if args.output_format == 'both':
            # ================================================================
            # –í–ê–†–ò–ê–ù–¢ 1: STANDARD (dict)
            # ================================================================
            print(f"\n{'‚ïê'*70}")
            print(f"üìä –í–ê–†–ò–ê–ù–¢ 1: STANDARD (dict)")
            print(f"{'‚ïê'*70}")

            result_dict = parser.parse_text(test_text, output_format='dict', use_cache=args.use_cache)
            sentences = result_dict
            print(f"\nReceived {len(sentences)} sentence(s)")

            all_tokens = [token for sent in sentences for token in sent]
            df = pd.DataFrame(all_tokens)

            print(f"\n{'‚îÄ'*70}")
            print(f"üìÑ DeepPavlov Joint Parsing ({args.tokenizer})")
            print(f"{'‚îÄ'*70}")
            if not df.empty:
                cols = ['id', 'form', 'lemma', 'upos', 'head', 'deprel']
                available_cols = [col for col in cols if col in df.columns]
                print(df[available_cols].to_string(index=False))

                print(f"\n{'‚îÄ'*70}")
                print(f"üìã Morphological Features")
                print(f"{'‚îÄ'*70}")
                if 'feats' in df.columns:
                    print(df[['form', 'feats']].to_string(index=False))

                if 'startchar' in df.columns and args.tokenizer == 'razdel':
                    print(f"\n{'‚îÄ'*70}")
                    print(f"üìç Character Offsets (Razdel)")
                    print(f"{'‚îÄ'*70}")
                    print(df[['form', 'startchar', 'endchar']].to_string(index=False))

            # ================================================================
            # –í–ê–†–ò–ê–ù–¢ 2: FULL (—Å probas)
            # ================================================================
            print(f"\n{'‚ïê'*70}")
            print(f"üìä –í–ê–†–ò–ê–ù–¢ 2: FULL (—Å probas)")
            print(f"{'‚ïê'*70}")

            result_full = parser.parse_text(test_text, output_format='full', use_cache=args.use_cache)

            print(f"\nüìã Structure:")
            print(f"  format: {result_full['format']}")
            print(f"  conllu: <{len(result_full['conllu'])} chars>")
            print(f"  sentences: {len(result_full['sentences'])} sentence(s)")

            # –í—ã–≤–æ–¥–∏–º –ü–ï–†–í–´–ï 3 –¢–û–ö–ï–ù–ê –¥–µ—Ç–∞–ª—å–Ω–æ
            print(f"\n{'‚îÄ'*70}")
            print(f"üìä First 3 tokens with probas:")
            print(f"{'‚îÄ'*70}")

            if result_full['sentences']:
                first_sent = result_full['sentences'][0]
                for tok_idx, token in enumerate(first_sent[:3], 1):
                    print(f"\n  [{tok_idx}] {token['form']}")
                    print(f"      {'‚îÄ'*62}")
                    print(f"      ID: {token['id']}")
                    print(f"      Lemma: {token['lemma']}")
                    print(f"      UPOS: {token['upos']}")

                    # UPOS proba —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π
                    upos_proba = token.get('upos_proba', 0)
                    bar = '‚ñà' * int(upos_proba * 20)
                    print(f"      UPOS confidence: {upos_proba:.4f} {bar}")

                    print(f"\n      Head: {token['head']}")
                    print(f"      Deprel: {token['deprel']}")

                    # Heads probabilities (TOP-5)
                    heads_p = token.get('heads_proba', [])
                    if heads_p:
                        print(f"\n      Heads probabilities (TOP-5 from K+1={len(heads_p)}):")
                        heads_enum = [(i, p) for i, p in enumerate(heads_p)]
                        heads_enum.sort(key=lambda x: -x[1])

                        for head_idx, prob in heads_enum[:5]:
                            if head_idx == 0:
                                head_label = "ROOT"
                            else:
                                if head_idx <= len(first_sent):
                                    head_form = first_sent[head_idx-1]['form']
                                    head_label = f"‚Üí {head_form} (id={head_idx})"
                                else:
                                    head_label = f"id={head_idx}"

                            marker = " ‚úì" if head_idx == token['head'] else ""
                            bar = '‚ñà' * int(prob * 20)
                            print(f"        [{head_idx:2d}] {head_label:20s} {prob:.4f} {bar}{marker}")

                    # Dependency relation probabilities (TOP-5)
                    deps_p = token.get('deps_proba', {})
                    if deps_p:
                        print(f"\n      Dependency relation probabilities (TOP-5):")
                        top_deps = sorted(deps_p.items(), key=lambda x: -x[1])[:5]

                        for deprel, prob in top_deps:
                            marker = " ‚úì" if deprel == token['deprel'] else ""
                            bar = '‚ñà' * int(prob * 20)
                            print(f"        {deprel:12s} {prob:.4f} {bar}{marker}")

                # –°–æ–∫—Ä–∞—â—ë–Ω–Ω—ã–π –≤—ã–≤–æ–¥ –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
                if len(first_sent) > 3:
                    print(f"\n  ... –∏ –µ—â—ë {len(first_sent) - 3} —Ç–æ–∫–µ–Ω(–æ–≤) —Å probas")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            print(f"\n{'‚îÄ'*70}")
            print(f"üìà Confidence Statistics:")
            print(f"{'‚îÄ'*70}")

            all_upos = []
            all_heads = []
            all_deps = []

            for sent in result_full['sentences']:
                for token in sent:
                    all_upos.append(token.get('upos_proba', 0))

                    heads_p = token.get('heads_proba', [])
                    if heads_p and token['head'] < len(heads_p):
                        all_heads.append(heads_p[token['head']])

                    deps_p = token.get('deps_proba', {})
                    if token['deprel'] in deps_p:
                        all_deps.append(deps_p[token['deprel']])

            if all_upos:
                print(f"\nUPOS confidence:")
                print(f"  Average: {sum(all_upos)/len(all_upos):.4f}")
                print(f"  Min: {min(all_upos):.4f}")
                print(f"  Max: {max(all_upos):.4f}")

            if all_heads:
                print(f"\nHead attachment confidence:")
                print(f"  Average: {sum(all_heads)/len(all_heads):.4f}")
                print(f"  Min: {min(all_heads):.4f}")
                print(f"  Max: {max(all_heads):.4f}")

            if all_deps:
                print(f"\nDependency relation confidence:")
                print(f"  Average: {sum(all_deps)/len(all_deps):.4f}")
                print(f"  Min: {min(all_deps):.4f}")
                print(f"  Max: {max(all_deps):.4f}")

        # ====================================================================
        # –û–¥–∏–Ω–æ—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
        # ====================================================================
        elif args.output_format == 'conllu':
            result = parser.parse_text(test_text, output_format='conllu', use_cache=args.use_cache)
            print(f"\n{'‚îÄ'*70}")
            print(f"üìÑ CoNLL-U Output ({args.tokenizer})")
            print(f"{'‚îÄ'*70}\n")
            print(result)

        elif args.output_format == 'full':
            result_full = parser.parse_text(test_text, output_format='full', use_cache=args.use_cache)

            print(f"\n{'‚îÄ'*70}")
            print(f"üìä FULL Output with probas")
            print(f"{'‚îÄ'*70}")

            # –ü–æ–ª–Ω—ã–π –≤—ã–≤–æ–¥ –í–°–ï–• —Ç–æ–∫–µ–Ω–æ–≤
            for sent_idx, sent in enumerate(result_full['sentences'], 1):
                print(f"\n{'‚ïê'*70}")
                print(f"Sentence {sent_idx}: {len(sent)} tokens")
                print(f"{'‚ïê'*70}")

                for tok_idx, token in enumerate(sent, 1):
                    print(f"\n  [{tok_idx}] {token['form']}")
                    print(f"      {'‚îÄ'*62}")
                    print(f"      ID: {token['id']}")
                    print(f"      Lemma: {token['lemma']}")
                    print(f"      UPOS: {token['upos']}")

                    upos_proba = token.get('upos_proba', 0)
                    bar = '‚ñà' * int(upos_proba * 20)
                    print(f"      UPOS confidence: {upos_proba:.4f} {bar}")

                    print(f"\n      Head: {token['head']}")
                    print(f"      Deprel: {token['deprel']}")

                    heads_p = token.get('heads_proba', [])
                    if heads_p:
                        print(f"\n      Heads probabilities (TOP-5):")
                        heads_enum = [(i, p) for i, p in enumerate(heads_p)]
                        heads_enum.sort(key=lambda x: -x[1])

                        for head_idx, prob in heads_enum[:5]:
                            if head_idx == 0:
                                head_label = "ROOT"
                            else:
                                if head_idx <= len(sent):
                                    head_form = sent[head_idx-1]['form']
                                    head_label = f"‚Üí {head_form} (id={head_idx})"
                                else:
                                    head_label = f"id={head_idx}"

                            marker = " ‚úì" if head_idx == token['head'] else ""
                            bar = '‚ñà' * int(prob * 20)
                            print(f"        [{head_idx:2d}] {head_label:20s} {prob:.4f} {bar}{marker}")

                    deps_p = token.get('deps_proba', {})
                    if deps_p:
                        print(f"\n      Dependency relation probabilities (TOP-5):")
                        top_deps = sorted(deps_p.items(), key=lambda x: -x[1])[:5]

                        for deprel, prob in top_deps:
                            marker = " ‚úì" if deprel == token['deprel'] else ""
                            bar = '‚ñà' * int(prob * 20)
                            print(f"        {deprel:12s} {prob:.4f} {bar}{marker}")

        else:  # 'dict'
            result = parser.parse_text(test_text, output_format='dict', use_cache=args.use_cache)
            sentences = result
            print(f"\nReceived {len(sentences)} sentence(s)")

            all_tokens = [token for sent in sentences for token in sent]
            df = pd.DataFrame(all_tokens)

            print(f"\n{'‚îÄ'*70}")
            print(f"üìÑ DeepPavlov Joint Parsing ({args.tokenizer})")
            print(f"{'‚îÄ'*70}")
            if not df.empty:
                cols = ['id', 'form', 'lemma', 'upos', 'head', 'deprel']
                available_cols = [col for col in cols if col in df.columns]
                print(df[available_cols].to_string(index=False))

                print(f"\n{'‚îÄ'*70}")
                print(f"üìã Morphological Features")
                print(f"{'‚îÄ'*70}")
                if 'feats' in df.columns:
                    print(df[['form', 'feats']].to_string(index=False))

                if 'startchar' in df.columns and args.tokenizer == 'razdel':
                    print(f"\n{'‚îÄ'*70}")
                    print(f"üìç Character Offsets (Razdel)")
                    print(f"{'‚îÄ'*70}")
                    print(df[['form', 'startchar', 'endchar']].to_string(index=False))

        print(f"\n{'='*70}")
        print(f"‚úÖ Test completed!")
        print(f"{'='*70}")

    except Exception as e:
        print(f"\n‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
