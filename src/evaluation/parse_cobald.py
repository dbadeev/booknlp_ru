#!/usr/bin/env python3.11
"""
CoBaLD Parser Benchmark –Ω–∞ SynTagRus test set.
"""

import sys
from pathlib import Path

# –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å ROOT –≤ —Å–∞–º–æ–º –Ω–∞—á–∞–ª–µ
ROOT = Path(__file__).resolve().parents[2]

# –î–æ–±–∞–≤–∏—Ç—å src –≤ PYTHONPATH
sys.path.insert(0, str(ROOT))
sys.path.insert(0, str(ROOT / "src"))

from typing import List
from rich.console import Console
from rich.progress import Progress
from rich.table import Table
import json

console = Console()

DATA_DIR = ROOT / "data"
SYNTAGRUS_DIR = DATA_DIR / "syntagrus"
RESULTS_DIR = ROOT / "results" / "benchmarks"
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
SYNTAGRUS_TEST = SYNTAGRUS_DIR / "ru_syntagrus-ud-test.conllu"
COBALD_OUTPUT = RESULTS_DIR / "cobald_predictions.conllu"



def load_cobald_pipeline():
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å CoBaLD Parser —á–µ—Ä–µ–∑ –∫–∞—Å—Ç–æ–º–Ω—ã–π Pipeline."""
    console.print("üîç –ó–∞–≥—Ä—É–∑–∫–∞ CoBaLD Parser...\n")

    try:
        # –î–æ–±–∞–≤–∏—Ç—å src –≤ PYTHONPATH
        src_path = ROOT / "src"
        if str(src_path) not in sys.path:
            sys.path.insert(0, str(src_path))

        console.print(f"üì¶ PYTHONPATH: {src_path}\n")

        # –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–∞—Å—Ç–æ–º–Ω—ã–µ –∫–ª–∞—Å—Å—ã
        from cobald_parser import (
            CobaldParser,
            CobaldParserConfig,
            ConlluTokenClassificationPipeline
        )
        console.print("‚úÖ –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –∫–∞—Å—Ç–æ–º–Ω—ã–µ –∫–ª–∞—Å—Å—ã CoBaLD\n")

        model_name = "CoBaLD/xlm-roberta-base-cobald-parser-ru"

        console.print(f"üì• –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –∏–∑ {model_name}...")
        console.print("   (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç)\n")

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
        model = CobaldParser.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        model.eval()

        console.print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n")

        # –ù–∞—Å—Ç—Ä–æ–∏—Ç—å tokenizer/sentenizer
        console.print("üì¶ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏...")

        try:
            from razdel import sentenize, tokenize

            def sentenizer(text):
                return [s.text for s in sentenize(text)]

            def tokenizer(text):
                return [t.text for t in tokenize(text)]

            console.print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º razdel –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏\n")

        except ImportError:
            console.print("‚ö†Ô∏è  razdel –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            console.print("   pip install razdel\n")
            console.print("   –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é\n")

            def sentenizer(text):
                import re
                # –ü—Ä–æ—Å—Ç–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è
                sentences = re.split(r'[.!?]+', text)
                return [s.strip() for s in sentences if s.strip()]

            def tokenizer(text):
                # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
                return text.split()

        # –°–æ–∑–¥–∞—Ç—å pipeline
        pipeline = ConlluTokenClassificationPipeline(
            model=model,
            tokenizer=tokenizer,
            sentenizer=sentenizer
        )

        console.print("‚úÖ CoBaLD Pipeline –≥–æ—Ç–æ–≤\n")
        return pipeline

    except ImportError as e:
        console.print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}\n")
        console.print("–û—Ç–ª–∞–¥–∫–∞:")
        console.print(f"  ROOT: {ROOT}")
        console.print(f"  src path: {ROOT / 'src'}")
        console.print(f"  cobald_parser exists: {(ROOT / 'src' / 'cobald_parser').exists()}")
        console.print(f"  __init__.py exists: {(ROOT / 'src' / 'cobald_parser' / '__init__.py').exists()}")
        console.print(f"\n  –§–∞–π–ª—ã –≤ cobald_parser:")
        cobald_dir = ROOT / 'src' / 'cobald_parser'
        if cobald_dir.exists():
            for f in cobald_dir.glob('*.py'):
                console.print(f"    - {f.name}")
        console.print("\n–ü—Ä–æ–≤–µ—Ä—å:")
        console.print("  1. –í—Å–µ —Ñ–∞–π–ª—ã —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω—ã –≤ src/cobald_parser/")
        console.print("  2. __init__.py —Å–æ–∑–¥–∞–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–æ")
        console.print("  3. –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:")
        console.print("     pip install transformers torch razdel\n")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    except Exception as e:
        console.print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}\n")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def parse_conllu_for_text(filepath: Path, limit: int = None) -> List[str]:
    """–ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–∑ CoNLL-U."""
    texts = []

    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            if line.startswith('# text ='):
                text = line.split('=', 1)[1].strip()
                texts.append(text)

                if limit and len(texts) >= limit:
                    break

    return texts


def run_cobald_parsing(pipeline, texts: List[str]) -> str:
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å CoBaLD Parser –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö."""
    console.print(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ {len(texts)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π...\n")

    all_results = []
    errors = 0

    with Progress() as progress:
        task = progress.add_task(
            "[cyan]–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π...",
            total=len(texts)
        )

        for i, text in enumerate(texts, 1):
            try:
                # –ó–∞–ø—É—Å—Ç–∏—Ç—å pipeline
                result = pipeline(text, output_format='str')
                all_results.append(result)

            except Exception as e:
                errors += 1
                console.print(f"\n‚ö†Ô∏è  –û—à–∏–±–∫–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ {i}: {text[:50]}...")
                console.print(f"   {str(e)[:100]}")

                # –î–æ–±–∞–≤–∏—Ç—å placeholder
                all_results.append(f"# ERROR on sentence {i}: {e}")

            progress.update(task, advance=1)

    if errors > 0:
        console.print(f"\n‚ö†Ô∏è  –û—à–∏–±–æ–∫ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {errors}/{len(texts)}\n")

    return "\n\n".join(all_results)


def parse_conllu_simple(filepath: Path) -> list:
    """–ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä CoNLL-U (–¥–ª—è –æ—Ü–µ–Ω–∫–∏)."""
    sentences = []
    current_sent = []

    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.rstrip('\n')

            if not line or line.startswith('#'):
                if current_sent:
                    sentences.append(current_sent)
                    current_sent = []
                continue

            parts = line.split('\t')
            if len(parts) < 8:
                continue

            token_id = parts[0]

            # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å range –∏ null tokens
            if '-' in token_id or '.' in token_id:
                continue

            try:
                token_id = int(token_id)
                head = int(parts[6])
                deprel = parts[7]

                current_sent.append({
                    'id': token_id,
                    'form': parts[1],
                    'head': head,
                    'deprel': deprel
                })
            except (ValueError, IndexError):
                continue

    if current_sent:
        sentences.append(current_sent)

    return sentences


def calculate_metrics(gold_sentences: list, pred_sentences: list) -> dict:
    """–†–∞—Å—á–µ—Ç UAS –∏ LAS."""
    if len(gold_sentences) != len(pred_sentences):
        console.print(f"‚ö†Ô∏è  –†–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π:")
        console.print(f"   Gold: {len(gold_sentences)}")
        console.print(f"   Pred: {len(pred_sentences)}")
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º—É–º
        min_len = min(len(gold_sentences), len(pred_sentences))
        gold_sentences = gold_sentences[:min_len]
        pred_sentences = pred_sentences[:min_len]

    uas_correct = 0
    las_correct = 0
    total_tokens = 0

    for gold_sent, pred_sent in zip(gold_sentences, pred_sentences):
        # Align tokens by id
        gold_dict = {t['id']: t for t in gold_sent}
        pred_dict = {t['id']: t for t in pred_sent}

        for token_id in gold_dict:
            if token_id not in pred_dict:
                continue

            total_tokens += 1

            if pred_dict[token_id]['head'] == gold_dict[token_id]['head']:
                uas_correct += 1

                if pred_dict[token_id]['deprel'] == gold_dict[token_id]['deprel']:
                    las_correct += 1

    if total_tokens == 0:
        return {}

    return {
        "UAS": uas_correct / total_tokens,
        "LAS": las_correct / total_tokens,
        "uas_correct": uas_correct,
        "las_correct": las_correct,
        "total_tokens": total_tokens
    }


def evaluate_cobald_predictions(gold_file: Path, pred_file: Path):
    """–û—Ü–µ–Ω–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è CoBaLD."""
    console.print("\nüî¢ –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫...\n")

    try:
        gold_sentences = parse_conllu_simple(gold_file)
        pred_sentences = parse_conllu_simple(pred_file)

        console.print(f"‚úÖ Gold: {len(gold_sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
        console.print(f"‚úÖ Pred: {len(pred_sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n")

        metrics = calculate_metrics(gold_sentences, pred_sentences)

        if not metrics:
            console.print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏")
            return None

        return metrics

    except Exception as e:
        console.print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏: {e}")
        import traceback
        traceback.print_exc()
        return None


def display_results(metrics: dict):
    """–ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã."""
    table = Table(title="üìà CoBaLD Parser on SynTagRus-test")
    table.add_column("–ú–µ—Ç—Ä–∏–∫–∞", style="cyan")
    table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", style="green")

    table.add_row("UAS", f"{metrics['UAS']:.4f} ({metrics['UAS'] * 100:.2f}%)")
    table.add_row("LAS", f"{metrics['LAS']:.4f} ({metrics['LAS'] * 100:.2f}%)")
    table.add_row("–ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö UAS", str(metrics['uas_correct']))
    table.add_row("–ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö LAS", str(metrics['las_correct']))
    table.add_row("–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤", str(metrics['total_tokens']))

    console.print("\n")
    console.print(table)


def main():
    console.print("=" * 80)
    console.print("CoBaLD Parser Benchmark".center(80))
    console.print("=" * 80 + "\n")

    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–∞–π–ª—ã
    cobald_dir = ROOT / "src" / "cobald_parser"
    if not cobald_dir.exists():
        console.print(f"‚ùå {cobald_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        sys.exit(1)

    console.print(f"‚úÖ –ü–∞–ø–∫–∞ CoBaLD: {cobald_dir}")
    console.print(f"   –§–∞–π–ª–æ–≤: {len(list(cobald_dir.glob('*.py')))}\n")

    if not SYNTAGRUS_TEST.exists():
        console.print(f"‚ùå {SYNTAGRUS_TEST} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        sys.exit(1)

    # –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã
    console.print("–í—ã–±–µ—Ä–∏ —Ä–µ–∂–∏–º:\n")
    console.print("  1. –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç (10 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)")
    console.print("  2. –ü–æ–ª–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ (8800 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)\n")

    mode = input("–†–µ–∂–∏–º [1/2]: ").strip()

    if mode == '1':
        limit = 10
        console.print("\nüß™ –¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º: 10 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n")
    elif mode == '2':
        limit = None
        console.print("\nüöÄ –ü–æ–ª–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫: –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è")
        console.print("   ‚è±Ô∏è  –í—Ä–µ–º—è: ~10-30 –º–∏–Ω—É—Ç –Ω–∞ CPU\n")
    else:
        console.print("\n–û—Ç–º–µ–Ω–µ–Ω–æ.")
        sys.exit(0)

    # –ó–∞–≥—Ä—É–∑–∏—Ç—å pipeline
    pipeline = load_cobald_pipeline()

    # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—Å—Ç—ã
    console.print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞—é –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ {SYNTAGRUS_TEST.name}...")
    texts = parse_conllu_for_text(SYNTAGRUS_TEST, limit=limit)
    console.print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n")

    # –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥
    conllu_output = run_cobald_parsing(pipeline, texts)

    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_file = COBALD_OUTPUT if limit is None else RESULTS_DIR / "cobald_test.conllu"
    console.print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ {output_file.name}...")
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(conllu_output)
    console.print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ\n")

    # –û—Ü–µ–Ω–∏—Ç—å (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞)
    if limit is None:
        metrics = evaluate_cobald_predictions(SYNTAGRUS_TEST, output_file)

        if metrics:
            display_results(metrics)

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å JSON
            results_json = RESULTS_DIR / "cobald_metrics.json"
            with open(results_json, 'w', encoding='utf-8') as f:
                json.dump({
                    "parser": "cobald",
                    "dataset": "SynTagRus-test",
                    "metrics": {
                        "UAS": metrics['UAS'],
                        "LAS": metrics['LAS']
                    },
                    "details": metrics
                }, f, indent=2)

            console.print(f"\nüíæ –ú–µ—Ç—Ä–∏–∫–∏: {results_json.name}")
    else:
        console.print(f"üìÑ –ü—Ä–æ–≤–µ—Ä—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç: cat {output_file}")

    console.print("\n" + "=" * 80)
    console.print("\n‚ú® –ì–æ—Ç–æ–≤–æ!\n")


if __name__ == "__main__":
    main()
